{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc22338c85d486a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:50.538223Z",
     "start_time": "2025-08-10T12:54:50.534161Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:04.297305Z",
     "iopub.status.busy": "2025-08-18T11:20:04.296833Z",
     "iopub.status.idle": "2025-08-18T11:20:04.301178Z",
     "shell.execute_reply": "2025-08-18T11:20:04.300809Z"
    },
    "papermill": {
     "duration": 0.014534,
     "end_time": "2025-08-18T11:20:04.301750",
     "exception": false,
     "start_time": "2025-08-18T11:20:04.287216",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "data_path     = \"/home/omadbek/projects/Sherlock/custom_data/label_generation/test_data_generation.parquet\"\n",
    "labels_path   = \"/home/omadbek/projects/Sherlock/custom_data/label_generation/test_labels_generation.parquet\"\n",
    "archetype_directory = \"/home/omadbek/projects/ArcheType\"\n",
    "run_all_directory = \"/home/omadbek/projects/run_all\"\n",
    "prompt_type = \"zero-shot\"\n",
    "tune = \"0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "694689a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:04.311048Z",
     "iopub.status.busy": "2025-08-18T11:20:04.310804Z",
     "iopub.status.idle": "2025-08-18T11:20:04.313164Z",
     "shell.execute_reply": "2025-08-18T11:20:04.312837Z"
    },
    "papermill": {
     "duration": 0.007585,
     "end_time": "2025-08-18T11:20:04.313774",
     "exception": false,
     "start_time": "2025-08-18T11:20:04.306189",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "tune = \"llama_run9\"\n",
    "data_path = \"/home/omadbek/projects/Sherlock/custom_data/label_generation/test_data_generation.parquet\"\n",
    "labels_path = \"/home/omadbek/projects/Sherlock/custom_data/label_generation/test_labels_generation.parquet\"\n",
    "archetype_directory = \"/home/omadbek/projects/ArcheType\"\n",
    "run_all_directory = \"/home/omadbek/projects/run_all\"\n",
    "prompt_type = \"few-shot\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99491884dd57ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:50.924054Z",
     "start_time": "2025-08-10T12:54:50.922649Z"
    },
    "papermill": {
     "duration": 0.009422,
     "end_time": "2025-08-18T11:20:04.331451",
     "exception": false,
     "start_time": "2025-08-18T11:20:04.322029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:53.253081Z",
     "start_time": "2025-08-10T12:54:51.078840Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:04.348897Z",
     "iopub.status.busy": "2025-08-18T11:20:04.348559Z",
     "iopub.status.idle": "2025-08-18T11:20:06.460599Z",
     "shell.execute_reply": "2025-08-18T11:20:06.459556Z"
    },
    "papermill": {
     "duration": 2.121663,
     "end_time": "2025-08-18T11:20:06.462333",
     "exception": false,
     "start_time": "2025-08-18T11:20:04.340670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omadbek/.conda/envs/archetype/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#https://colab.research.google.com/drive/1BEZ_qgtVqSmOmCTuhHs7lHiYB5M5_myg?usp=sharing\n",
    "\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "#import gzipƒ\n",
    "from tqdm.auto import tqdm\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import chain\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from retry import retry\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d565ecc3-2433-4b7b-bfa7-d8225b0db815",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:53.437735Z",
     "start_time": "2025-08-10T12:54:53.294044Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:06.485747Z",
     "iopub.status.busy": "2025-08-18T11:20:06.485344Z",
     "iopub.status.idle": "2025-08-18T11:20:06.639061Z",
     "shell.execute_reply": "2025-08-18T11:20:06.638345Z"
    },
    "papermill": {
     "duration": 0.16618,
     "end_time": "2025-08-18T11:20:06.639909",
     "exception": false,
     "start_time": "2025-08-18T11:20:06.473729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EST_CHARS_PER_TOKEN=4\r\n",
      "MAX_LEN=2000*EST_CHARS_PER_TOKEN\r\n",
      "INTEGER_SET = set(r\"0123456789,/\\+-.^_()[] :\")\r\n",
      "BOOLEAN_SET = set([\"True\", \"true\", \"False\", \"false\", \"yes\", \"Yes\", \"No\", \"no\"])\r\n",
      "\r\n",
      "ARCHETYPE_PATH = \"/home/omadbek/projects/ArcheType\"\r\n",
      "DOTENV_PATH = \"/home/omadbek/projects/ArcheType/.env\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/omadbek/projects/ArcheType/src/const.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "881a695b-e8d4-4c8b-ab64-52f8152df91a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:53.502834Z",
     "start_time": "2025-08-10T12:54:53.497845Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:06.683513Z",
     "iopub.status.busy": "2025-08-18T11:20:06.683299Z",
     "iopub.status.idle": "2025-08-18T11:20:06.688582Z",
     "shell.execute_reply": "2025-08-18T11:20:06.688054Z"
    },
    "papermill": {
     "duration": 0.040228,
     "end_time": "2025-08-18T11:20:06.689741",
     "exception": false,
     "start_time": "2025-08-18T11:20:06.649513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/omadbek/projects/ArcheType\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dafde7b-0eef-4af8-a652-012a533151a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:53.870657Z",
     "start_time": "2025-08-10T12:54:53.577489Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:06.712652Z",
     "iopub.status.busy": "2025-08-18T11:20:06.712516Z",
     "iopub.status.idle": "2025-08-18T11:20:06.965665Z",
     "shell.execute_reply": "2025-08-18T11:20:06.965038Z"
    },
    "papermill": {
     "duration": 0.266378,
     "end_time": "2025-08-18T11:20:06.967030",
     "exception": false,
     "start_time": "2025-08-18T11:20:06.700652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())      # → 1\n",
    "print(torch.cuda.get_device_name(0))  # → the one you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "229d1025-a37a-4c7d-95f9-5714da81c743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:53.940428Z",
     "start_time": "2025-08-10T12:54:53.935351Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:06.990935Z",
     "iopub.status.busy": "2025-08-18T11:20:06.990650Z",
     "iopub.status.idle": "2025-08-18T11:20:06.995553Z",
     "shell.execute_reply": "2025-08-18T11:20:06.994959Z"
    },
    "papermill": {
     "duration": 0.018192,
     "end_time": "2025-08-18T11:20:06.996721",
     "exception": false,
     "start_time": "2025-08-18T11:20:06.978529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_labels(label, label_set):\n",
    "  label = label.lower().strip()\n",
    "  ldm = {k.lower().strip() : v.lower().strip() for k, v in label_set['dict_map'].items()}\n",
    "  if label_set.get(\"abbrev_map\", -1) != -1:\n",
    "    lda = {k.lower().strip() : v.lower().strip() for k, v in label_set['abbrev_map'].items()}\n",
    "    ldares = lda.get(label, \"\")\n",
    "    if ldares != \"\":\n",
    "      label = ldares\n",
    "  if label.endswith(\"/name\"):\n",
    "    label = label[:-5]\n",
    "  remap = ldm.get(label, -1)\n",
    "  if remap != -1:\n",
    "    label = remap\n",
    "  return label.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b47fba85-2e32-4803-82d4-278a673a03ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:54.020681Z",
     "start_time": "2025-08-10T12:54:54.017389Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:07.020508Z",
     "iopub.status.busy": "2025-08-18T11:20:07.020368Z",
     "iopub.status.idle": "2025-08-18T11:20:07.023176Z",
     "shell.execute_reply": "2025-08-18T11:20:07.022627Z"
    },
    "papermill": {
     "duration": 0.016267,
     "end_time": "2025-08-18T11:20:07.024380",
     "exception": false,
     "start_time": "2025-08-18T11:20:07.008113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LABELS = ['age', 'case_status', 'contact_setting', 'date', 'gender', 'id',\n",
    "#       'location', 'medical_boolean', 'occupation', 'outcome', 'symptoms']\n",
    "\n",
    "LABELS = ['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db475a1-be44-4499-980d-127a62cab476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:54.092993Z",
     "start_time": "2025-08-10T12:54:54.087936Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:07.048399Z",
     "iopub.status.busy": "2025-08-18T11:20:07.048109Z",
     "iopub.status.idle": "2025-08-18T11:20:07.054044Z",
     "shell.execute_reply": "2025-08-18T11:20:07.053245Z"
    },
    "papermill": {
     "duration": 0.01953,
     "end_time": "2025-08-18T11:20:07.055285",
     "exception": false,
     "start_time": "2025-08-18T11:20:07.035755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sotab_integer_labels = [\"age\", \"date\"]\n",
    "sotab_float_labels   = []\n",
    "\n",
    "# everything else must go here\n",
    "sotab_other_labels = [\n",
    "  \"case_status\",\n",
    "  \"gender\",\n",
    "  \"id\",\n",
    "  \"location\",\n",
    "  \"medical_boolean\",\n",
    "  \"occupation\",\n",
    "  \"outcome\",\n",
    "  \"symptoms\"\n",
    "]\n",
    "\n",
    "sotab_top_hier = {\n",
    "  \"integer\": sotab_integer_labels,\n",
    "  \"float\":   sotab_float_labels,\n",
    "  \"other\":   sotab_other_labels\n",
    "}\n",
    "\n",
    "sotab_identifier = [\"id\"]\n",
    "sotab_category   = [\"gender\", \"medical_boolean\", \"outcome\"]\n",
    "sotab_text       = [\"location\", \"symptoms\", \"occupation\"]\n",
    "\n",
    "\n",
    "sotab_other_hier = {\n",
    "  \"Identifier\": sotab_identifier,\n",
    "  \"category\":   sotab_category,\n",
    "  \"text\":       sotab_text\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e79430-47a7-4d5a-9f97-b5524ed9305a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:54.174403Z",
     "start_time": "2025-08-10T12:54:54.171092Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:07.079427Z",
     "iopub.status.busy": "2025-08-18T11:20:07.078636Z",
     "iopub.status.idle": "2025-08-18T11:20:07.082745Z",
     "shell.execute_reply": "2025-08-18T11:20:07.081973Z"
    },
    "papermill": {
     "duration": 0.017588,
     "end_time": "2025-08-18T11:20:07.084066",
     "exception": false,
     "start_time": "2025-08-18T11:20:07.066478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand_seed=13\n",
    "EST_CHARS_PER_TOKEN=4\n",
    "MAX_LEN=2000*EST_CHARS_PER_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff3bbf8b-5706-486b-9cd0-857411301200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:54.259035Z",
     "start_time": "2025-08-10T12:54:54.255860Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:07.108273Z",
     "iopub.status.busy": "2025-08-18T11:20:07.107726Z",
     "iopub.status.idle": "2025-08-18T11:20:07.111668Z",
     "shell.execute_reply": "2025-08-18T11:20:07.110801Z"
    },
    "papermill": {
     "duration": 0.017154,
     "end_time": "2025-08-18T11:20:07.112808",
     "exception": false,
     "start_time": "2025-08-18T11:20:07.095654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"/home/omadbek/projects/alpaca/outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c10f1c02-1fd2-4795-9ee0-17b6b16ad4b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:54.352163Z",
     "start_time": "2025-08-10T12:54:54.343343Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:07.130341Z",
     "iopub.status.busy": "2025-08-18T11:20:07.130055Z",
     "iopub.status.idle": "2025-08-18T11:20:07.139838Z",
     "shell.execute_reply": "2025-08-18T11:20:07.138957Z"
    },
    "papermill": {
     "duration": 0.019967,
     "end_time": "2025-08-18T11:20:07.140971",
     "exception": false,
     "start_time": "2025-08-18T11:20:07.121004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROMPTS\n",
    "\n",
    "def llm_prompts(input_list, options_str=None):\n",
    "    if prompt_type == \"zero-shot\":\n",
    "        prompt = \\\n",
    "            f\"\"\"\n",
    "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
    "\n",
    "                INSTRUCTIONS:\n",
    "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
    "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
    "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
    "\n",
    "                INPUT: {input_list}\n",
    "                OPTIONS: {options_str}\n",
    "                ANSWER:\n",
    "            \"\"\"\n",
    "    elif prompt_type == \"few-shot\":\n",
    "        prompt = \\\n",
    "            f\"\"\"\n",
    "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
    "\n",
    "                INSTRUCTIONS:\n",
    "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
    "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
    "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
    "\n",
    "                This is just examples how it could look like:\n",
    "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
    "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
    "                ANSWER: vaccine_names\n",
    "\n",
    "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
    "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
    "                ANSWER: disease\n",
    "\n",
    "                Now it's your turn:\n",
    "                INPUT: {input_list}\n",
    "                OPTIONS: {options_str}\n",
    "                ANSWER:\n",
    "            \"\"\"\n",
    "        # prompt = f\"\"\"SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
    "        #\n",
    "        #             INSTRUCTIONS:\n",
    "        #              • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
    "        #              • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label, do not generate label that strictly follows what is inside data.\n",
    "        #              • Respond with EXACTLY the label token, no additional words, punctuation, or explanation.\n",
    "        #\n",
    "        #             This is just examples how it could look like:\n",
    "        #             INPUT: [26, 29, 35, 45]\n",
    "        #             OPTIONS: [\"id\", \"age\", \"date\", \"location\"]\n",
    "        #             ANSWER: age\n",
    "        #\n",
    "        #             INPUT: [\"taxi driver\", \"lawyer\", \"pilot\", \"entrepreneur\"]\n",
    "        #             OPTIONS: [\"location\", \"medical_boolean\", \"symptoms\", \"occupation\"]\n",
    "        #             ANSWER: occupation\n",
    "        #\n",
    "        #             Now it's your turn:\n",
    "        #             INPUT: {input_list}\n",
    "        #             OPTIONS: {options_str}\n",
    "        #             ANSWER:\n",
    "        #         \"\"\"\n",
    "    elif prompt_type == \"chain-thoughts\":\n",
    "        prompt = \\\n",
    "            f\"\"\"\n",
    "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
    "\n",
    "                INSTRUCTIONS:\n",
    "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
    "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
    "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation. Do not output step by step reasoaning steps. Only output final answer **\n",
    "\n",
    "                INPUT: {input_list}\n",
    "                OPTIONS: {options_str}\n",
    "\n",
    "                Let’s think step by step:\n",
    "                1. Identify the key features in the INPUT.\n",
    "                2. Try to predict values type like: int, date, string.\n",
    "                3. Compare against each option to see which matches best.\n",
    "                4. If none match, decide on a broad new label in snake_case.\n",
    "                5. Final Answer:\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "        # SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
    "        #\n",
    "        #         INSTRUCTIONS:\n",
    "        #         • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
    "        #         • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label, do not generate label that strictly follows what is inside data.\n",
    "        #         • Respond with EXACTLY the label token, no additional words, punctuation, or explanation. Do not output step by step reasoaning steps. Only output final answer\n",
    "        #\n",
    "        #         INPUT: {input_list}\n",
    "        #         OPTIONS: {options_str}\n",
    "        #\n",
    "        #         Let’s think step by step:\n",
    "        #         1. Identify the key features in the INPUT.\n",
    "        #         2. Try to predict values type like: int, date, string.\n",
    "        #         2. Compare against each option to see which matches best.\n",
    "        #         3. If none match, decide on a broad new label in snake_case.\n",
    "        #         4. Final Answer:\n",
    "        #     \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7bdfa-04c5-4106-b791-166e74bbc98c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:54.443184Z",
     "start_time": "2025-08-10T12:54:54.440738Z"
    },
    "papermill": {
     "duration": 0.007988,
     "end_time": "2025-08-18T11:20:07.157082",
     "exception": false,
     "start_time": "2025-08-18T11:20:07.149094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba93a7e3-1802-4db0-96e5-6e9f8c9e1921",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:55.969591Z",
     "start_time": "2025-08-10T12:54:54.494542Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:07.174772Z",
     "iopub.status.busy": "2025-08-18T11:20:07.174190Z",
     "iopub.status.idle": "2025-08-18T11:20:08.768496Z",
     "shell.execute_reply": "2025-08-18T11:20:08.767663Z"
    },
    "papermill": {
     "duration": 1.604131,
     "end_time": "2025-08-18T11:20:08.769354",
     "exception": false,
     "start_time": "2025-08-18T11:20:07.165223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Load the embedding model once at module scope\n",
    "_sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
    "\n",
    "def _normalize_label(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Lowercase, drop non-alphanumerics,\n",
    "    and strip a trailing 's' for crude singularization.\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-z0-9]', '', s)\n",
    "    if s.endswith('s'):\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "\n",
    "def _embed_labels(labels: List[str]):\n",
    "    \"\"\"\n",
    "    Encode a list of labels into normalized sentence‐embeddings.\n",
    "    \"\"\"\n",
    "    return _sent_model.encode(labels, normalize_embeddings=True)\n",
    "\n",
    "class _FuzzyLabelMatcher:\n",
    "    def __init__(\n",
    "        self,\n",
    "        fixed_labels: List[str],\n",
    "        embed_threshold: float,\n",
    "        fuzz_threshold: int\n",
    "    ):\n",
    "        self.fixed_labels = fixed_labels\n",
    "        self.embed_threshold = embed_threshold\n",
    "        self.fuzz_threshold = fuzz_threshold\n",
    "        self._update_embeddings()\n",
    "\n",
    "    def _update_embeddings(self):\n",
    "        self._embeddings = (\n",
    "            _embed_labels(self.fixed_labels)\n",
    "            if self.fixed_labels else\n",
    "            None\n",
    "        )\n",
    "\n",
    "    def _find_match(self, label: str):\n",
    "\n",
    "        norm_label = label.lower()\n",
    "\n",
    "        if not self.fixed_labels:\n",
    "            return None\n",
    "\n",
    "        # 1) token- and exact-normalize as before…\n",
    "        for existing in self.fixed_labels:\n",
    "            tokens = re.split(r'[^a-zA-Z0-9]+', existing.lower())\n",
    "            if norm_label in tokens:\n",
    "                return existing\n",
    "\n",
    "        # 2) compute all fuzzy scores & pick the best\n",
    "        fuzz_scores = [\n",
    "            (existing, fuzz.token_sort_ratio(label, existing))\n",
    "            for existing in self.fixed_labels\n",
    "        ]\n",
    "        best_fuzz_label, best_fuzz_score = max(fuzz_scores, key=lambda x: x[1])\n",
    "\n",
    "        # 3) compute all embed sims & pick the best\n",
    "        emb     = _sent_model.encode([label], normalize_embeddings=True)[0]\n",
    "        sims    = util.cos_sim(emb, self._embeddings)[0].tolist()\n",
    "        best_idx = max(range(len(sims)), key=lambda i: sims[i])\n",
    "        best_emb_label, best_emb_score = (\n",
    "            self.fixed_labels[best_idx],\n",
    "            sims[best_idx],\n",
    "        )\n",
    "        \n",
    "        #print(f\"[DEBUG] Comparing '{label}' → \"\n",
    "        #      f\"best_fuzz:('{best_fuzz_label}', {best_fuzz_score}), \"\n",
    "        #      f\"best_emb:('{best_emb_label}', {best_emb_score:.2f})\")\n",
    "\n",
    "        # 4) if either passes its threshold, choose the stronger signal\n",
    "        if best_fuzz_score >= self.fuzz_threshold and best_emb_score >= self.embed_threshold:\n",
    "            # pick whichever of the two is relatively stronger\n",
    "            # (you could also prefer embed over fuzz, etc.)\n",
    "            if best_emb_score >= best_fuzz_score / 100:\n",
    "                return best_emb_label\n",
    "            else:\n",
    "                return best_fuzz_label\n",
    "\n",
    "        return None\n",
    "\n",
    "    def resolve(self, orig_label: str) -> str:\n",
    "        \"\"\"\n",
    "        If `orig_label` matches an existing one, return that existing label.\n",
    "        Otherwise, add `orig_label` to fixed_labels and return it.\n",
    "        \"\"\"\n",
    "        match = self._find_match(orig_label)\n",
    "        if match:\n",
    "            return match\n",
    "\n",
    "        # new label → add and update embeddings\n",
    "        self.fixed_labels.append(orig_label)\n",
    "        self._update_embeddings()\n",
    "        return orig_label\n",
    "\n",
    "def resolve_label(\n",
    "    orig_label: str,\n",
    "    fixed_labels: List[str],\n",
    "    embed_threshold: float = 0.82,\n",
    "    fuzz_threshold: int = 90\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Given a newly generated label `orig_label` and a list of\n",
    "    `fixed_labels`, return a tuple:\n",
    "      (resolved_label, updated_fixed_labels).\n",
    "\n",
    "    - If `orig_label` is similar to an existing label, `resolved_label`\n",
    "      is that existing label.\n",
    "    - Otherwise, `orig_label` is appended to fixed_labels, and returned.\n",
    "    \"\"\"\n",
    "    print(\"_FuzzyLabelMatcher list: \", fixed_labels)\n",
    "    matcher = _FuzzyLabelMatcher(fixed_labels, embed_threshold, fuzz_threshold)\n",
    "    resolved = matcher.resolve(orig_label)\n",
    "    return resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "770c20a8a920a24e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:56.707299Z",
     "start_time": "2025-08-10T12:54:56.107734Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:08.780400Z",
     "iopub.status.busy": "2025-08-18T11:20:08.779786Z",
     "iopub.status.idle": "2025-08-18T11:20:09.335291Z",
     "shell.execute_reply": "2025-08-18T11:20:09.334541Z"
    },
    "papermill": {
     "duration": 0.562296,
     "end_time": "2025-08-18T11:20:09.336608",
     "exception": false,
     "start_time": "2025-08-18T11:20:08.774312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the embedding model once at module scope\n",
    "_sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
    "\n",
    "class _CosineLabelMatcher:\n",
    "    def __init__(\n",
    "        self,\n",
    "        fixed_labels: List[str],\n",
    "        embed_threshold: float\n",
    "    ):\n",
    "        self.fixed_labels = fixed_labels\n",
    "        self.embed_threshold = embed_threshold\n",
    "        self._update_embeddings()\n",
    "\n",
    "    def _update_embeddings(self):\n",
    "        self._embeddings = (\n",
    "            _sent_model.encode(self.fixed_labels, normalize_embeddings=True)\n",
    "            if self.fixed_labels else None\n",
    "        )\n",
    "\n",
    "    def _normalize_tokens(self, text: str) -> List[str]:\n",
    "        return re.split(r'[^a-zA-Z0-9]+', text.lower())\n",
    "\n",
    "    def _find_match(self, label: str) -> str:\n",
    "        if not self.fixed_labels:\n",
    "            return None\n",
    "\n",
    "        norm = label.lower()\n",
    "        # 1) token- or exact-match\n",
    "        for existing in self.fixed_labels:\n",
    "            if norm in self._normalize_tokens(existing):\n",
    "                return existing\n",
    "\n",
    "        # 2) embed-based similarity\n",
    "        emb = _sent_model.encode([label], normalize_embeddings=True)[0]\n",
    "        sims = util.cos_sim(emb, self._embeddings)[0].tolist()\n",
    "        best_idx = max(range(len(sims)), key=lambda i: sims[i])\n",
    "        best_label = self.fixed_labels[best_idx]\n",
    "        best_score = sims[best_idx]\n",
    "\n",
    "        if best_score >= self.embed_threshold:\n",
    "            return best_label\n",
    "\n",
    "        return None\n",
    "\n",
    "    def resolve(self, orig_label: str) -> str:\n",
    "        match = self._find_match(orig_label)\n",
    "        if match:\n",
    "            return match\n",
    "        # new label → add and update embeddings\n",
    "        self.fixed_labels.append(orig_label)\n",
    "        self._update_embeddings()\n",
    "        return orig_label\n",
    "\n",
    "\n",
    "def resolve_label(\n",
    "    orig_label: str,\n",
    "    fixed_labels: List[str],\n",
    "    embed_threshold: float = 0.82\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Given a newly generated label `orig_label` and a list of\n",
    "    `fixed_labels`, return the resolved label:\n",
    "\n",
    "      - If `orig_label` is similar to an existing label (by cosine\n",
    "        similarity or exact token match), returns that existing label.\n",
    "      - Otherwise, appends `orig_label` to `fixed_labels` and returns it.\n",
    "    \"\"\"\n",
    "    matcher = _CosineLabelMatcher(fixed_labels, embed_threshold)\n",
    "    return matcher.resolve(orig_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f0ad0852087fc42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:56.833423Z",
     "start_time": "2025-08-10T12:54:56.830825Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:09.352259Z",
     "iopub.status.busy": "2025-08-18T11:20:09.351866Z",
     "iopub.status.idle": "2025-08-18T11:20:09.354740Z",
     "shell.execute_reply": "2025-08-18T11:20:09.354329Z"
    },
    "papermill": {
     "duration": 0.010084,
     "end_time": "2025-08-18T11:20:09.355285",
     "exception": false,
     "start_time": "2025-08-18T11:20:09.345201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fixed = [\"patient_id\", \"city_name\", \"medical_boolean\", \"date_of_birth\"]\n",
    "\n",
    "#resolved_label = resolve_label(\"patient_gender\", fixed, embed_threshold = 0.70,) #fuzz_threshold = 80)\n",
    "#print(\"Resolved label is: \", resolved_label)\n",
    "\n",
    "#fixed = [\"patient_ids\", \"city_name\", \"medical_boolean\", \"date_of_birth\"]\n",
    "#resolved_label = resolve_label(\"location\", fixed, embed_threshold = 0.50,)\n",
    "#print(\"Resolved label is: \", resolved_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b367fbc69ea0ce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:56.924640Z",
     "start_time": "2025-08-10T12:54:56.921816Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:09.365686Z",
     "iopub.status.busy": "2025-08-18T11:20:09.365252Z",
     "iopub.status.idle": "2025-08-18T11:20:09.367613Z",
     "shell.execute_reply": "2025-08-18T11:20:09.367209Z"
    },
    "papermill": {
     "duration": 0.008056,
     "end_time": "2025-08-18T11:20:09.368117",
     "exception": false,
     "start_time": "2025-08-18T11:20:09.360061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fixed = [\"location\", \"age\", \"patient_ids\", \"date_of_birth\"]\n",
    "\n",
    "#resolved_label = resolve_label(\"date_values\", fixed, embed_threshold = 0.40, fuzz_threshold = 40)\n",
    "#print(\"Resolved label is: \", resolved_label)\n",
    "#matcher = _FuzzyLabelMatcher(fixed, 0.82, 90)\n",
    "#print(matcher.remap_label(\"patient_id\", fixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76159fc4-0916-44b2-b99d-e6eca91be82e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:58.139560Z",
     "start_time": "2025-08-10T12:54:56.984289Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:09.378344Z",
     "iopub.status.busy": "2025-08-18T11:20:09.378105Z",
     "iopub.status.idle": "2025-08-18T11:20:11.071881Z",
     "shell.execute_reply": "2025-08-18T11:20:11.070772Z"
    },
    "papermill": {
     "duration": 1.700483,
     "end_time": "2025-08-18T11:20:11.073275",
     "exception": false,
     "start_time": "2025-08-18T11:20:09.372792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import infer_auto_device_map, init_empty_weights, load_checkpoint_and_dispatch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, \\\n",
    "    T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "import langchain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "#sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
    "\n",
    "\n",
    "def set_pipeline(k=1):\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=MAX_LEN,\n",
    "        temperature=0.5 * k,\n",
    "        top_p=0.80 - (0.1 * k),\n",
    "        repetition_penalty=1.3\n",
    "    )\n",
    "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    llm_chain = LLMChain(prompt=pt,\n",
    "                         llm=local_llm\n",
    "                         )\n",
    "    return pipe, local_llm, llm_chain\n",
    "\n",
    "\n",
    "curr_model = \"\"\n",
    "\n",
    "\n",
    "def init_model(model):\n",
    "    curr_model = model\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    if model == \"llama-65b\":\n",
    "        LLAMA_PATH = \"/scratch/bf996/text-generation-webui/models/llama-65b-hf\"\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(LLAMA_PATH)\n",
    "        config = AutoConfig.from_pretrained(LLAMA_PATH,\n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            load_in_8bit=True)\n",
    "        with init_empty_weights():\n",
    "            base_model = AutoModelForCausalLM.from_config(config)\n",
    "        base_model.tie_weights()\n",
    "        device_map = infer_auto_device_map(base_model, max_memory={0: \"60GiB\", \"cpu\": \"96GiB\"})\n",
    "        base_model = load_checkpoint_and_dispatch(\n",
    "            base_model,\n",
    "            LLAMA_PATH,\n",
    "            device_map=device_map\n",
    "        )\n",
    "    elif model == \"alpaca-13b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "        #tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "        base_model = LlamaForCausalLM.from_pretrained(\n",
    "            #model_path,\n",
    "            \"chavinlo/alpaca-native\",\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_8bit=True,\n",
    "            device_map='auto',\n",
    "        )\n",
    "    elif model == \"alpaca-fine-tuned\":\n",
    "        MAX_LEN = 2048\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    elif model == \"vicuna-13b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"eachadea/vicuna-13b\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"eachadea/vicuna-13b\",\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_8bit=True,\n",
    "            device_map='auto',\n",
    "        )\n",
    "    elif model == \"gpt4-x-alpaca\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"chavinlo/gpt4-x-alpaca\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\"chavinlo/gpt4-x-alpaca\", device_map=\"auto\",\n",
    "                                                          load_in_8bit=True)\n",
    "    elif model == \"t0pp\":\n",
    "        MAX_LEN = 512\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0pp\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\",\n",
    "                                                           torch_dtype=torch.float16, load_in_8bit=True)\n",
    "    elif model == \"flan-t5-xxl\":\n",
    "        MAX_LEN = 512\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\",\n",
    "                                                           torch_dtype=torch.float16, load_in_8bit=True)\n",
    "    elif model == \"flan-ul2\":\n",
    "        MAX_LEN = 512\n",
    "        base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16,\n",
    "                                                                device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "    elif model == \"galpaca-30b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"GeorgiaTechResearchInstitute/galpaca-30b\", device_map=\"auto\",\n",
    "                                                  torch_dtype=torch.float16, load_in_8bit=True)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\"GeorgiaTechResearchInstitute/galpaca-30b\")\n",
    "    elif model == \"opt-iml-max-30b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-iml-max-30b\", use_fast=False, padding_side='left')\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-iml-max-30b\", device_map=\"auto\",\n",
    "                                                          torch_dtype=torch.float16)\n",
    "    if model in [\"flan-t5-xxl\", \"t0pp\", \"flan-ul2\"]:\n",
    "        template = \"\"\"{instruction}\"\"\"\n",
    "    else:\n",
    "        template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "        ### Instruction: \n",
    "        {instruction}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    pt = PromptTemplate(template=template, input_variables=[\"instruction\"])\n",
    "    #Convert length from tokens to characters, leave room for model response\n",
    "    MAX_LEN = MAX_LEN * EST_CHARS_PER_TOKEN - 200\n",
    "    return base_model, tokenizer, template, pt, MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faa69500-a350-4dc9-9204-f5bea462888c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:58.276314Z",
     "start_time": "2025-08-10T12:54:58.257584Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:11.098560Z",
     "iopub.status.busy": "2025-08-18T11:20:11.098043Z",
     "iopub.status.idle": "2025-08-18T11:20:11.119593Z",
     "shell.execute_reply": "2025-08-18T11:20:11.118735Z"
    },
    "papermill": {
     "duration": 0.03553,
     "end_time": "2025-08-18T11:20:11.120802",
     "exception": false,
     "start_time": "2025-08-18T11:20:11.085272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sherlock_resp(df, gt_df, prompt_dict, model, label_indices, base_prompt, lsd):\n",
    "    isd4 = \"d4\" in lsd['name']\n",
    "    if \"sherlock\" in model:\n",
    "        model = sherlock_model\n",
    "        data_m = pd.Series(df[label_indices].astype(str).T.values.tolist())\n",
    "        extract_features(\n",
    "            \"../temporary.csv\",\n",
    "            data_m\n",
    "        )\n",
    "        feature_vectors = pd.read_csv(\"../temporary.csv\", dtype=np.float32)\n",
    "        predicted_labels = model.predict(feature_vectors, \"sherlock\")\n",
    "        iter_len = len(data_m)\n",
    "    elif \"doduo\" in model:\n",
    "        model = doduo_model\n",
    "        data_m = df[label_indices]\n",
    "        try:\n",
    "            annot_m = doduo_model.annotate_columns(data_m)\n",
    "            predicted_labels = annot_m.coltypes\n",
    "        except Exception as e:\n",
    "            print(f\"Exception {e} in Doduo, returning default \\n\")\n",
    "            predicted_labels = [\"text\" for i in range(len(data_m))]\n",
    "        iter_len = len(predicted_labels)\n",
    "    predicted_labels_dict = {i: sherlock_to_cta.get(predicted_labels[i], [predicted_labels[i]]) for i in\n",
    "                             range(iter_len)}\n",
    "\n",
    "    for idx, label_idx in zip(range(iter_len), label_indices):\n",
    "        prompt = base_prompt + \"_\" + str(label_idx)\n",
    "        if isd4:\n",
    "            ans = predicted_labels[0]\n",
    "            label = [s.lower() for s in lsd['d4_map'][gt_df]]\n",
    "        else:\n",
    "            gt_row = gt_df[gt_df['column_index'] == label_idx]\n",
    "            if len(gt_row) != 1:\n",
    "                continue\n",
    "            label = fix_labels(gt_row['label'].item(), lsd)\n",
    "            ans = [fix_labels(item, lsd) for item in predicted_labels_dict[idx]]\n",
    "        if isd4:\n",
    "            res = ans in label\n",
    "        else:\n",
    "            assert isinstance(ans, list), \"ans should be a list\"\n",
    "            res = label in ans\n",
    "        ans_dict = {\"response\": ans, \"context\": None, \"ground_truth\": label, \"correct\": res,\n",
    "                    \"orig_model_label\": predicted_labels[idx]}\n",
    "        prompt_dict[prompt] = ans_dict\n",
    "    return prompt\n",
    "\n",
    "\n",
    "@retry(Exception, tries=3, delay=3)\n",
    "def get_chatgpt_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None,\n",
    "                     method=[\"similarity\"], max_len=15000):\n",
    "    fixed_labels = [fix_labels(s, lsd) for s in lsd['label_set']]\n",
    "    model = \"gpt-3.5\"\n",
    "    context_labels = \", \".join(fixed_labels)\n",
    "    fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n",
    "    prompt = prompt_context_insert(context_labels, context, max_len, \"gpt-3.5\")\n",
    "    d_p = prompt_dict.get(prompt, -1)\n",
    "    if d_p != -1 and \"skip-existing\" in method:\n",
    "        #recompute_results(prompt_dict, prompt, model, cbc_pred=None, label_set=lsd)\n",
    "        return prompt\n",
    "    elif d_p != -1:\n",
    "        while prompt_dict.get(prompt, -1) != -1:\n",
    "            prompt = prompt + \"*\"\n",
    "    if response:\n",
    "        ans = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0,\n",
    "        ).choices[0]['message']['content']\n",
    "        #print(f\"Original ans is {ans}\")\n",
    "    ans_n = fuzzy_label_match(ans, fixed_labels, None, None, prompt, lsd, model, method=method)\n",
    "    #print(f\"Fuzzy ans is {ans_n}\")\n",
    "    res = ans_n == ground_truth\n",
    "    ans_dict = {\"response\": ans_n, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res,\n",
    "                \"original_model_answer\": ans}\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "    return prompt\n",
    "\n",
    "\n",
    "@retry(Exception, tries=5, delay=3)\n",
    "def get_ada_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None):\n",
    "    prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"ada-personal\")\n",
    "    if prompt_dict.get(prompt, -1) != -1:\n",
    "        #recompute_results(prompt_dict, prompt, \"ada-personal\", label_set=lsd)\n",
    "        return prompt\n",
    "    if response:\n",
    "        proc = subprocess.run(\n",
    "            [\"openai\", \"api\", \"completions.create\", \"-m\", \"ada:ft-personal:-2023-03-14-11-52-45\", \"-M\", \"3\", \"-p\",\n",
    "             prompt], capture_output=True, check=True)\n",
    "        ans = proc.stdout.decode(\"utf-8\")[len(prompt):].strip()\n",
    "    else:\n",
    "        ans = \"\"\n",
    "    res = ans.lower().strip().startswith(ground_truth)\n",
    "    ans_dict = {\"response\": ans, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res}\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "    return prompt\n",
    "\n",
    "def call_llama_model(session, link, prompt, lsd, var_params, generated_labels_list=None):\n",
    "    # Build the payload expected by the new LLaMA endpoint\n",
    "    payload = {\n",
    "        \"model\":      \"llama3.1:8b-instruct-q8_0\",\n",
    "        \"prompt\":     prompt,\n",
    "        \"stream\":     False,\n",
    "        \"options\": {\n",
    "            \"seed\": 42,\n",
    "            \"num_predict\": 30,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Choose session-based or direct requests call\n",
    "    client = session or requests\n",
    "    resp = client.post(link, json=payload)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    # Extract the generated text\n",
    "    data = resp.json()\n",
    "    text = data.get(\"response\", \"\")\n",
    "    origina_model_response = text.strip()\n",
    "\n",
    "\n",
    "    #print(\"LLama model was called\")\n",
    "    resolve_response = resolve_label(origina_model_response, generated_labels_list, embed_threshold = 0.70)\n",
    "\n",
    "\n",
    "    print(\"Original model response: \", origina_model_response)\n",
    "    print(\"Resolved response: \", resolve_response)\n",
    "    # Resolved and original model response\n",
    "    return resolve_response, origina_model_response\n",
    "    \n",
    "    #return fix_labels(text.strip(), generated_labels_list)\n",
    "\n",
    "temperature = 0\n",
    "top_p = 0\n",
    "\n",
    "def extract_answer(orig_ans: str) -> str:\n",
    "    \"\"\"\n",
    "    If orig_ans contains 'ANSWER:...', return the text after the colon.\n",
    "    Otherwise, return orig_ans unchanged (stripped).\n",
    "    \"\"\"\n",
    "    m = re.search(r\"ANSWER\\s*:\\s*(.*)\", orig_ans, re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return orig_ans.strip()\n",
    "\n",
    "#generated_labels_list = []\n",
    "\n",
    "def generate_label(session, link, old_prompt, lsd, var_params, generated_labels_list, orig_ans):\n",
    "\n",
    "    if orig_ans.lower() != \"none\" and orig_ans not in generated_labels_list:\n",
    "        generated_labels_list.append(orig_ans)\n",
    "          \n",
    "    return picker_prompt, picked\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "@retry(Exception, tries=3, delay=3)\n",
    "def get_topp_resp(prompt, k):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "\n",
    "    temperature = 0.1 * k\n",
    "    top_p   = 0.90 - (0.1 * k)\n",
    "\n",
    "    outputs = base_model.generate(inputs,\n",
    "                                  max_length=MAX_LEN,\n",
    "                                  #do_sample=False,\n",
    "                                  #num_beams=1\n",
    "                                  temperature=temperature,\n",
    "                                  top_p=top_p,\n",
    "                                  repetition_penalty=1.3\n",
    "                                  )\n",
    "    orig_ans = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return extract_answer(orig_ans)\n",
    "\n",
    "\n",
    "\n",
    "@retry(Exception, tries=3, delay=3)\n",
    "def get_llama_resp(lsd: dict, context: list, ground_truth: str, prompt_dict: dict, link: str, response=True,\n",
    "                   session=None, cbc=None, model=\"llama\", limited_context=None,\n",
    "                   method=[\"ans_contains_gt\", \"gt_contains_ans\", \"resample\"], generated_labels_list = None):\n",
    "    #print(f\"in get llama resp, gt is {ground_truth}, context is {context}\")\n",
    "    isd4 = \"d4\" in lsd['name']\n",
    "    if isd4:\n",
    "        gtv = lsd['d4_map'][ground_truth]\n",
    "        if isinstance(gtv, str):\n",
    "            gtv = [gtv]\n",
    "        ground_truth = [s.lower() for s in gtv]\n",
    "    if \"hierarchical\" in method and not isd4:\n",
    "        dtype = get_base_dtype(limited_context)\n",
    "        fixed_labels = sotab_top_hier[dtype]\n",
    "    else:\n",
    "        fixed_labels = list(set([fix_labels(s, lsd) for s in lsd['label_set']]))\n",
    "    context_labels = \", \".join(fixed_labels)\n",
    "    fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n",
    "    if model in [\"llama-zs\", \"opt-iml-30b-zs\"]:\n",
    "        pipe, local_llm, llm_chain = set_pipeline(k=1)\n",
    "    prompt = prompt_context_insert(context_labels, context, MAX_LEN, model, options = generated_labels_list)\n",
    "    d_p = prompt_dict.get(prompt, -1)\n",
    "    #skip existing logic\n",
    "    if d_p != -1 and \"skip-existing\" in method:\n",
    "        # recompute_results(prompt_dict, prompt, \"llama\", cbc, lsd)\n",
    "        return prompt, prompt_dict[prompt][\"response\"]\n",
    "    elif d_p != -1:\n",
    "        while prompt_dict.get(prompt, -1) != -1:\n",
    "            prompt = prompt + \"*\"\n",
    "    #print(\"GET LLAMA NEETY GREEDY:\")\n",
    "    #print(prompt)\n",
    "    #response logic\n",
    "    original_model_response = \"\"\n",
    "    if not response:\n",
    "        orig_ans = ans_n = \"\"\n",
    "        original_model_response = \"\"\n",
    "    else:\n",
    "        orig_ans = apply_basic_rules(limited_context, None)\n",
    "        if orig_ans is None:\n",
    "            orig_ans, original_model_response = query_correct_model(model, prompt, context_labels, context, session, link, lsd, generated_labels_list)\n",
    "            \n",
    "            #hierarchical matching logic\n",
    "            if \"hierarchical\" in method and dtype == \"other\" and orig_ans not in ['email', 'URL', 'WebHTMLAction',\n",
    "                                                                                  'Photograph']:\n",
    "                next_label_set = sotab_other_hier.get(orig_ans, -1)\n",
    "                if next_label_set == -1:\n",
    "                    print(f\"Original answer {orig_ans} not found in hierarchy\")\n",
    "                    next_label_set = sotab_other_hier['text']\n",
    "                fixed_labels = list(set([fix_labels(s, lsd) for s in next_label_set]))\n",
    "                context_labels = \", \".join(fixed_labels)\n",
    "                fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n",
    "                orig_ans, original_model_response = query_correct_model(model, prompt, context_labels, context, session, link, lsd, generated_labels_list)\n",
    "                #fuzzy matching logic\n",
    "            #print(\"Fuzzy matching logic\")\n",
    "            #print(f\"Fixed LABELSS: {generated_labels_list}\")\n",
    "            #print(f\"Fuzzy prompt: {prompt}\")\n",
    "            #print(f\"Fuzzy lsd: {lsd}\")\n",
    "            ans_n = orig_ans.lower()\n",
    "            original_model_response = original_model_response.lower()\n",
    "    \n",
    "        else:\n",
    "            ans_n = orig_ans.lower()\n",
    "            original_model_response = original_model_response.lower()\n",
    "\n",
    "    print(f\"LLM Picker 1 answer: {ans_n}... Should be none in the beginning\")\n",
    "\n",
    "    if ans_n.lower() != \"none\" and ans_n not in generated_labels_list:\n",
    "        generated_labels_list.append(ans_n)\n",
    "\n",
    "    print(f\"List of label so far: {generated_labels_list}\")\n",
    "\n",
    " \n",
    "\n",
    "    #print(f\"final label set was {fixed_labels}, prediction was {ans_n}, ground truth was {ground_truth} \\n\")\n",
    "    if isd4:\n",
    "        res = ans_n in ground_truth\n",
    "    else:\n",
    "        res = ans_n == ground_truth\n",
    "        \n",
    "    ans_dict = {\"response\": ans_n, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res,\n",
    "                \"original_model_answer\": orig_ans}\n",
    "\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "\n",
    "    #print(f\"Final answer: {ans_n}\")\n",
    "    return prompt, ans_n, original_model_response\n",
    "\n",
    "@retry(Exception, tries=5, delay=3)\n",
    "def get_bloomz_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None):\n",
    "    prompt = prompt_context_insert(context_labels, context, 2000, \"bloomz\")\n",
    "    if prompt_dict.get(prompt, -1) != -1:\n",
    "        return prompt\n",
    "    if response:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda:0\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=5)\n",
    "    else:\n",
    "        response = \"\"\n",
    "    ans = tokenizer.decode(outputs[0]).split()[-1]\n",
    "    ans = ''.join(e for e in ans if e.isalnum()).lower()\n",
    "    res = ans == ground_truth\n",
    "    ans_dict = {\"response\": ans, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res}\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85950645-b679-4139-baed-fcaa30e91aa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:58.384756Z",
     "start_time": "2025-08-10T12:54:58.353705Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:11.151300Z",
     "iopub.status.busy": "2025-08-18T11:20:11.150876Z",
     "iopub.status.idle": "2025-08-18T11:20:11.184434Z",
     "shell.execute_reply": "2025-08-18T11:20:11.183609Z"
    },
    "papermill": {
     "duration": 0.048743,
     "end_time": "2025-08-18T11:20:11.185799",
     "exception": false,
     "start_time": "2025-08-18T11:20:11.137056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_integer(val):\n",
    "    return pd.to_numeric(val, downcast='integer', errors='ignore')\n",
    "\n",
    "\n",
    "def derive_meta_features(col):\n",
    "    features = {}\n",
    "    if not col.astype(str).apply(str.isnumeric).all():\n",
    "        return {\"std\": round(col.astype(str).str.len().std(), 2), \"mean\": round(col.astype(str).str.len().mean(), 2),\n",
    "                \"mode\": col.astype(str).str.len().mode().iloc[0].item(), \"median\": col.astype(str).str.len().median(),\n",
    "                \"max\": col.astype(str).str.len().max(), \"min\": col.astype(str).str.len().min(),\n",
    "                \"rolling-mean-window-4\": [0.0]}\n",
    "    col = col.dropna().astype(float)\n",
    "    if col.apply(float.is_integer).all():\n",
    "        col = col.astype(int)\n",
    "    #print(f\"Collecting metafeatures for column {col} \\n\")\n",
    "    features['std'] = round(col.std(), 2)\n",
    "    features['mean'] = round(col.mean(), 2)\n",
    "    features['mode'] = col.mode().iloc[0].item()\n",
    "    features['median'] = col.median()\n",
    "    features['max'] = col.max()\n",
    "    features['min'] = col.min()\n",
    "    indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=4)\n",
    "    features['rolling-mean-window-4'] = list(col.rolling(window=indexer, min_periods=1).mean())\n",
    "    return features\n",
    "\n",
    "\n",
    "def fix_mode(d):\n",
    "    if isinstance(d['mode'], pd.Series):\n",
    "        d['mode'] = d['mode'].loc[0].item()\n",
    "    return d\n",
    "\n",
    "\n",
    "def split_meta_features(d):\n",
    "    return pd.Series(\n",
    "        [d.get('std', \"N/A\"), d.get('mean', \"N/A\"), d.get('median', \"N/A\"), d.get('mode', \"N/A\"), d.get('max', \"N/A\"),\n",
    "         d.get('min', \"N/A\")])\n",
    "\n",
    "\n",
    "def prompt_context_insert(context_labels: str, context: str, max_len: int = 2000, model: str = \"gpt-3.5\", options: list[str] = None):\n",
    "    if model == \"bloomz\":\n",
    "        s = f'SYSTEM: You are an AI research assistant. You use a tone that is technical and scientific. USER: Please select the field from {context_labels} which best describes the context below. Respond with the name of the field and nothing else. \\n CONTEXT: {context}'\n",
    "    elif model == \"gpt-3.5\":\n",
    "        s = f'SYSTEM: Please select the field from {context_labels} which best describes the context. Respond only with the name of the field. \\n CONTEXT: {context}'\n",
    "    elif model == \"ada-personal\":\n",
    "        s = f'{context}$'\n",
    "    elif model == \"llama-old\":\n",
    "        s = f'INSTRUCTION: Select the field from the category which matches the input. \\n CATEGORIES: {context_labels} \\n INPUT:{context} \\n OUTPUT: '\n",
    "    elif \"-zs\" in model:\n",
    "        ct = \"[\" + \", \".join(context).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")[\n",
    "                   :MAX_LEN - 100 - len(context_labels)] + \"]\"\n",
    "        lb = \"\\n\".join([\"- \" + c for c in context_labels.split(\", \")])\n",
    "        #s = f'How might one classify the following input? \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n ANSWER:'\n",
    "        if model == \"opt-iml-max-30b-zs\":\n",
    "            s = f'Select the option which best describes the input. \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n'\n",
    "        else:\n",
    "            # Original prompt\n",
    "            s = f'INSTRUCTION: Select the option which best describes the input. \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n ANSWER:'\n",
    "            \n",
    "    elif model == \"llama\":\n",
    "        ct = \"[\" + \", \".join(context).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")[\n",
    "                   :MAX_LEN - 100 - len(context_labels)] + \"]\"\n",
    "        lb = \"\\n\".join([\"- \" + c for c in context_labels.split(\", \")])\n",
    "        \n",
    "        #s = f'INSTRUCTION: Select the category which best matches the input. \\n INPUT:{context} .\\n OPTIONS:\\n{lb} \\n CATEGORY: '\n",
    "\n",
    "        #s = f'INSTRUCTION: Select the category which best matches the input. If category is not matching the input return none. Do not provide any further text, only label if it exists or none. \\n INPUT:{context} .\\n OPTIONS:\\n - none \\n CATEGORY: '\n",
    "        #opt_set = options + [\"none\"]\n",
    "        \n",
    "        options_str = \" - \".join(options)\n",
    "        \n",
    "        #if options is not None:\n",
    "        #    options_str = \" - \".join(options)\n",
    "        #else:\n",
    "        #    options_str = \"- none\"\n",
    "            \n",
    "        print(f\"Prompt context insert {options_str}\")\n",
    "\n",
    "        # Picker 1 prompt\n",
    "        s = picker_prompt = llm_prompts(ct, options_str)\n",
    "\n",
    "    elif model == \"llama-retry\":\n",
    "        s = f'INSTRUCTION: Select the category which best matches the input. \\n INPUT:{context} \\n CATEGORY: '\n",
    "    #Truncate if prompt exceeds maximum length\n",
    "    if len(s) > max_len:\n",
    "        s = s[:max_len - 3]\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def recompute_results(prompt_dict, prompt, model_str, cbc_pred, label_set):\n",
    "    dict_val = prompt_dict.get(prompt, -1)\n",
    "    dict_val['cbc_pred'] = cbc_pred\n",
    "    if model_str == \"llama\":\n",
    "        if cbc_pred and (cbc_pred in catboost_cats):\n",
    "            print(f\"using cbcpred label: {cbc_pred} \\n\")\n",
    "            dict_val['response'] = fix_labels(cbc_pred, label_set)\n",
    "        dict_val['correct'] = ((dict_val['ground_truth'] == dict_val['response']) or (\n",
    "                    dict_val['response'] and (dict_val['response']) in dict_val['ground_truth']))\n",
    "    prompt_dict[prompt] = dict_val\n",
    "\n",
    "\n",
    "def make_json(prompt, var_params):\n",
    "    p = deepcopy(params)\n",
    "    if var_params:\n",
    "        for k, v in var_params.items():\n",
    "            p[k] = v\n",
    "    return {\n",
    "        \"data\": [\n",
    "            prompt,\n",
    "            p['max_new_tokens'],\n",
    "            p['do_sample'],\n",
    "            p['temperature'],\n",
    "            p['top_p'],\n",
    "            p['typical_p'],\n",
    "            p['repetition_penalty'],\n",
    "            p['encoder_repetition_penalty'],\n",
    "            p['top_k'],\n",
    "            p['min_length'],\n",
    "            p['no_repeat_ngram_size'],\n",
    "            p['num_beams'],\n",
    "            p['penalty_alpha'],\n",
    "            p['length_penalty'],\n",
    "            p['early_stopping'],\n",
    "            p['seed'],\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def ans_contains_gt(ans_n, fixed_labels):\n",
    "    for fixed_label in fixed_labels:\n",
    "        if fixed_label in ans_n:\n",
    "            print(f\"Fuzzy label {ans_n} contains gt label {fixed_label}: MATCH \\n\")\n",
    "            ans_n = fixed_label\n",
    "            return ans_n\n",
    "    return None\n",
    "\n",
    "\n",
    "def gt_contains_ans(ans_n, fixed_labels):\n",
    "    if ans_n == \"\":\n",
    "        return None\n",
    "    for fixed_label in fixed_labels:\n",
    "        if ans_n in fixed_label:\n",
    "            print(f\"GT label {fixed_label} contains fuzzy label {ans_n}: MATCH \\n\")\n",
    "            ans_n = fixed_label\n",
    "            return ans_n\n",
    "    return None\n",
    "\n",
    "\n",
    "def basic_contains(ans_n, fixed_labels, method):\n",
    "    #TODO: not sure the order should be fixed like this, could be made flexible\n",
    "    if ans_n in fixed_labels:\n",
    "        return ans_n\n",
    "    if \"ans_contains_gt\" in method:\n",
    "        res = ans_contains_gt(ans_n, fixed_labels)\n",
    "        if res:\n",
    "            return res\n",
    "    if \"gt_contains_ans\" in method:\n",
    "        res = gt_contains_ans(ans_n, fixed_labels)\n",
    "        if res:\n",
    "            return res\n",
    "    return None\n",
    "\n",
    "\n",
    "def fuzzy_label_match(orig_ans, fixed_labels, session, link, prompt, lsd, model,\n",
    "                      method=[\"ans_contains_gt\", \"gt_contains_ans\", \"resample\"]):\n",
    "\n",
    "    #answer is already in label set, no fuzzy match needed\n",
    "    ans_n = fix_labels(orig_ans, lsd)\n",
    "    res = basic_contains(ans_n, fixed_labels, method)\n",
    "    if res:\n",
    "        return res\n",
    "    if \"similarity\" in method:\n",
    "        ans_embedding = sent_model.encode(ans_n)\n",
    "        lbl_embeddings = sent_model.encode(fixed_labels)\n",
    "        sims = {lbl: util.pytorch_cos_sim(ans_embedding, le) for lbl, le in zip(fixed_labels, lbl_embeddings)}\n",
    "        return max(sims, key=sims.get)\n",
    "    if \"resample\" in method:\n",
    "        #fuzzy label matching strategy\n",
    "        for k in range(2, 6):\n",
    "            if \"gpt\" in model:\n",
    "                ans_n = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    temperature=0 + k / 10,\n",
    "                ).choices[0]['message']['content'].lower()\n",
    "            elif model in [\"llama-zs\", \"opt-iml-30b-zs\"]:\n",
    "                pipe, local_llm, llm_chain = set_pipeline(k=k)\n",
    "                ans_n = llm_chain.run(prompt)\n",
    "            elif model in [\"topp-zs\", \"flan-ul2-zs\"]:\n",
    "                ans_n = get_topp_resp(prompt, k)\n",
    "            else:\n",
    "                rep_pen = params['repetition_penalty']\n",
    "                top_p = params['top_p']\n",
    "                temp = params['temperature']\n",
    "                ans_n = call_llama_model(session, link, prompt, lsd,\n",
    "                                         {'no_repeat_ngram_size': 1, 'top_p': top_p - (0.1 * k), 'temperature': 0.9})\n",
    "                params['top_p'] = top_p\n",
    "                params['temperature'] = temp\n",
    "            res = basic_contains(ans_n, fixed_labels, method)\n",
    "            if res:\n",
    "                return res\n",
    "    #print(\"Applying fallback label, 'text' \\n\")\n",
    "    return 'text'\n",
    "\n",
    "\n",
    "INTEGER_SET = set(r\"0123456789,/\\+-.^_()[] :\")\n",
    "\n",
    "\n",
    "def get_base_dtype(context):\n",
    "    dtype = \"integer\"\n",
    "    for item in context:\n",
    "        if not all(char in INTEGER_SET for char in item):\n",
    "            #print(f\"String is OTHER because: {[char for char in item if char not in INTEGER_SET]}\")\n",
    "            return \"other\"\n",
    "        try:\n",
    "            if item.endswith(\".0\") or item.endswith(\",0\"):\n",
    "                item = item[:-2]\n",
    "                item = str(int(item))\n",
    "            if item.endswith(\".00\") or item.endswith(\",00\"):\n",
    "                item = item[:-3]\n",
    "                item = str(int(item))\n",
    "        except:\n",
    "            return \"float\"\n",
    "        temp_item = re.sub(r\"[^a-zA-Z0-9.]\", \"\", item)\n",
    "        if not temp_item.isdigit():\n",
    "            #print(f\"string is FLOAT because {temp_item} is not an integer\")\n",
    "            dtype = \"float\"\n",
    "    return dtype\n",
    "\n",
    "\n",
    "def query_correct_model(model, prompt, context_labels, context, session, link, lsd, generated_labels_list = None):\n",
    "    original_model_response = \"\"\n",
    "    if model in [\"llama-zs\", \"opt-iml-max-30b-zs\"]:\n",
    "        orig_ans = llm_chain.run(prompt)\n",
    "        if orig_ans is None:\n",
    "            prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"llama-retry\")\n",
    "            orig_ans = llm_chain.run(prompt)\n",
    "    elif model in [\"topp-zs\", \"flan-ul2-zs\"]:\n",
    "        orig_ans = get_topp_resp(prompt, 1)\n",
    "    else:\n",
    "        orig_ans, original_model_response = call_llama_model(session, link, prompt, lsd, None, generated_labels_list)\n",
    "        if orig_ans is None:\n",
    "            prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"llama-retry\")\n",
    "            orig_ans, original_model_response = call_llama_model(session, link, prompt, lsd, None, generated_labels_list)\n",
    "    return orig_ans, original_model_response\n",
    "\n",
    "\n",
    "def get_df_sample_col(col, rand_seed, len_context, min_variance=2, replace=False):\n",
    "    df = pd.Series(col)\n",
    "    ignore_list = [\"None\", 'none', 'NaN', 'nan', 'N/A', 'na', '']\n",
    "    sample_list = list(set(p[:75] for p in pd.unique(df.astype(str)[col]) if p not in ignore_list))\n",
    "    if len(sample_list) < 1:\n",
    "        return [\"None\"] * len_context\n",
    "    if len(sample_list) < len_context:\n",
    "        sample_list = sample_list * len_context\n",
    "    if len(sample_list) > len_context:\n",
    "        sample_list = sample_list[:len_context]\n",
    "    assert len(sample_list) == len_context, f\"An index in val_indices is length {len(sample_list)}\"\n",
    "    return sample_list\n",
    "\n",
    "\n",
    "def check_substr_contains_only_set(str, acceptable_chars):\n",
    "    validation = set(str)\n",
    "    print(\"Checking if it contains only \", acceptable_chars)\n",
    "    if validation.issubset(acceptable_chars):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def insert_source(context, fname):\n",
    "    pattern = r\"_([^_]*)_\"  # Matches substrings that start and end with \"_\"\n",
    "    matcher = re.search(pattern, fname)\n",
    "    addstr = str(matcher.group()).replace(\"_\", \"\").split(\".\")[0]\n",
    "    #context.insert(0, \"SRC_FILE: \" + addstr + \"COL_VALS: \")\n",
    "    context.insert(0, \"SRC: \" + addstr)\n",
    "    return context\n",
    "\n",
    "\n",
    "def get_df_sample(df, rand_seed, val_indices, len_context, min_variance=1, replace=False, full=False, other_col=False,\n",
    "                  max_len=8000):\n",
    "    column_samples = {}\n",
    "    ignore_list = [\"None\", 'none', 'NaN', 'nan', 'N/A', 'na', '']\n",
    "    for col in df.columns:\n",
    "        sample_list = list(\n",
    "            set(p[:max_len // (len_context * 3)] for p in pd.unique(df.astype(str)[col]) if p not in ignore_list))\n",
    "        #reformat integer samples\n",
    "        sl_mod = []\n",
    "        # Meta-features\n",
    "        if full:\n",
    "            meta_features = derive_meta_features(df[col])\n",
    "            meta_features['rolling-mean-window-4'] = meta_features['rolling-mean-window-4'][:5]\n",
    "        # Sampling from other columns\n",
    "        if other_col:\n",
    "            sample_list_fill_size = len_context - len(sample_list)\n",
    "            nc = len(df.columns)\n",
    "            per_column_context = max(1, sample_list_fill_size // nc)\n",
    "            for idx, oc in enumerate(df.columns):\n",
    "                items = df[oc].astype(str).iloc[0:per_column_context].tolist()\n",
    "                sample_list = sample_list + [\"OC: \" + str(item) for item in items]\n",
    "        if not sample_list:\n",
    "            sample_list = [\"None\"]\n",
    "        if len(sample_list) < len_context:\n",
    "            sample_list = sample_list * len_context\n",
    "        if len(sample_list) > len_context:\n",
    "            sample_list = sample_list[:len_context]\n",
    "        assert len(sample_list) == len_context, \"An index in val_indices is length \" + str(len(sample_list))\n",
    "        if full:\n",
    "            if meta_features['std'] == \"N/A\":\n",
    "                sample_list = sample_list + [\"\" for k, v in meta_features.items()]\n",
    "            else:\n",
    "                sample_list = sample_list + [str(k) + \": \" + str(v) for k, v in meta_features.items()]\n",
    "        # print(\"sample list\")\n",
    "        # print(sample_list)\n",
    "        column_samples[col] = sample_list\n",
    "        # print(\"column samples\")\n",
    "        # print(column_samples)\n",
    "    return pd.DataFrame.from_dict(column_samples)\n",
    "\n",
    "\n",
    "NUMERIC_AND_COMMA = set('0123456789,')\n",
    "\n",
    "BOOLEAN_SET = [\"True\", \"true\", \"False\", \"false\", \"yes\", \"Yes\", \"No\", \"no\"]\n",
    "\n",
    "\n",
    "def apply_basic_rules(context, lbl):\n",
    "    if not context:\n",
    "        return lbl\n",
    "    if not isinstance(context, list):\n",
    "        return lbl\n",
    "    try:\n",
    "        if all(s.endswith(\" g\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" kg\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" lb\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" lbs\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" pounds\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" cal\") for s in context):\n",
    "            lbl = \"calories\"\n",
    "        if all(s.endswith(\" kcal\") for s in context):\n",
    "            lbl = \"calories\"\n",
    "        if all(s.endswith(\" calories\") for s in context):\n",
    "            lbl = \"calories\"\n",
    "        if all(\"review\" in s.lower() for s in context):\n",
    "            lbl = \"review\"\n",
    "        if all(\"recipe\" in s.lower() for s in context):\n",
    "            lbl = \"recipe\"\n",
    "        if lbl and \"openopen\" in lbl:\n",
    "            lbl = \"openinghours\"\n",
    "        if all(s in BOOLEAN_SET for s in context):\n",
    "            lbl = \"medical_boolean\"\n",
    "        return lbl\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} in apply_basic_rules with context {context}\")\n",
    "        return lbl\n",
    "\n",
    "\n",
    "def get_cbc_pred(orig_label, numeric_labels):\n",
    "    try:\n",
    "        #FOR VALIDATION\n",
    "        #cbc_filematch = dfv[dfv['df_path'] == str(f)]\n",
    "        #FOR TEST SET\n",
    "        cbc_filematch = dft[dft['df_path'] == str(f)]\n",
    "        cbc_labelmatch = cbc_filematch[cbc_filematch['label'] == orig_label]\n",
    "        if len(cbc_labelmatch) == 1:\n",
    "            cbc_pred = numeric_labels[cbc_labelmatch['preds'].item()]\n",
    "        else:\n",
    "            cbc_pred = None\n",
    "    except Exception as e:\n",
    "        print(\"cbc excpetion: \")\n",
    "        print(e)\n",
    "        cbc_pred = None\n",
    "\n",
    "\n",
    "def run_val(model: str, save_path: str, inputs: list, label_set: list, input_df: pd.DataFrame, resume: bool = True,\n",
    "            results: bool = True, stop_early: int = -1, rand_seed: int = 13, sample_size: int = 5, link: str = None,\n",
    "            response: bool = True, summ_stats: bool = False, table_src: bool = False, other_col: bool = False,\n",
    "            skip_short: bool = False, min_var: int = 0, method: list = [\"similarity\"]):\n",
    "    inputs = [Path(f) for f in inputs]\n",
    "\n",
    "    infmods = \"sherlock\" in model or \"doduo\" in model\n",
    "    isd4 = \"d4\" in label_set['name']\n",
    "    if resume and os.path.isfile(save_path):\n",
    "        with open(save_path, 'r', encoding='utf-8') as f:\n",
    "            prompt_dict = json.load(f)\n",
    "    else:\n",
    "        prompt_dict = {}\n",
    "    s = requests.Session()\n",
    "    if \"-zs\" in model:\n",
    "        base_model.eval()\n",
    "    if isinstance(inputs, dict):\n",
    "        labels = [\"_\".join(k.split(\"_\")[:-1]) for k in inputs.keys()]\n",
    "        inputs = list(inputs.values())\n",
    "    for idx, f in tqdm(enumerate(inputs), total=len(inputs)):\n",
    "        if idx % 100 == 0:\n",
    "            with open(save_path, 'w', encoding='utf-8') as alt_f:\n",
    "                #print(\"pd\", prompt_dict, \"\\n\")\n",
    "                json.dump(prompt_dict, alt_f, ensure_ascii=False, indent=4)\n",
    "        if stop_early > -1 and idx == stop_early:\n",
    "            break\n",
    "        if isd4:\n",
    "            f_df = f\n",
    "            label_indices = [2]\n",
    "            gt_labels = labels[idx]\n",
    "        else:\n",
    "            gt_labels = input_df[input_df['table_name'] == f.name]\n",
    "            label_indices = gt_labels['column_index'].unique().tolist()\n",
    "\n",
    "            if f.suffix.lower() == '.csv':\n",
    "                f_df = pd.read_csv(f)\n",
    "            else:\n",
    "                f_df = pd.read_json(f, compression='infer', lines=True)\n",
    "\n",
    "        if infmods:\n",
    "            label_indices = [\"values\"]\n",
    "            key = get_sherlock_resp(f_df, gt_labels, prompt_dict, model, label_indices, str(f), label_set)\n",
    "            continue\n",
    "        sample_df = get_df_sample(f_df, rand_seed, label_indices, sample_size, full=summ_stats, other_col=other_col,\n",
    "                                  max_len=MAX_LEN)\n",
    "        #print(f\"in main loop, sample_df is {sample_df}\")\n",
    "        f_df_cols = f_df.columns\n",
    "        for idx, col in enumerate(f_df_cols):\n",
    "            if idx not in label_indices:\n",
    "                continue\n",
    "            #NOTE: skipping evaluation for columns with insufficient variance in the column\n",
    "            #       if len(pd.unique(sample_df.astype(str)[col])) < min_var:\n",
    "            #         continue\n",
    "            if isd4:\n",
    "                orig_label = gt_labels\n",
    "            else:\n",
    "                gt_row = gt_labels[gt_labels['column_index'] == idx]\n",
    "                orig_label = gt_row['label'].item()\n",
    "            label = fix_labels(orig_label, label_set)\n",
    "            limited_context = sample_df[col].tolist()[:sample_size]\n",
    "            #NOTE: could consider using min_var here\n",
    "            #if full and len(pd.unique(sample_df[col].tolist())) < 3:\n",
    "            if table_src:\n",
    "                context = insert_source(sample_df[col].tolist(), f.name)\n",
    "            else:\n",
    "                context = sample_df[col].tolist()\n",
    "            if \"gpt-3.5\" in model:\n",
    "                key = get_chatgpt_resp(label_set, context, label, prompt_dict, response=response, session=s,\n",
    "                                       method=method)\n",
    "            elif \"ada-personal\" in model:\n",
    "                key = get_ada_resp(label_set, context, label, prompt_dict, response=response, session=s)\n",
    "            elif \"bloomz\" in model:\n",
    "                key = get_bloomz_resp(label_set, context, label, prompt_dict, response=response, session=s)\n",
    "            elif \"llama\" in model or \"-zs\" in model:\n",
    "                #cbc_pred = get_cbc_pred(orig_label, numeric_labels)\n",
    "                cbc_pred = None\n",
    "                key = get_llama_resp(label_set, context, label, prompt_dict, link=link, response=response, session=s,\n",
    "                                     cbc=cbc_pred, model=model, limited_context=limited_context, method=method)\n",
    "                # print(\"Key: \", key, \"\\n\")\n",
    "                #print(\"pdk\", prompt_dict[key], \"\\n\")\n",
    "            prompt_dict[key]['original_label'] = orig_label\n",
    "            prompt_dict[key]['file+idx'] = str(f) + \"_\" + str(idx)\n",
    "    with open(save_path, 'w', encoding='utf-8') as my_f:\n",
    "        json.dump(prompt_dict, my_f, ensure_ascii=False, indent=4)\n",
    "    if results:\n",
    "        results_checker(save_path, skip_duplicates=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "392f663e-4f8f-4671-ab88-21575e2fffbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:58.453678Z",
     "start_time": "2025-08-10T12:54:58.451500Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:11.210466Z",
     "iopub.status.busy": "2025-08-18T11:20:11.210259Z",
     "iopub.status.idle": "2025-08-18T11:20:11.213641Z",
     "shell.execute_reply": "2025-08-18T11:20:11.212698Z"
    },
    "papermill": {
     "duration": 0.0169,
     "end_time": "2025-08-18T11:20:11.214849",
     "exception": false,
     "start_time": "2025-08-18T11:20:11.197949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02993b81-f6b9-4b7b-b14f-a0ba901faef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:58.579095Z",
     "start_time": "2025-08-10T12:54:58.560714Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:11.239683Z",
     "iopub.status.busy": "2025-08-18T11:20:11.239425Z",
     "iopub.status.idle": "2025-08-18T11:20:11.257078Z",
     "shell.execute_reply": "2025-08-18T11:20:11.256148Z"
    },
    "papermill": {
     "duration": 0.031858,
     "end_time": "2025-08-18T11:20:11.258539",
     "exception": false,
     "start_time": "2025-08-18T11:20:11.226681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from statistics import mean\n",
    "\n",
    "ENDINGS = [\"ANSWER:\", \"CATEGORY:\"]\n",
    "\n",
    "\n",
    "def results_checker_doduo(file_name, skip_duplicates=True):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "    correct = 0\n",
    "    n = len(d)\n",
    "    per_class_results = dict()\n",
    "    for k, v in d.items():\n",
    "        response_set = set(v[\"response\"])\n",
    "        for r in response_set:\n",
    "            per_class_results.setdefault(r, {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"Total\": 0})\n",
    "        per_class_results.setdefault(v[\"ground_truth\"], {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"Total\": 0})\n",
    "        if v['correct'] == True:\n",
    "            correct += 1\n",
    "            per_class_results[v[\"ground_truth\"]][\"TP\"] += 1\n",
    "        else:\n",
    "            per_class_results[v[\"ground_truth\"]][\"FN\"] += 1\n",
    "            for r in response_set:\n",
    "                per_class_results[r][\"FP\"] += 1\n",
    "        per_class_results[v[\"ground_truth\"]][\"Total\"] += 1\n",
    "\n",
    "    for k, v in per_class_results.items():\n",
    "        v['F1'] = (2 * v[\"TP\"]) / (2 * v[\"TP\"] + v[\"FP\"] + v[\"FN\"])\n",
    "\n",
    "    weighted_f1 = sum([v[\"F1\"] * v[\"Total\"] for k, v in per_class_results.items()]) / n\n",
    "    unweighted_f1 = mean([v[\"F1\"] for k, v in per_class_results.items()])\n",
    "\n",
    "    print(\n",
    "        f\"Total entries: {n} \\n Accuracy: {round(correct / n, 4)} \\n Weighted F1: {round(weighted_f1, 4)} \\n Unweighted F1: {round(unweighted_f1, 4)}\")\n",
    "\n",
    "\n",
    "def results_checker(file_name, skip_duplicates=True):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    if skip_duplicates:\n",
    "        d = {k: v for k, v in d.items() if \"CATEGORY: *\" not in str(k)}\n",
    "\n",
    "    # build the lists\n",
    "    y_true = [v[\"ground_truth\"] for v in d.values()]\n",
    "    y_pred = [v[\"response\"] for v in d.values()]\n",
    "\n",
    "    # overall stats\n",
    "    correct = sum(1 for gt, pred in zip(y_true, y_pred) if gt == pred)\n",
    "    n = len(y_true)\n",
    "    print(f\"Total entries: {n}\")\n",
    "    print(f\"Accuracy:     {correct / n:.4f}\\n\")\n",
    "\n",
    "    # per-class report\n",
    "    print(classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        digits=4,  # 4 decimal places\n",
    "        zero_division=0  # to avoid warnings if a class is never predicted\n",
    "    ))\n",
    "\n",
    "    # --- new: build a flattened metrics dict ---\n",
    "    raw_report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    flat = {}\n",
    "    # raw_report has keys for each label, plus 'macro avg', 'weighted avg', and 'accuracy'\n",
    "    for label, m in raw_report.items():\n",
    "        if label == \"accuracy\":\n",
    "            flat[\"accuracy\"] = m\n",
    "        else:\n",
    "            for metric_name, val in m.items():\n",
    "                flat[f\"{label}_{metric_name}\"] = val\n",
    "\n",
    "    # add summary fields\n",
    "    flat[\"total_entries\"] = n\n",
    "    # filename identifier: take it from your JSON filename variable\n",
    "    flat[\"run_name\"]      = os.path.basename(file_name).replace(\".json\",\"\")\n",
    "\n",
    "    # convert to one-row DataFrame\n",
    "    df = pd.DataFrame([flat])\n",
    "\n",
    "    metrics_csv = f\"{archetype_directory}/all_metrics.csv\"\n",
    "\n",
    "    # append (or create) the master CSV\n",
    "    if not os.path.isfile(metrics_csv):\n",
    "        df.to_csv(metrics_csv, index=False, float_format=\"%.4f\")\n",
    "    else:\n",
    "        df.to_csv(metrics_csv, mode=\"a\", header=False, index=False, float_format=\"%.4f\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8db2960f-ac1d-4199-9cbc-85b6d60f66e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:58.657293Z",
     "start_time": "2025-08-10T12:54:58.652101Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:11.283579Z",
     "iopub.status.busy": "2025-08-18T11:20:11.283011Z",
     "iopub.status.idle": "2025-08-18T11:20:11.289101Z",
     "shell.execute_reply": "2025-08-18T11:20:11.288081Z"
    },
    "papermill": {
     "duration": 0.020197,
     "end_time": "2025-08-18T11:20:11.290587",
     "exception": false,
     "start_time": "2025-08-18T11:20:11.270390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def missing_entries(f1, f2):\n",
    "    with open(f1, \"r\") as file1:\n",
    "        d1 = json.load(file1)\n",
    "    with open(f2, \"r\") as file2:\n",
    "        d2 = json.load(file2)\n",
    "    paths1 = set([v[\"file+idx\"] for _, v in d1.items()])\n",
    "    paths2 = set([v[\"file+idx\"] for _, v in d2.items()])\n",
    "    return paths1 - paths2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23213247-0e7a-4b24-9eb5-a623264183ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:58.755225Z",
     "start_time": "2025-08-10T12:54:58.733209Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:11.316060Z",
     "iopub.status.busy": "2025-08-18T11:20:11.315759Z",
     "iopub.status.idle": "2025-08-18T11:20:11.339218Z",
     "shell.execute_reply": "2025-08-18T11:20:11.338180Z"
    },
    "papermill": {
     "duration": 0.038134,
     "end_time": "2025-08-18T11:20:11.340728",
     "exception": false,
     "start_time": "2025-08-18T11:20:11.302594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_val_parquet(\n",
    "        model: str,\n",
    "        save_path: str,\n",
    "        labels_path: str,\n",
    "        data_path: str,\n",
    "        label_set: dict,\n",
    "        resume: bool = True,\n",
    "        results: bool = True,\n",
    "        stop_early: int = -1,\n",
    "        rand_seed: int = 13,\n",
    "        sample_size: int = 5,\n",
    "        link: str = None,\n",
    "        response: bool = True,\n",
    "        summ_stats: bool = False,\n",
    "        table_src: bool = False,\n",
    "        other_col: bool = False,\n",
    "        skip_short: bool = False,\n",
    "        min_var: int = 0,\n",
    "        method: list = [\"similarity\"],\n",
    "        results_checker=None,\n",
    "        MAX_LEN: int = 1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Validation loop adapted for parquet-based inputs:\n",
    "\n",
    "    - labels_path: path to a parquet file with columns ['__index_level_0__', 'type']\n",
    "    - data_path:   path to a parquet file with columns ['__index_level_0__', 'values']\n",
    "    - label_set:   dict containing 'name', 'label_set', 'dict_map', 'abbrev_map'\n",
    "\n",
    "    Each row in the merged DataFrame represents one column to predict:\n",
    "      - __index_level_0__ (column index)\n",
    "      - type (ground truth label)\n",
    "      - values (comma-separated or list of column values)\n",
    "    \"\"\"\n",
    "\n",
    "    total_inference_time = 0.0\n",
    "    inference_times = []\n",
    "\n",
    "    # llm output directory\n",
    "    llm_response_output_path = os.path.join(run_all_directory, \"temp-results\", \"llm-outputs.txt\")\n",
    "\n",
    "    # if directory exist\n",
    "    os.makedirs(os.path.dirname(llm_response_output_path), exist_ok=True)\n",
    "\n",
    "    # remove any existing files in the directory\n",
    "    if os.path.isfile(llm_response_output_path):\n",
    "        os.remove(llm_response_output_path)\n",
    "\n",
    "    generated_labels_list = []\n",
    "        \n",
    "    \n",
    "    # Load or initialize cache\n",
    "    if resume and os.path.isfile(save_path):\n",
    "        with open(save_path, 'r', encoding='utf-8') as f:\n",
    "            prompt_dict = json.load(f)\n",
    "    else:\n",
    "        prompt_dict = {}\n",
    "    \n",
    "\n",
    "    # Read parquet inputs and bring index into a column\n",
    "    # Read parquet inputs and bring index into a column\n",
    "    labels_df = pd.read_parquet(labels_path).reset_index()\n",
    "    data_df = pd.read_parquet(data_path).reset_index()\n",
    "\n",
    "    # Identify the index column name (either __index_level_0__ or generic index)\n",
    "    labels_idx_col = '__index_level_0__' if '__index_level_0__' in labels_df.columns else 'index'\n",
    "    data_idx_col = '__index_level_0__' if '__index_level_0__' in data_df.columns else 'index'\n",
    "\n",
    "    # Rename for clarity: index → col_idx, type → label, values stays values\n",
    "    labels_df = labels_df.rename(columns={labels_idx_col: 'col_idx', 'type': 'label'})\n",
    "    data_df = data_df.rename(columns={data_idx_col: 'col_idx', 'values': 'values'})\n",
    "\n",
    "    # Remap labels using LABEL_MAP_LC\n",
    "    # assumes remap_labels(series, mapping) is defined and LABEL_MAP_LC is available\n",
    "\n",
    "    #labels_df['label'] = remap_labels(labels_df['label'], LABEL_MAP_LC)\n",
    "\n",
    "    # Filter out __none__ labels\n",
    "    labels_df = labels_df[labels_df['label'] != \"__none__\"]\n",
    "\n",
    "    # Merge on column index\n",
    "    merged = pd.merge(labels_df, data_df, on='col_idx', how='inner')\n",
    "\n",
    "    # Prepare session and model\n",
    "    session = requests.Session()\n",
    "    if \"-zs\" in model:\n",
    "        base_model.eval()\n",
    "\n",
    "    # Iterate over each column instance\n",
    "    for idx, row in tqdm(enumerate(merged.itertuples(index=False)), total=len(merged)):\n",
    "        \n",
    "        # Periodic cache save\n",
    "        if idx % 100 == 0:\n",
    "            with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(prompt_dict, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        if stop_early > -1 and idx == stop_early:\n",
    "            break\n",
    "\n",
    "        col_idx = row.col_idx\n",
    "        orig_label = row.label\n",
    "        raw_vals = row.values\n",
    "\n",
    "        # Parse raw values into a list\n",
    "        if isinstance(raw_vals, str):\n",
    "            vals = raw_vals.split(',')\n",
    "        else:\n",
    "            vals = list(raw_vals)\n",
    "\n",
    "        # Deduplicate and sample\n",
    "        vals = [str(x) for x in vals]\n",
    "        unique_vals = pd.unique(vals)\n",
    "        context_list = unique_vals.tolist()[:sample_size]\n",
    "\n",
    "        # Build context\n",
    "        if table_src:\n",
    "            context = insert_source(context_list, str(col_idx))\n",
    "        else:\n",
    "            context = context_list\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        # Model call\n",
    "        if \"gpt-3.5\" in model:\n",
    "            key = get_chatgpt_resp(label_set, context, orig_label,\n",
    "                                   prompt_dict, response=response,\n",
    "                                   session=session, method=method)\n",
    "        elif \"ada-personal\" in model:\n",
    "            key = get_ada_resp(label_set, context, orig_label,\n",
    "                               prompt_dict, response=response,\n",
    "                               session=session)\n",
    "        elif \"bloomz\" in model:\n",
    "            key = get_bloomz_resp(label_set, context, orig_label,\n",
    "                                  prompt_dict, response=response,\n",
    "                                  session=session)\n",
    "        else:\n",
    "            raw_prompt, answer, original_model_response = get_llama_resp(label_set, context, orig_label,\n",
    "                                 prompt_dict, link=link,\n",
    "                                 response=response,\n",
    "                                 session=session,\n",
    "                                 cbc=None,\n",
    "                                 model=model,\n",
    "                                 limited_context=context_list,\n",
    "                                 method=method,\n",
    "                                 generated_labels_list = generated_labels_list)\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        # Record metadata\n",
    "        prompt_dict[raw_prompt]['original_label'] = orig_label\n",
    "        prompt_dict[raw_prompt]['file+idx'] = str(col_idx)\n",
    "\n",
    "        inference_time = end_time - start_time\n",
    "        inference_times.append(inference_time)\n",
    "        total_inference_time += inference_time\n",
    "\n",
    "        # but you also get to see the actual label answer:\n",
    "        print(\"PROMPT SENT:\\n\", raw_prompt)\n",
    "        print(\"MODEL ANSWER:\", answer)\n",
    "\n",
    "        with open(llm_response_output_path, \"a\", encoding='utf-8') as f:\n",
    "            f.write(f\"---\\n\")\n",
    "            f.write(f\"PROMPT SENT: {raw_prompt}\\n\")\n",
    "            f.write(f\"Original model answer: {original_model_response}\\n\")\n",
    "            f.write(f\"Final ANSWER: {answer}\\n\")\n",
    "\n",
    "    n_calls = len(inference_times)\n",
    "    if n_calls > 0:\n",
    "        print(f\"Total inference time: {total_inference_time:.2f}s over {n_calls} calls\")\n",
    "        print(f\"  → average per call: {total_inference_time/n_calls:.3f}s\")\n",
    "        print(f\"  → min / max per call: {min(inference_times):.3f}s / {max(inference_times):.3f}s\")\n",
    "\n",
    "    # Final cache save\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(prompt_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Optional result summary\n",
    "    #if results and results_checker is not None:\n",
    "    #    results_checker(save_path, skip_duplicates=False)\n",
    "\n",
    "    return prompt_dict, total_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbb07a01-53a9-4639-b197-fb24a0479456",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:54:58.817286Z",
     "start_time": "2025-08-10T12:54:58.813119Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:11.365753Z",
     "iopub.status.busy": "2025-08-18T11:20:11.365451Z",
     "iopub.status.idle": "2025-08-18T11:20:11.370230Z",
     "shell.execute_reply": "2025-08-18T11:20:11.369347Z"
    },
    "papermill": {
     "duration": 0.018887,
     "end_time": "2025-08-18T11:20:11.371621",
     "exception": false,
     "start_time": "2025-08-18T11:20:11.352734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_set = {\n",
    "  \"name\": \"custom_csv\",     # any string that does NOT contain \"d4\"\n",
    "  \"label_set\": LABELS,      # the list of your labels, used by similarity\n",
    "  \"dict_map\": { lab: lab for lab in LABELS },\n",
    "  \"abbrev_map\": {}          # or your real abbrev map\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cd83aaa-8825-403e-9f26-b62e8fd0765e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:56:19.485167Z",
     "start_time": "2025-08-10T12:54:58.879646Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:20:11.396644Z",
     "iopub.status.busy": "2025-08-18T11:20:11.396358Z",
     "iopub.status.idle": "2025-08-18T11:21:21.038010Z",
     "shell.execute_reply": "2025-08-18T11:21:21.037257Z"
    },
    "papermill": {
     "duration": 69.655446,
     "end_time": "2025-08-18T11:21:21.039063",
     "exception": false,
     "start_time": "2025-08-18T11:20:11.383617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac250447b7484252a4984790c244865c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1300962/2036495029.py:112: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unique_vals = pd.unique(vals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt context insert \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [benv-21955-ben-337, benv-22716-ben-965, benv-22146-ben-503, benv-21955-ben-340, benv-22146-ben-502]\n",
      "                OPTIONS: \n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [no]\n",
      "                OPTIONS: patient_ids\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_status\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [diednovhep, alive, stilltreatment]\n",
      "                OPTIONS: patient_ids - medical_boolean\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [no, yes]\n",
      "                OPTIONS: patient_ids - medical_boolean\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  dates\n",
      "Resolved response:  dates\n",
      "LLM Picker 1 answer: dates... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [2020-06-26, 2020-07-01, 2020-06-12, 2020-06-04, 2020-06-21]\n",
      "                OPTIONS: patient_ids - medical_boolean\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: dates\n",
      "Prompt context insert patient_ids - medical_boolean - dates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  dates\n",
      "Resolved response:  dates\n",
      "LLM Picker 1 answer: dates... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [2022-04-08, 2022-04-26, 2022-04-20, 2022-04-15, 2022-05-15]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: dates\n",
      "Prompt context insert patient_ids - medical_boolean - dates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  dates\n",
      "Resolved response:  dates\n",
      "LLM Picker 1 answer: dates... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [2022-05-31, 2022-05-12, 2022-05-09, 2022-05-04]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: dates\n",
      "Prompt context insert patient_ids - medical_boolean - dates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  dates\n",
      "Resolved response:  dates\n",
      "LLM Picker 1 answer: dates... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [2020-05-26, 2020-07-07]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: dates\n",
      "Prompt context insert patient_ids - medical_boolean - dates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [non, oui]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  dates\n",
      "Resolved response:  dates\n",
      "LLM Picker 1 answer: dates... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [2019-09-26, 2019-03-15, 2019-09-12, 2019-07-21, 2018-10-17]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: dates\n",
      "Prompt context insert patient_ids - medical_boolean - dates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  disease\n",
      "Resolved response:  disease\n",
      "LLM Picker 1 answer: disease... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [cholera, nil]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: disease\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [vivant, dcd]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [f, m]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [no]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  dates\n",
      "Resolved response:  dates\n",
      "LLM Picker 1 answer: dates... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [2020-06-22, 2020-06-20]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: dates\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [yes, no]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [g4lb0023, 0bl5g006, 0bg05l46, 48g500lb, 2l0b07g0]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [55, 6, 70, 8, 34]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  dates\n",
      "Resolved response:  dates\n",
      "LLM Picker 1 answer: dates... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [2020-07-11, 2020-07-25, 2020-07-26, 2020-07-27, 2020-07-17]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: dates\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [inc, oui, non]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [non, inc, oui]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [bouca, damara, bossangoa, ndl, ippy]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [15, 22, 71, 31, 40]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [inc, oui, non]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            *\n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [-brq6-7tm7ecd300o21, brq2et6-21do-0831cm0, qm0-2et1-30462bcr9od, d--3b23emo620c1tr0q4, 0r26b0-edt137om-cq2]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [inconnu, non, oui]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [ramsey, solano, monmouth, chelan, broward]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  dates\n",
      "Resolved response:  dates\n",
      "LLM Picker 1 answer: dates... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [2020-08-07, 2020-07-28, 2020-06-07, 2020-07-10, 2020-06-04]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: dates\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [f, m]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            *\n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [estonia, greenland, new zealand, romania, germany]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  dates\n",
      "Resolved response:  dates\n",
      "LLM Picker 1 answer: dates... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [2022-11-06, 2022-11-03, 2022-11-04, 2022-10-28, 2022-11-09]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: dates\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [oui, non]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [1.0, 0.0]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  disease\n",
      "Resolved response:  disease\n",
      "LLM Picker 1 answer: disease... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [communautaire, funerailles, nosocomial]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: disease\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  age_ranges\n",
      "Resolved response:  age_ranges\n",
      "LLM Picker 1 answer: age_ranges... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [20s, 60s, 40s, 50s, 10s]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: age_ranges\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [ouaka, basse-kotto, ouham-pend, bangui, haute-kotto]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [2t050w0g, 90tg066w, 00209wgt, 1d3tcer00q90, 0909wgt3]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  dates\n",
      "Resolved response:  dates\n",
      "LLM Picker 1 answer: dates... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [2024-02-09, 2024-02-22, 2024-01-29, 2024-01-30, 2024-03-12]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: dates\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [inc, non, oui]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  dates\n",
      "Resolved response:  dates\n",
      "LLM Picker 1 answer: dates... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [2022-03-20, 2022-04-28, 2022-05-03, 2022-03-17, 2022-03-30]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: dates\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [inconnu, oui, non]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [azerbaijan, cuba, slovakia, albania, ukraine]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [no, yes]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  dates\n",
      "Resolved response:  dates\n",
      "LLM Picker 1 answer: dates... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [2020-05-31, 2020-06-02, 2020-06-30, 2020-06-06, 2020-06-29]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: dates\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [no]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [dcd, vivant]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [6l170b0g, g08l03b0, l20gb406, 0bg90l29, 3gl0780b]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [no]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            *\n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  patient_ids\n",
      "Resolved response:  patient_ids\n",
      "LLM Picker 1 answer: patient_ids... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [kaga-bandoro, bambio, kmo, bangui, ouham-pend]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: patient_ids\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [confirme, probable]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  disease\n",
      "Resolved response:  disease\n",
      "LLM Picker 1 answer: disease... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [hepatitis b, high blood pressure,  hypercholesterol,  mia, copd]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: disease\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  infirmier\n",
      "Resolved response:  infirmier\n",
      "LLM Picker 1 answer: infirmier... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges', 'infirmier']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [chasseur, cultivateur, infirmier, commercant, reco]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: infirmier\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges - infirmier\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges', 'infirmier']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [no, yes]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges - infirmier\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges - infirmier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:  medical_boolean\n",
      "Resolved response:  medical_boolean\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges', 'infirmier']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [0.0, 1.0]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges - infirmier\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Prompt context insert patient_ids - medical_boolean - dates - disease - age_ranges - infirmier\n",
      "LLM Picker 1 answer: medical_boolean... Should be none in the beginning\n",
      "List of label so far: ['patient_ids', 'medical_boolean', 'dates', 'disease', 'age_ranges', 'infirmier']\n",
      "PROMPT SENT:\n",
      " \n",
      "                SYSTEM: You are an epidemiology data steward labeling synthetically generated data. Note that you will be working with patient data that was synthetically generated and doesn't exist in real-world.\n",
      "\n",
      "                INSTRUCTIONS:\n",
      "                • Choose exactly ONE label from the OPTIONS list that best describes the INPUT. Note that INPUT should be relative to the OPTIONS that you have. Don't select label if it's just a little bit matches with INPUT.\n",
      "                • If none OPTIONS apply, you can generate new label that will match the INPUT. The generated label should be in snake_case format. Generate broad label.\n",
      "                • **Respond with EXACTLY the label token, no additional words, punctuation, or explanation.**\n",
      "\n",
      "                This is just examples how it could look like:\n",
      "                INPUT: [\"Moderna\", \"AstraZeneca\", \"Novavax\"]\n",
      "                OPTIONS: [\"treatment\", \"country\", \"vaccine_names\"]\n",
      "                ANSWER: vaccine_names\n",
      "\n",
      "                INPUT: [\"Tuberculosis\", \"Malaria\", \"Ebola\"]\n",
      "                OPTIONS: [\"vaccine_names\", \"disease\", \"country\"]\n",
      "                ANSWER: disease\n",
      "\n",
      "                Now it's your turn:\n",
      "                INPUT: [no]\n",
      "                OPTIONS: patient_ids - medical_boolean - dates - disease - age_ranges - infirmier\n",
      "                ANSWER:\n",
      "            \n",
      "MODEL ANSWER: medical_boolean\n",
      "Total inference time: 69.46s over 55 calls\n",
      "  → average per call: 1.263s\n",
      "  → min / max per call: 0.000s / 2.562s\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"alpaca-fine-tuned\"\n",
    "#model_name = \"flan-ul2\"\n",
    "#model_name = \"alpaca-13b\"\n",
    "\n",
    "model_name=\"llama\"\n",
    "\n",
    "filename = f\"custom-data-{model_name}-label-generation-{prompt_type}-{tune}.json\"\n",
    "\n",
    "sp = f\"{archetype_directory}/custom_data_logs/{filename}\"\n",
    "\n",
    "\n",
    "dirpath = os.path.dirname(sp)\n",
    "os.makedirs(dirpath, exist_ok=True)\n",
    "\n",
    "#model_name = \"flan-t5-xxl\"\n",
    "\n",
    "#base_model, tokenizer, template, pt, MAX_LEN = init_model(model_name)\n",
    "\n",
    "# Test set\n",
    "#labels_path = \"/home/omadbek/projects/Sherlock/custom_data/label_generation/test_labels_generation.parquet\"\n",
    "#data_path = \"/home/omadbek/projects/Sherlock/custom_data/label_generation/test_data_generation.parquet\"\n",
    "\n",
    "\n",
    "# LLAMA\n",
    "prompt_dict, inference_time = run_val_parquet(\n",
    "    model=\"llama\",\n",
    "    save_path=sp,\n",
    "    labels_path=labels_path,\n",
    "    data_path=data_path,\n",
    "    label_set=label_set,\n",
    "    method=[\"similarity\"],\n",
    "    resume=False,\n",
    "    sample_size=5,\n",
    "    #stop_early = 30,\n",
    "    link = \"http://localhost:11434/api/generate\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe4311a83efa0299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:56:19.544446Z",
     "start_time": "2025-08-10T12:56:19.542395Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:21:21.055235Z",
     "iopub.status.busy": "2025-08-18T11:21:21.055033Z",
     "iopub.status.idle": "2025-08-18T11:21:21.059230Z",
     "shell.execute_reply": "2025-08-18T11:21:21.058854Z"
    },
    "papermill": {
     "duration": 0.012679,
     "end_time": "2025-08-18T11:21:21.059792",
     "exception": false,
     "start_time": "2025-08-18T11:21:21.047113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File exists: /home/omadbek/projects/ArcheType/custom_data_logs/custom-data-llama-label-generation-few-shot-llama_run9.json\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(sp):\n",
    "    print(f\"✅ File exists: {sp}\")\n",
    "else:\n",
    "    print(f\"❌ File not found: {sp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c18a5cc-b04b-4450-a816-0d60a5e2a173",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:56:19.639067Z",
     "start_time": "2025-08-10T12:56:19.629874Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:21:21.074704Z",
     "iopub.status.busy": "2025-08-18T11:21:21.074573Z",
     "iopub.status.idle": "2025-08-18T11:21:21.081138Z",
     "shell.execute_reply": "2025-08-18T11:21:21.080746Z"
    },
    "papermill": {
     "duration": 0.014735,
     "end_time": "2025-08-18T11:21:21.081680",
     "exception": false,
     "start_time": "2025-08-18T11:21:21.066945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Fuzzy matcher\n",
    "\n",
    "# --- Evaluation function ---\n",
    "def evaluate_and_remap(\n",
    "    file_name: str,\n",
    "    embed_threshold: float = 0.25, # cos_threshold (semantic)\n",
    "    #fuzz_threshold: int = 25 # fuzz_threshold (lexical)\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads your JSON, uses fuzzy matching to snap each 'response' to the closest\n",
    "    ground-truth label (if similar), and counts fuzzy-correct matches.\n",
    "    Adds two new fields to each record in the loaded dict:\n",
    "      - 'resolved_response': the matched label or original response\n",
    "      - 'correct_fuzzy': bool, whether resolved_response == ground_truth\n",
    "    Returns the updated data dict and the fuzzy accuracy.\n",
    "    \"\"\"\n",
    "    # 1) Load JSON\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 2) Build matcher over your ground-truth set\n",
    "    label_set = sorted({v[\"ground_truth\"] for v in data.values()})\n",
    "    matcher = _CosineLabelMatcher(label_set, embed_threshold)\n",
    "\n",
    "    # 3) Remap each response & compute fuzzy correctness\n",
    "    total = 0\n",
    "    fuzzy_correct = 0\n",
    "    for record in data.values():\n",
    "        total += 1\n",
    "        orig = record[\"response\"]\n",
    "        match = matcher.resolve(orig)\n",
    "        resolved = match if match is not None else orig\n",
    "        record[\"resolved_response\"] = resolved\n",
    "        is_correct = (resolved == record[\"ground_truth\"])\n",
    "        record[\"correct_fuzzy\"] = is_correct\n",
    "        if is_correct:\n",
    "            fuzzy_correct += 1\n",
    "\n",
    "    # TEST\n",
    "    # 4) Build truth and pred lists\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for rec in data.values():\n",
    "        y_true.append(rec[\"ground_truth\"])\n",
    "        y_pred.append(rec[\"resolved_response\"])\n",
    "\n",
    "    # 5) Print out class‐support (how many of each ground truth label)\n",
    "    #support = Counter(y_true)\n",
    "    #print(\"Class counts (support):\")\n",
    "    #for label, cnt in support.items():\n",
    "    #    print(f\"  {label:15s} → {cnt}\")\n",
    "    #print()\n",
    "\n",
    "    # 6) Classification report (precision/recall/F₁ + support)\n",
    "    print(\"Per-class precision / recall / F₁ / support:\")\n",
    "    print(classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        #labels=sorted(support),   # ensures a fixed order\n",
    "        digits=4,\n",
    "    ))\n",
    "\n",
    "\n",
    "    raw_report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # 7) Macro / micro F₁\n",
    "    #macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    #micro_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    #print(f\"Macro-average F₁: {macro_f1:.4f}\")\n",
    "    #print(f\"Micro-average F₁: {micro_f1:.4f}\")\n",
    "\n",
    "    # 4) Print fuzzy accuracy\n",
    "    acc = fuzzy_correct / total if total else 0.0\n",
    "    total_correct = f\"{fuzzy_correct}/{total}\" \n",
    "    print(f\"Fuzzy‐matched correct: {total_correct}  →  Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return total_correct, acc, total, raw_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d33e0466-abe0-4074-b1a0-4c25c3302673",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:57:03.166426Z",
     "start_time": "2025-08-10T12:56:19.705136Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:21:21.096652Z",
     "iopub.status.busy": "2025-08-18T11:21:21.096507Z",
     "iopub.status.idle": "2025-08-18T11:22:01.657966Z",
     "shell.execute_reply": "2025-08-18T11:22:01.657301Z"
    },
    "papermill": {
     "duration": 40.576951,
     "end_time": "2025-08-18T11:22:01.665853",
     "exception": false,
     "start_time": "2025-08-18T11:21:21.088902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-class precision / recall / F₁ / support:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "            age     1.0000    0.3333    0.5000         3\n",
      "    case_status     0.0000    0.0000    0.0000         2\n",
      "contact_setting     0.0000    0.0000    0.0000         1\n",
      "           date     1.0000    1.0000    1.0000        12\n",
      "         gender     0.0000    0.0000    0.0000         2\n",
      "             id     0.3333    1.0000    0.5000         5\n",
      "       location     0.0000    0.0000    0.0000         6\n",
      "medical_boolean     0.7692    1.0000    0.8696        20\n",
      "     occupation     0.0000    0.0000    0.0000         1\n",
      "        outcome     0.0000    0.0000    0.0000         2\n",
      "       symptoms     0.0000    0.0000    0.0000         1\n",
      "\n",
      "       accuracy                         0.6909        55\n",
      "      macro avg     0.2821    0.3030    0.2609        55\n",
      "   weighted avg     0.5828    0.6909    0.6071        55\n",
      "\n",
      "Fuzzy‐matched correct: 38/55  →  Accuracy: 0.6909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omadbek/.conda/envs/archetype/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/omadbek/.conda/envs/archetype/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/omadbek/.conda/envs/archetype/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "total_correct, fuzzy_acc, total, raw_report = evaluate_and_remap(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e465bfe25954645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:57:03.322264Z",
     "start_time": "2025-08-10T12:57:03.320359Z"
    },
    "papermill": {
     "duration": 0.007198,
     "end_time": "2025-08-18T11:22:01.680378",
     "exception": false,
     "start_time": "2025-08-18T11:22:01.673180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a658307-6081-4049-a4f2-01da06172f95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:23.812692Z",
     "start_time": "2025-08-10T12:57:03.400828Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:22:01.695626Z",
     "iopub.status.busy": "2025-08-18T11:22:01.695476Z",
     "iopub.status.idle": "2025-08-18T11:23:18.442695Z",
     "shell.execute_reply": "2025-08-18T11:23:18.441482Z"
    },
    "papermill": {
     "duration": 76.756351,
     "end_time": "2025-08-18T11:23:18.443923",
     "exception": false,
     "start_time": "2025-08-18T11:22:01.687572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json, re, pandas as pd\n",
    "#from rapidfuzz import fuzz\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# load data\n",
    "data = json.load(open(sp))\n",
    "rows = []\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
    "for rec in data.values():\n",
    "    gt   = rec[\"ground_truth\"]\n",
    "    resp = rec[\"response\"]\n",
    "    # fuzzy\n",
    "    #fuzz_score = fuzz.token_sort_ratio(resp, gt)\n",
    "    # cosine\n",
    "    emb_r = model.encode([resp], normalize_embeddings=True)\n",
    "    emb_g = model.encode([gt],   normalize_embeddings=True)\n",
    "    cos_score = util.cos_sim(emb_r, emb_g)[0][0].item()\n",
    "    rows.append({\n",
    "        \"ground_truth\": gt,\n",
    "        \"response\":     resp,\n",
    "        #\"fuzz_score\":   fuzz_score,\n",
    "        \"cos_score\":    cos_score\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b1ac587-8505-46da-88d1-a04bbe3dec87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:23.886365Z",
     "start_time": "2025-08-10T12:58:23.876672Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:23:18.460420Z",
     "iopub.status.busy": "2025-08-18T11:23:18.460242Z",
     "iopub.status.idle": "2025-08-18T11:23:18.469027Z",
     "shell.execute_reply": "2025-08-18T11:23:18.468498Z"
    },
    "papermill": {
     "duration": 0.017761,
     "end_time": "2025-08-18T11:23:18.469557",
     "exception": false,
     "start_time": "2025-08-18T11:23:18.451796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ground_truth         response  cos_score\n",
      "0                id      patient_ids   0.413760\n",
      "1   medical_boolean  medical_boolean   1.000000\n",
      "2           outcome      patient_ids   0.201846\n",
      "3   medical_boolean  medical_boolean   1.000000\n",
      "4              date            dates   0.903609\n",
      "5              date            dates   0.903609\n",
      "6              date            dates   0.903609\n",
      "7              date            dates   0.903609\n",
      "8   medical_boolean  medical_boolean   1.000000\n",
      "9              date            dates   0.903609\n",
      "10  medical_boolean          disease   0.409297\n",
      "11      case_status      patient_ids   0.372566\n",
      "12           gender  medical_boolean   0.269256\n",
      "13  medical_boolean  medical_boolean   1.000000\n",
      "14             date            dates   0.903609\n",
      "15  medical_boolean  medical_boolean   1.000000\n",
      "16               id      patient_ids   0.413760\n",
      "17              age      patient_ids   0.167688\n",
      "18             date            dates   0.903609\n",
      "19  medical_boolean  medical_boolean   1.000000\n",
      "20  medical_boolean  medical_boolean   1.000000\n",
      "21         location      patient_ids   0.118107\n",
      "22              age      patient_ids   0.167688\n",
      "23  medical_boolean  medical_boolean   1.000000\n",
      "24               id      patient_ids   0.413760\n",
      "25  medical_boolean  medical_boolean   1.000000\n",
      "26         location      patient_ids   0.118107\n",
      "27             date            dates   0.903609\n",
      "28           gender  medical_boolean   0.269256\n",
      "29         location      patient_ids   0.118107\n",
      "30             date            dates   0.903609\n",
      "31  medical_boolean  medical_boolean   1.000000\n",
      "32  medical_boolean  medical_boolean   1.000000\n",
      "33  contact_setting          disease   0.077651\n",
      "34              age       age_ranges   0.658423\n",
      "35         location      patient_ids   0.118107\n",
      "36               id      patient_ids   0.413760\n",
      "37             date            dates   0.903609\n",
      "38  medical_boolean  medical_boolean   1.000000\n",
      "39             date            dates   0.903609\n",
      "40  medical_boolean  medical_boolean   1.000000\n",
      "41         location      patient_ids   0.118107\n",
      "42  medical_boolean  medical_boolean   1.000000\n",
      "43             date            dates   0.903609\n",
      "44  medical_boolean  medical_boolean   1.000000\n",
      "45          outcome  medical_boolean   0.304995\n",
      "46               id      patient_ids   0.413760\n",
      "47  medical_boolean  medical_boolean   1.000000\n",
      "48         location      patient_ids   0.118107\n",
      "49      case_status  medical_boolean   0.330923\n",
      "50         symptoms          disease   0.381346\n",
      "51       occupation        infirmier   0.308934\n",
      "52  medical_boolean  medical_boolean   1.000000\n",
      "53  medical_boolean  medical_boolean   1.000000\n",
      "54  medical_boolean  medical_boolean   1.000000\n"
     ]
    }
   ],
   "source": [
    "# Put to txt file\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3985f1c2-a8de-4d77-96ae-a6588bb84c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:23.974902Z",
     "start_time": "2025-08-10T12:58:23.969134Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:23:18.484855Z",
     "iopub.status.busy": "2025-08-18T11:23:18.484623Z",
     "iopub.status.idle": "2025-08-18T11:23:18.490181Z",
     "shell.execute_reply": "2025-08-18T11:23:18.489567Z"
    },
    "papermill": {
     "duration": 0.014088,
     "end_time": "2025-08-18T11:23:18.490960",
     "exception": false,
     "start_time": "2025-08-18T11:23:18.476872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote fuzzy results to /home/omadbek/projects/run_all/temp-results/fuzzy_result.txt\n"
     ]
    }
   ],
   "source": [
    "out_path = f\"{run_all_directory}/temp-results/fuzzy_result.txt\"\n",
    "\n",
    "out_dir = os.path.dirname(out_path)   # “./temprorary”\n",
    "os.makedirs(out_dir, exist_ok=True)   # create it (if needed)\n",
    "\n",
    "with open(out_path, \"a\") as f:\n",
    "    f.write(\"==== ArcheType Output. LLM label generation results ====\\n\")\n",
    "    f.write(f\"Fuzzy‐matched correct semantic types: {total_correct}\"\n",
    "            f\"  →  Accuracy: {fuzzy_acc:.4f}\\n\")\n",
    "    f.write(\"Similarity Scores: \\n\")\n",
    "    f.write(df.to_string(index=True))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Wrote fuzzy results to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d81ec162056f853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.066345Z",
     "start_time": "2025-08-10T12:58:24.050243Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:23:18.507233Z",
     "iopub.status.busy": "2025-08-18T11:23:18.506749Z",
     "iopub.status.idle": "2025-08-18T11:23:18.514666Z",
     "shell.execute_reply": "2025-08-18T11:23:18.514097Z"
    },
    "papermill": {
     "duration": 0.016782,
     "end_time": "2025-08-18T11:23:18.515585",
     "exception": false,
     "start_time": "2025-08-18T11:23:18.498803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flat = {}\n",
    "\n",
    "for label, m in raw_report.items():\n",
    "    if label == \"accuracy\":\n",
    "        flat[\"accuracy\"] = m\n",
    "    else:\n",
    "        for metric_name, val in m.items():\n",
    "            flat[f\"{label}_{metric_name}\"] = val\n",
    "\n",
    "run_name = f\"{model_name}-label-generation-{prompt_type}-{tune}\"\n",
    "# add summary fields\n",
    "flat[\"total_entries\"] = total\n",
    "# filename identifier: take it from your JSON filename variable\n",
    "flat[\"run_name\"] = run_name\n",
    "flat[\"inference_time\"] = inference_time\n",
    "flat[\"Matching accuracy\"] = f\"{fuzzy_acc:.2f}\"\n",
    "\n",
    "# convert to one-row DataFrame\n",
    "df = pd.DataFrame([flat])\n",
    "\n",
    "metrics_csv = f\"{archetype_directory}/all_metrics.csv\"\n",
    "\n",
    "# append (or create) the master CSV\n",
    "if not os.path.isfile(metrics_csv):\n",
    "    df.to_csv(metrics_csv, index=False, float_format=\"%.4f\")\n",
    "else:\n",
    "    df.to_csv(metrics_csv, mode=\"a\", header=False, index=False, float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2bbabd084979a",
   "metadata": {
    "papermill": {
     "duration": 0.007397,
     "end_time": "2025-08-18T11:23:18.530566",
     "exception": false,
     "start_time": "2025-08-18T11:23:18.523169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Remapping and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "130f3f89-a2f9-4f64-908a-f7bfbf37bd25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.127845Z",
     "start_time": "2025-08-10T12:58:24.117752Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:23:18.546587Z",
     "iopub.status.busy": "2025-08-18T11:23:18.546137Z",
     "iopub.status.idle": "2025-08-18T11:23:18.557228Z",
     "shell.execute_reply": "2025-08-18T11:23:18.556335Z"
    },
    "papermill": {
     "duration": 0.02022,
     "end_time": "2025-08-18T11:23:18.558345",
     "exception": false,
     "start_time": "2025-08-18T11:23:18.538125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport json\\nimport pandas as pd\\nfrom sklearn.metrics import classification_report, confusion_matrix\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\ndata_dir = \"../custom_data_logs\"\\n\\n# 1) Load results\\nwith open(sp, \\'r\\', encoding=\\'utf-8\\') as f:\\n    data = json.load(f)\\n\\n# 2) Flatten to DataFrame\\nrecords = []\\nfor entry in data.values():\\n    records.append({\\n        \\'file_idx\\': entry[\\'file+idx\\'],\\n        \\'ground_truth\\': entry[\\'ground_truth\\'],\\n        \\'predicted\\': entry[\\'response\\']\\n    })\\ndf = pd.DataFrame(records)\\n\\nprint(\"Unique values: \", sorted(df[\\'predicted\\'].unique()))\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"../custom_data_logs\"\n",
    "\n",
    "# 1) Load results\n",
    "with open(sp, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2) Flatten to DataFrame\n",
    "records = []\n",
    "for entry in data.values():\n",
    "    records.append({\n",
    "        'file_idx': entry['file+idx'],\n",
    "        'ground_truth': entry['ground_truth'],\n",
    "        'predicted': entry['response']\n",
    "    })\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "print(\"Unique values: \", sorted(df['predicted'].unique()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79ecc133-27f6-48c2-9387-24e8eab89d32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.192064Z",
     "start_time": "2025-08-10T12:58:24.186981Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:23:18.575316Z",
     "iopub.status.busy": "2025-08-18T11:23:18.574620Z",
     "iopub.status.idle": "2025-08-18T11:23:18.580396Z",
     "shell.execute_reply": "2025-08-18T11:23:18.579499Z"
    },
    "papermill": {
     "duration": 0.015147,
     "end_time": "2025-08-18T11:23:18.581619",
     "exception": false,
     "start_time": "2025-08-18T11:23:18.566472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsummary = (\\n    df.groupby('predicted')['ground_truth']\\n      .agg(['nunique', lambda x: list(x.unique())])\\n      .rename(columns={'nunique':'# distinct', '<lambda_0>':'values'})\\n)\\nprint(summary)\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "summary = (\n",
    "    df.groupby('predicted')['ground_truth']\n",
    "      .agg(['nunique', lambda x: list(x.unique())])\n",
    "      .rename(columns={'nunique':'# distinct', '<lambda_0>':'values'})\n",
    ")\n",
    "print(summary)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88259c1e-0fb6-4f44-a9e4-6f0fa627ee75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.285192Z",
     "start_time": "2025-08-10T12:58:24.279990Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:23:18.598179Z",
     "iopub.status.busy": "2025-08-18T11:23:18.597712Z",
     "iopub.status.idle": "2025-08-18T11:23:18.603237Z",
     "shell.execute_reply": "2025-08-18T11:23:18.602381Z"
    },
    "papermill": {
     "duration": 0.014606,
     "end_time": "2025-08-18T11:23:18.604163",
     "exception": false,
     "start_time": "2025-08-18T11:23:18.589557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# build a dict of {predicted_label: list_of_unique_ground_truths}\\nunique_vals = {\\n    cat: df.loc[df[\\'predicted\\'] == cat, \\'ground_truth\\'].unique().tolist()\\n    for cat in df[\\'predicted\\'].unique()\\n}\\n\\n# print them out\\nfor cat, vals in unique_vals.items():\\n    print(f\"{cat} ({len(vals)} distinct values):\")\\n    print(vals, \"\\n\")\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# build a dict of {predicted_label: list_of_unique_ground_truths}\n",
    "unique_vals = {\n",
    "    cat: df.loc[df['predicted'] == cat, 'ground_truth'].unique().tolist()\n",
    "    for cat in df['predicted'].unique()\n",
    "}\n",
    "\n",
    "# print them out\n",
    "for cat, vals in unique_vals.items():\n",
    "    print(f\"{cat} ({len(vals)} distinct values):\")\n",
    "    print(vals, \"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48da796a-9465-42db-afa8-4774033a5766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.400891Z",
     "start_time": "2025-08-10T12:58:24.394547Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-18T11:23:18.620694Z",
     "iopub.status.busy": "2025-08-18T11:23:18.620054Z",
     "iopub.status.idle": "2025-08-18T11:23:18.626504Z",
     "shell.execute_reply": "2025-08-18T11:23:18.625660Z"
    },
    "papermill": {
     "duration": 0.015956,
     "end_time": "2025-08-18T11:23:18.627768",
     "exception": false,
     "start_time": "2025-08-18T11:23:18.611812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport json\\nimport pandas as pd\\nfrom sklearn.metrics import classification_report, confusion_matrix\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# 3) Define mapping dict\\nlabel_mapping = {\\n    # identifiers\\n    \\'patient_id\\':                \\'id\\',\\n    \\'patient_ids\\':               \\'id\\',\\n\\n    # yes/no, true/false\\n    \\'response_status\\':          \\'medical_boolean\\',\\n    \\'medical_boolean\\':          \\'medical_boolean\\',\\n\\n    # recovery/outcome\\n    \\'patient_recovery_status\\':   \\'outcome\\',\\n\\n    # dates\\n    \\'date_range\\':               \\'date\\',\\n\\n    # gender\\n    \\'gender_type\\':              \\'gender\\',\\n\\n    # locations\\n    \\'country_list\\':             \\'location\\',\\n\\n    # free‐text details\\n    \\'diagnosis_details\\':        \\'symptoms\\',\\n    \\'role_types\\':               \\'occupation\\',\\n}\\n\\n# 4) Apply mapping\\ndf[\\'mapped_predicted\\'] = df[\\'predicted\\'].map(label_mapping).fillna(df[\\'predicted\\'])\\n\\n# 5) Display DataFrame\\n#print(df[[\\'file_idx\\', \\'ground_truth\\', \\'predicted\\', \\'mapped_predicted\\']])\\n\\n# 6) Classification report\\nprint(\"Classification Report:\")\\nprint(classification_report(df[\\'ground_truth\\'], df[\\'mapped_predicted\\'], zero_division=0))\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 3) Define mapping dict\n",
    "label_mapping = {\n",
    "    # identifiers\n",
    "    'patient_id':                'id',\n",
    "    'patient_ids':               'id',\n",
    "\n",
    "    # yes/no, true/false\n",
    "    'response_status':          'medical_boolean',\n",
    "    'medical_boolean':          'medical_boolean',\n",
    "\n",
    "    # recovery/outcome\n",
    "    'patient_recovery_status':   'outcome',\n",
    "\n",
    "    # dates\n",
    "    'date_range':               'date',\n",
    "\n",
    "    # gender\n",
    "    'gender_type':              'gender',\n",
    "\n",
    "    # locations\n",
    "    'country_list':             'location',\n",
    "\n",
    "    # free‐text details\n",
    "    'diagnosis_details':        'symptoms',\n",
    "    'role_types':               'occupation',\n",
    "}\n",
    "\n",
    "# 4) Apply mapping\n",
    "df['mapped_predicted'] = df['predicted'].map(label_mapping).fillna(df['predicted'])\n",
    "\n",
    "# 5) Display DataFrame\n",
    "#print(df[['file_idx', 'ground_truth', 'predicted', 'mapped_predicted']])\n",
    "\n",
    "# 6) Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(df['ground_truth'], df['mapped_predicted'], zero_division=0))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b45c2-1068-45dc-80bb-e58d7ecc21ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.478350Z",
     "start_time": "2025-08-10T12:58:24.475307Z"
    },
    "papermill": {
     "duration": 0.007668,
     "end_time": "2025-08-18T11:23:18.643544",
     "exception": false,
     "start_time": "2025-08-18T11:23:18.635876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53929977b924aa0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.516325Z",
     "start_time": "2025-08-10T12:58:24.513518Z"
    },
    "papermill": {
     "duration": 0.007657,
     "end_time": "2025-08-18T11:23:18.658916",
     "exception": false,
     "start_time": "2025-08-18T11:23:18.651259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ee197ebdfa942",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T12:58:24.611921Z",
     "start_time": "2025-08-10T12:58:24.609252Z"
    },
    "papermill": {
     "duration": 0.007713,
     "end_time": "2025-08-18T11:23:18.674379",
     "exception": false,
     "start_time": "2025-08-18T11:23:18.666666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 197.025434,
   "end_time": "2025-08-18T11:23:20.604180",
   "environment_variables": {},
   "exception": null,
   "input_path": "label-generation.ipynb",
   "output_path": "/home/omadbek/projects/ArcheType/papermill_notebooks/test_out_llama_run9.ipynb",
   "parameters": {
    "archetype_directory": "/home/omadbek/projects/ArcheType",
    "data_path": "/home/omadbek/projects/Sherlock/custom_data/label_generation/test_data_generation.parquet",
    "labels_path": "/home/omadbek/projects/Sherlock/custom_data/label_generation/test_labels_generation.parquet",
    "prompt_type": "few-shot",
    "run_all_directory": "/home/omadbek/projects/run_all",
    "tune": "llama_run9"
   },
   "start_time": "2025-08-18T11:20:03.578746",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "24db0dfb7531432185fa3c7278d61552": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7a5c440c637b4b2b869cad9523a558d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8da0d27171914c60912b5ed18b5b9224": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a014eabc724c46c9ba7231f2b023cb87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a6ffc90782064eccbf9f8569d944f728": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_24db0dfb7531432185fa3c7278d61552",
       "placeholder": "​",
       "style": "IPY_MODEL_a9f2a4a1ac96416284ed0e972a550359",
       "value": " 55/55 [01:09&lt;00:00,  1.21s/it]"
      }
     },
     "a9f2a4a1ac96416284ed0e972a550359": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ac250447b7484252a4984790c244865c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e0fa57a2eaf74fe28255d2b0d744ab8e",
        "IPY_MODEL_b3b35efa3d6641c4aa45a5cfa02ee729",
        "IPY_MODEL_a6ffc90782064eccbf9f8569d944f728"
       ],
       "layout": "IPY_MODEL_b7f163987b19471e821b9e9f121bfbb7"
      }
     },
     "b3b35efa3d6641c4aa45a5cfa02ee729": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7a5c440c637b4b2b869cad9523a558d6",
       "max": 55.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c7ff470be9e64e6887d9fa8f68249977",
       "value": 55.0
      }
     },
     "b7f163987b19471e821b9e9f121bfbb7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c7ff470be9e64e6887d9fa8f68249977": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e0fa57a2eaf74fe28255d2b0d744ab8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8da0d27171914c60912b5ed18b5b9224",
       "placeholder": "​",
       "style": "IPY_MODEL_a014eabc724c46c9ba7231f2b023cb87",
       "value": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}