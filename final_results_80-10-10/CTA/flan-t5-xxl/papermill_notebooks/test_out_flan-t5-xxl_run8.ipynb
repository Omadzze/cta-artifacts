{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc22338c85d486a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:54.072636Z",
     "start_time": "2025-07-23T08:58:54.068268Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:23.287617Z",
     "iopub.status.busy": "2025-08-08T13:51:23.287310Z",
     "iopub.status.idle": "2025-08-08T13:51:23.291636Z",
     "shell.execute_reply": "2025-08-08T13:51:23.291332Z"
    },
    "papermill": {
     "duration": 0.014384,
     "end_time": "2025-08-08T13:51:23.292510",
     "exception": false,
     "start_time": "2025-08-08T13:51:23.278126",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "tune = \"0.0\"\n",
    "model_name = \"alpaca-fine-tuned\"\n",
    "sherlock_path = \"/home/omadbek/projects/Sherlock\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd516ed6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:23.307197Z",
     "iopub.status.busy": "2025-08-08T13:51:23.306766Z",
     "iopub.status.idle": "2025-08-08T13:51:23.309035Z",
     "shell.execute_reply": "2025-08-08T13:51:23.308679Z"
    },
    "papermill": {
     "duration": 0.010419,
     "end_time": "2025-08-08T13:51:23.309759",
     "exception": false,
     "start_time": "2025-08-08T13:51:23.299340",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "tune = \"flan-t5-xxl_run8\"\n",
    "model_name = \"flan-t5-xxl\"\n",
    "sherlock_path = \"/home/omadbek/projects/Sherlock\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99491884dd57ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:54.609947Z",
     "start_time": "2025-07-23T08:58:54.608030Z"
    },
    "papermill": {
     "duration": 0.006833,
     "end_time": "2025-08-08T13:51:23.323204",
     "exception": false,
     "start_time": "2025-08-08T13:51:23.316371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:56.993787Z",
     "start_time": "2025-07-23T08:58:55.132320Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:23.338419Z",
     "iopub.status.busy": "2025-08-08T13:51:23.338160Z",
     "iopub.status.idle": "2025-08-08T13:51:25.457517Z",
     "shell.execute_reply": "2025-08-08T13:51:25.456320Z"
    },
    "papermill": {
     "duration": 2.128917,
     "end_time": "2025-08-08T13:51:25.459151",
     "exception": false,
     "start_time": "2025-08-08T13:51:23.330234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omadbek/.conda/envs/archetype/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#https://colab.research.google.com/drive/1BEZ_qgtVqSmOmCTuhHs7lHiYB5M5_myg?usp=sharing\n",
    "\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "#import gzipƒ\n",
    "from tqdm.auto import tqdm\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import chain\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from retry import retry\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9606158edf70524f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:57.071860Z",
     "start_time": "2025-07-23T08:58:57.070082Z"
    },
    "papermill": {
     "duration": 0.010006,
     "end_time": "2025-08-08T13:51:25.480251",
     "exception": false,
     "start_time": "2025-08-08T13:51:25.470245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d565ecc3-2433-4b7b-bfa7-d8225b0db815",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:57.248484Z",
     "start_time": "2025-07-23T08:58:57.102601Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:25.528332Z",
     "iopub.status.busy": "2025-08-08T13:51:25.527847Z",
     "iopub.status.idle": "2025-08-08T13:51:25.676473Z",
     "shell.execute_reply": "2025-08-08T13:51:25.675573Z"
    },
    "papermill": {
     "duration": 0.186644,
     "end_time": "2025-08-08T13:51:25.677279",
     "exception": false,
     "start_time": "2025-08-08T13:51:25.490635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EST_CHARS_PER_TOKEN=4\r\n",
      "MAX_LEN=2000*EST_CHARS_PER_TOKEN\r\n",
      "INTEGER_SET = set(r\"0123456789,/\\+-.^_()[] :\")\r\n",
      "BOOLEAN_SET = set([\"True\", \"true\", \"False\", \"false\", \"yes\", \"Yes\", \"No\", \"no\"])\r\n",
      "\r\n",
      "ARCHETYPE_PATH = \"/home/omadbek/projects/ArcheType\"\r\n",
      "DOTENV_PATH = \"/home/omadbek/projects/ArcheType/.env\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/omadbek/projects/ArcheType/src/const.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "881a695b-e8d4-4c8b-ab64-52f8152df91a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:57.311681Z",
     "start_time": "2025-07-23T08:58:57.306530Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:25.687648Z",
     "iopub.status.busy": "2025-08-08T13:51:25.687017Z",
     "iopub.status.idle": "2025-08-08T13:51:25.691085Z",
     "shell.execute_reply": "2025-08-08T13:51:25.690621Z"
    },
    "papermill": {
     "duration": 0.009833,
     "end_time": "2025-08-08T13:51:25.691716",
     "exception": false,
     "start_time": "2025-08-08T13:51:25.681883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/omadbek/projects/ArcheType\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14d5a0-ee3e-4fd7-8731-41a907223247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:57.439465Z",
     "start_time": "2025-07-23T08:58:57.437071Z"
    },
    "papermill": {
     "duration": 0.004263,
     "end_time": "2025-08-08T13:51:25.700299",
     "exception": false,
     "start_time": "2025-08-08T13:51:25.696036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a93e55-abb0-4938-86f5-eb75925a80ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:57.477907Z",
     "start_time": "2025-07-23T08:58:57.475461Z"
    },
    "papermill": {
     "duration": 0.004266,
     "end_time": "2025-08-08T13:51:25.708916",
     "exception": false,
     "start_time": "2025-08-08T13:51:25.704650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dafde7b-0eef-4af8-a652-012a533151a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:58.448219Z",
     "start_time": "2025-07-23T08:58:57.534639Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:25.718420Z",
     "iopub.status.busy": "2025-08-08T13:51:25.718155Z",
     "iopub.status.idle": "2025-08-08T13:51:26.037753Z",
     "shell.execute_reply": "2025-08-08T13:51:26.036974Z"
    },
    "papermill": {
     "duration": 0.325854,
     "end_time": "2025-08-08T13:51:26.039016",
     "exception": false,
     "start_time": "2025-08-08T13:51:25.713162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())      # → 1\n",
    "print(torch.cuda.get_device_name(0))  # → the one you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f14519-48c9-4c42-9aca-9e0945b3aeda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:58.497008Z",
     "start_time": "2025-07-23T08:58:58.495646Z"
    },
    "papermill": {
     "duration": 0.010297,
     "end_time": "2025-08-08T13:51:26.059813",
     "exception": false,
     "start_time": "2025-08-08T13:51:26.049516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee7c1b25-6937-45ad-ac6e-1b41a995bdbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:58.617035Z",
     "start_time": "2025-07-23T08:58:58.577814Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:26.082328Z",
     "iopub.status.busy": "2025-08-08T13:51:26.082054Z",
     "iopub.status.idle": "2025-08-08T13:51:26.116997Z",
     "shell.execute_reply": "2025-08-08T13:51:26.116140Z"
    },
    "papermill": {
     "duration": 0.047775,
     "end_time": "2025-08-08T13:51:26.118209",
     "exception": false,
     "start_time": "2025-08-08T13:51:26.070434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = f\"{sherlock_path}/custom_data\"\n",
    "\n",
    "test_data_path = f\"{data_dir}/raw/test_data.parquet\"\n",
    "test_labels_path = f\"{data_dir}/raw/test_labels.parquet\"\n",
    "\n",
    "# Testing\n",
    "X_test = pd.read_parquet(test_data_path)\n",
    "y_test = pd.read_parquet(test_labels_path)\n",
    "\n",
    "#cta_gt = load_and_remap_cta_gt(f\"{data_dir}/cta_gt.csv\", LABEL_MAP_LC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1676f8571572e2aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:58.653124Z",
     "start_time": "2025-07-23T08:58:58.651794Z"
    },
    "papermill": {
     "duration": 0.010718,
     "end_time": "2025-08-08T13:51:26.140032",
     "exception": false,
     "start_time": "2025-08-08T13:51:26.129314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9e21bc-f2ed-497d-9787-3105aa55f53e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:58.781910Z",
     "start_time": "2025-07-23T08:58:58.772761Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:26.162786Z",
     "iopub.status.busy": "2025-08-08T13:51:26.162538Z",
     "iopub.status.idle": "2025-08-08T13:51:26.173203Z",
     "shell.execute_reply": "2025-08-08T13:51:26.172230Z"
    },
    "papermill": {
     "duration": 0.023807,
     "end_time": "2025-08-08T13:51:26.174716",
     "exception": false,
     "start_time": "2025-08-08T13:51:26.150909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['age', 'case_status', 'contact_setting', 'date', 'gender', 'id',\n",
       "       'location', 'medical_boolean', 'occupation', 'outcome', 'symptoms'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "229d1025-a37a-4c7d-95f9-5714da81c743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:58.931412Z",
     "start_time": "2025-07-23T08:58:58.925291Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:26.197774Z",
     "iopub.status.busy": "2025-08-08T13:51:26.197457Z",
     "iopub.status.idle": "2025-08-08T13:51:26.207700Z",
     "shell.execute_reply": "2025-08-08T13:51:26.206670Z"
    },
    "papermill": {
     "duration": 0.023587,
     "end_time": "2025-08-08T13:51:26.209150",
     "exception": false,
     "start_time": "2025-08-08T13:51:26.185563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_labels(label, label_set):\n",
    "  label = label.lower().strip()\n",
    "  ldm = {k.lower().strip() : v.lower().strip() for k, v in label_set['dict_map'].items()}\n",
    "  if label_set.get(\"abbrev_map\", -1) != -1:\n",
    "    lda = {k.lower().strip() : v.lower().strip() for k, v in label_set['abbrev_map'].items()}\n",
    "    ldares = lda.get(label, \"\")\n",
    "    if ldares != \"\":\n",
    "      label = ldares\n",
    "  if label.endswith(\"/name\"):\n",
    "    label = label[:-5]\n",
    "  remap = ldm.get(label, -1)\n",
    "  if remap != -1:\n",
    "    label = remap\n",
    "  return label.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b47fba85-2e32-4803-82d4-278a673a03ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:59.002073Z",
     "start_time": "2025-07-23T08:58:58.998630Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:26.232273Z",
     "iopub.status.busy": "2025-08-08T13:51:26.231969Z",
     "iopub.status.idle": "2025-08-08T13:51:26.236431Z",
     "shell.execute_reply": "2025-08-08T13:51:26.235552Z"
    },
    "papermill": {
     "duration": 0.017627,
     "end_time": "2025-08-08T13:51:26.237794",
     "exception": false,
     "start_time": "2025-08-08T13:51:26.220167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LABELS = ['age', 'case_status', 'contact_setting', 'date', 'gender', 'id',\n",
    "       'location', 'medical_boolean', 'occupation', 'outcome', 'symptoms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1db475a1-be44-4499-980d-127a62cab476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:59.078152Z",
     "start_time": "2025-07-23T08:58:59.073181Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:26.261023Z",
     "iopub.status.busy": "2025-08-08T13:51:26.260734Z",
     "iopub.status.idle": "2025-08-08T13:51:26.266749Z",
     "shell.execute_reply": "2025-08-08T13:51:26.265870Z"
    },
    "papermill": {
     "duration": 0.01932,
     "end_time": "2025-08-08T13:51:26.268216",
     "exception": false,
     "start_time": "2025-08-08T13:51:26.248896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sotab_integer_labels = [\"age\", \"date\"]\n",
    "sotab_float_labels   = []\n",
    "\n",
    "# everything else must go here\n",
    "sotab_other_labels = [\n",
    "  \"case_status\",\n",
    "  \"gender\",\n",
    "  \"id\",\n",
    "  \"location\",\n",
    "  \"medical_boolean\",\n",
    "  \"occupation\",\n",
    "  \"outcome\",\n",
    "  \"symptoms\"\n",
    "]\n",
    "\n",
    "sotab_top_hier = {\n",
    "  \"integer\": sotab_integer_labels,\n",
    "  \"float\":   sotab_float_labels,\n",
    "  \"other\":   sotab_other_labels\n",
    "}\n",
    "\n",
    "sotab_identifier = [\"id\"]\n",
    "sotab_category   = [\"gender\", \"medical_boolean\", \"outcome\"]\n",
    "sotab_text       = [\"location\", \"symptoms\", \"occupation\"]\n",
    "\n",
    "\n",
    "sotab_other_hier = {\n",
    "  \"Identifier\": sotab_identifier,\n",
    "  \"category\":   sotab_category,\n",
    "  \"text\":       sotab_text\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60e79430-47a7-4d5a-9f97-b5524ed9305a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:59.183853Z",
     "start_time": "2025-07-23T08:58:59.180674Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:26.291705Z",
     "iopub.status.busy": "2025-08-08T13:51:26.291400Z",
     "iopub.status.idle": "2025-08-08T13:51:26.295761Z",
     "shell.execute_reply": "2025-08-08T13:51:26.294879Z"
    },
    "papermill": {
     "duration": 0.0176,
     "end_time": "2025-08-08T13:51:26.297102",
     "exception": false,
     "start_time": "2025-08-08T13:51:26.279502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand_seed=13\n",
    "EST_CHARS_PER_TOKEN=4\n",
    "MAX_LEN=2000*EST_CHARS_PER_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff3bbf8b-5706-486b-9cd0-857411301200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:58:59.274067Z",
     "start_time": "2025-07-23T08:58:59.270738Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:26.321067Z",
     "iopub.status.busy": "2025-08-08T13:51:26.320161Z",
     "iopub.status.idle": "2025-08-08T13:51:26.324343Z",
     "shell.execute_reply": "2025-08-08T13:51:26.323412Z"
    },
    "papermill": {
     "duration": 0.017252,
     "end_time": "2025-08-08T13:51:26.325501",
     "exception": false,
     "start_time": "2025-08-08T13:51:26.308249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"/home/omadbek/projects/alpaca/outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76159fc4-0916-44b2-b99d-e6eca91be82e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:59:01.858891Z",
     "start_time": "2025-07-23T08:58:59.452361Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:26.342780Z",
     "iopub.status.busy": "2025-08-08T13:51:26.342502Z",
     "iopub.status.idle": "2025-08-08T13:51:28.990752Z",
     "shell.execute_reply": "2025-08-08T13:51:28.989894Z"
    },
    "papermill": {
     "duration": 2.658833,
     "end_time": "2025-08-08T13:51:28.992339",
     "exception": false,
     "start_time": "2025-08-08T13:51:26.333506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import infer_auto_device_map, init_empty_weights, load_checkpoint_and_dispatch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, \\\n",
    "    T5ForConditionalGeneration, LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "import langchain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
    "\n",
    "\n",
    "def set_pipeline(k=1):\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=MAX_LEN,\n",
    "        temperature=0.5 * k,\n",
    "        top_p=0.80 - (0.1 * k),\n",
    "        repetition_penalty=1.3\n",
    "    )\n",
    "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    llm_chain = LLMChain(prompt=pt,\n",
    "                         llm=local_llm\n",
    "                         )\n",
    "    return pipe, local_llm, llm_chain\n",
    "\n",
    "\n",
    "curr_model = \"\"\n",
    "\n",
    "\n",
    "def init_model(model):\n",
    "    curr_model = model\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    if model == \"llama-65b\":\n",
    "        LLAMA_PATH = \"/scratch/bf996/text-generation-webui/models/llama-65b-hf\"\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(LLAMA_PATH)\n",
    "        config = AutoConfig.from_pretrained(LLAMA_PATH,\n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            load_in_8bit=True)\n",
    "        with init_empty_weights():\n",
    "            base_model = AutoModelForCausalLM.from_config(config)\n",
    "        base_model.tie_weights()\n",
    "        device_map = infer_auto_device_map(base_model, max_memory={0: \"60GiB\", \"cpu\": \"96GiB\"})\n",
    "        base_model = load_checkpoint_and_dispatch(\n",
    "            base_model,\n",
    "            LLAMA_PATH,\n",
    "            device_map=device_map\n",
    "        )\n",
    "    elif model == \"alpaca-13b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "        #tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "        base_model = LlamaForCausalLM.from_pretrained(\n",
    "            #model_path,\n",
    "            \"chavinlo/alpaca-native\",\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_8bit=True,\n",
    "            device_map='auto',\n",
    "        )\n",
    "    elif model == \"alpaca-fine-tuned\":\n",
    "        MAX_LEN = 2048\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    elif model == \"vicuna-13b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"eachadea/vicuna-13b\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"eachadea/vicuna-13b\",\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_8bit=True,\n",
    "            device_map='auto',\n",
    "        )\n",
    "    elif model == \"gpt4-x-alpaca\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"chavinlo/gpt4-x-alpaca\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\"chavinlo/gpt4-x-alpaca\", device_map=\"auto\",\n",
    "                                                          load_in_8bit=True)\n",
    "    elif model == \"t0pp\":\n",
    "        MAX_LEN = 512\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0pp\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\",\n",
    "                                                           torch_dtype=torch.float16, load_in_8bit=True)\n",
    "    elif model == \"flan-t5-xxl\":\n",
    "        MAX_LEN = 512\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\",\n",
    "                                                           torch_dtype=torch.float16, load_in_8bit=True)\n",
    "    elif model == \"flan-ul2\":\n",
    "        MAX_LEN = 512\n",
    "        base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16,\n",
    "                                                                device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "    elif model == \"galpaca-30b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"GeorgiaTechResearchInstitute/galpaca-30b\", device_map=\"auto\",\n",
    "                                                  torch_dtype=torch.float16, load_in_8bit=True)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\"GeorgiaTechResearchInstitute/galpaca-30b\")\n",
    "    elif model == \"opt-iml-max-30b\":\n",
    "        MAX_LEN = 2048\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-iml-max-30b\", use_fast=False, padding_side='left')\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-iml-max-30b\", device_map=\"auto\",\n",
    "                                                          torch_dtype=torch.float16)\n",
    "    if model in [\"flan-t5-xxl\", \"t0pp\", \"flan-ul2\"]:\n",
    "        template = \"\"\"{instruction}\"\"\"\n",
    "    else:\n",
    "        template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "        ### Instruction: \n",
    "        {instruction}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    pt = PromptTemplate(template=template, input_variables=[\"instruction\"])\n",
    "    #Convert length from tokens to characters, leave room for model response\n",
    "    MAX_LEN = MAX_LEN * EST_CHARS_PER_TOKEN - 200\n",
    "    return base_model, tokenizer, template, pt, MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faa69500-a350-4dc9-9204-f5bea462888c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:59:02.031852Z",
     "start_time": "2025-07-23T08:59:02.013693Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:29.009016Z",
     "iopub.status.busy": "2025-08-08T13:51:29.008050Z",
     "iopub.status.idle": "2025-08-08T13:51:29.029649Z",
     "shell.execute_reply": "2025-08-08T13:51:29.028992Z"
    },
    "papermill": {
     "duration": 0.029868,
     "end_time": "2025-08-08T13:51:29.030685",
     "exception": false,
     "start_time": "2025-08-08T13:51:29.000817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sherlock_resp(df, gt_df, prompt_dict, model, label_indices, base_prompt, lsd):\n",
    "    isd4 = \"d4\" in lsd['name']\n",
    "    if \"sherlock\" in model:\n",
    "        model = sherlock_model\n",
    "        data_m = pd.Series(df[label_indices].astype(str).T.values.tolist())\n",
    "        extract_features(\n",
    "            \"../temporary.csv\",\n",
    "            data_m\n",
    "        )\n",
    "        feature_vectors = pd.read_csv(\"../temporary.csv\", dtype=np.float32)\n",
    "        predicted_labels = model.predict(feature_vectors, \"sherlock\")\n",
    "        iter_len = len(data_m)\n",
    "    elif \"doduo\" in model:\n",
    "        model = doduo_model\n",
    "        data_m = df[label_indices]\n",
    "        try:\n",
    "            annot_m = doduo_model.annotate_columns(data_m)\n",
    "            predicted_labels = annot_m.coltypes\n",
    "        except Exception as e:\n",
    "            print(f\"Exception {e} in Doduo, returning default \\n\")\n",
    "            predicted_labels = [\"text\" for i in range(len(data_m))]\n",
    "        iter_len = len(predicted_labels)\n",
    "    predicted_labels_dict = {i: sherlock_to_cta.get(predicted_labels[i], [predicted_labels[i]]) for i in\n",
    "                             range(iter_len)}\n",
    "\n",
    "    for idx, label_idx in zip(range(iter_len), label_indices):\n",
    "        prompt = base_prompt + \"_\" + str(label_idx)\n",
    "        if isd4:\n",
    "            ans = predicted_labels[0]\n",
    "            label = [s.lower() for s in lsd['d4_map'][gt_df]]\n",
    "        else:\n",
    "            gt_row = gt_df[gt_df['column_index'] == label_idx]\n",
    "            if len(gt_row) != 1:\n",
    "                continue\n",
    "            label = fix_labels(gt_row['label'].item(), lsd)\n",
    "            ans = [fix_labels(item, lsd) for item in predicted_labels_dict[idx]]\n",
    "        if isd4:\n",
    "            res = ans in label\n",
    "        else:\n",
    "            assert isinstance(ans, list), \"ans should be a list\"\n",
    "            res = label in ans\n",
    "        ans_dict = {\"response\": ans, \"context\": None, \"ground_truth\": label, \"correct\": res,\n",
    "                    \"orig_model_label\": predicted_labels[idx]}\n",
    "        prompt_dict[prompt] = ans_dict\n",
    "    return prompt\n",
    "\n",
    "\n",
    "@retry(Exception, tries=3, delay=3)\n",
    "def get_chatgpt_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None,\n",
    "                     method=[\"similarity\"], max_len=15000):\n",
    "    fixed_labels = [fix_labels(s, lsd) for s in lsd['label_set']]\n",
    "    model = \"gpt-3.5\"\n",
    "    context_labels = \", \".join(fixed_labels)\n",
    "    fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n",
    "    prompt = prompt_context_insert(context_labels, context, max_len, \"gpt-3.5\")\n",
    "    d_p = prompt_dict.get(prompt, -1)\n",
    "    if d_p != -1 and \"skip-existing\" in method:\n",
    "        #recompute_results(prompt_dict, prompt, model, cbc_pred=None, label_set=lsd)\n",
    "        return prompt\n",
    "    elif d_p != -1:\n",
    "        while prompt_dict.get(prompt, -1) != -1:\n",
    "            prompt = prompt + \"*\"\n",
    "    if response:\n",
    "        ans = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0,\n",
    "        ).choices[0]['message']['content']\n",
    "        #print(f\"Original ans is {ans}\")\n",
    "    ans_n = fuzzy_label_match(ans, fixed_labels, None, None, prompt, lsd, model, method=method)\n",
    "    #print(f\"Fuzzy ans is {ans_n}\")\n",
    "    res = ans_n == ground_truth\n",
    "    ans_dict = {\"response\": ans_n, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res,\n",
    "                \"original_model_answer\": ans}\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "    return prompt\n",
    "\n",
    "\n",
    "@retry(Exception, tries=5, delay=3)\n",
    "def get_ada_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None):\n",
    "    prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"ada-personal\")\n",
    "    if prompt_dict.get(prompt, -1) != -1:\n",
    "        #recompute_results(prompt_dict, prompt, \"ada-personal\", label_set=lsd)\n",
    "        return prompt\n",
    "    if response:\n",
    "        proc = subprocess.run(\n",
    "            [\"openai\", \"api\", \"completions.create\", \"-m\", \"ada:ft-personal:-2023-03-14-11-52-45\", \"-M\", \"3\", \"-p\",\n",
    "             prompt], capture_output=True, check=True)\n",
    "        ans = proc.stdout.decode(\"utf-8\")[len(prompt):].strip()\n",
    "    else:\n",
    "        ans = \"\"\n",
    "    res = ans.lower().strip().startswith(ground_truth)\n",
    "    ans_dict = {\"response\": ans, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res}\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "    return prompt\n",
    "\n",
    "\"\"\"\n",
    "def call_llama_model(session, link, prompt, lsd, var_params):\n",
    "    fixed_labels = [fix_labels(s, lsd) for s in lsd['label_set']]\n",
    "    if session:\n",
    "        ans = session.post(link, json=make_json(prompt, var_params))\n",
    "    else:\n",
    "        ans = requests.post(link, json=make_json(prompt, var_params))\n",
    "    ans = ans.json()[\"data\"]\n",
    "    ans_n = fix_labels(ans[0][len(prompt):].strip(), lsd)\n",
    "    return ans_n\n",
    "\"\"\"\n",
    "\n",
    "def extract_answer_llama(orig_ans: str) -> str:\n",
    "    pattern = re.compile(r\"\\n\\n(?:\\*\\*([^*]+)\\*\\*|(\\w+))\\n\\n\")\n",
    "    \n",
    "    for m in pattern.finditer(orig_ans):\n",
    "        word = m.group(1) or m.group(2)\n",
    "        return word\n",
    "\n",
    "    #return word\n",
    "    #print(word)\n",
    "\n",
    "\n",
    "def call_llama_model(session, link, prompt, lsd, var_params):\n",
    "    # Build the payload expected by the new LLaMA endpoint\n",
    "    payload = {\n",
    "        \"model\":      \"llama3.1:8b-instruct-q8_0\",\n",
    "        \"prompt\":     prompt,\n",
    "        \"max_tokens\": 30,\n",
    "        \"stream\":     False\n",
    "    }\n",
    "\n",
    "    # Choose session-based or direct requests call\n",
    "    client = session or requests\n",
    "    resp = client.post(link, json=payload)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    # Extract the generated text\n",
    "    data = resp.json()\n",
    "    text = data.get(\"response\", \"\")\n",
    "\n",
    "    return extract_answer_llama(text.strip())\n",
    "    # Apply your label-fixing routine\n",
    "    #return fix_labels(text.strip(), lsd)\n",
    "\n",
    "temperature = 0\n",
    "top_p = 0\n",
    "\n",
    "def extract_answer(orig_ans: str) -> str:\n",
    "    \"\"\"\n",
    "    If orig_ans contains 'ANSWER:...', return the text after the colon.\n",
    "    Otherwise, return orig_ans unchanged (stripped).\n",
    "    \"\"\"\n",
    "    m = re.search(r\"ANSWER\\s*:\\s*(.*)\", orig_ans, re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return orig_ans.strip()\n",
    "\n",
    "@retry(Exception, tries=3, delay=3)\n",
    "def get_topp_resp(prompt, k):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "\n",
    "    temperature = 0.1 * k\n",
    "    top_p   = 0.90 - (0.1 * k)\n",
    "\n",
    "    outputs = base_model.generate(inputs,\n",
    "                                  max_length=MAX_LEN,\n",
    "                                  #do_sample=False,\n",
    "                                  #num_beams=1\n",
    "                                  temperature=temperature,\n",
    "                                  top_p=top_p,\n",
    "                                  repetition_penalty=1.3\n",
    "                                  )\n",
    "    orig_ans = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return extract_answer(orig_ans)\n",
    "    # 2. Second - Fine-Tuned\n",
    "\n",
    "    \"\"\"\n",
    "    m = re.search(r\"ANSWER\\s*:\\s*(.*)\", orig_ans)\n",
    "    if m:\n",
    "        # group(1) is “whatever comes after the colon, up until the end of the line”\n",
    "        after = m.group(1).strip()\n",
    "        # If after is non‐empty, split on whitespace or newline to get the first token\n",
    "        if after:\n",
    "            predicted = after.split()[0]\n",
    "        else:\n",
    "            # Matched “ANSWER:” but there was nothing after it\n",
    "            predicted = \"Didn't printed answer!\"\n",
    "    else:\n",
    "        # No “ANSWER:” at all in full_text\n",
    "        predicted = \"NO ANSWER PROVIDED!\"\n",
    "\n",
    "    return predicted\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # 3. Full answer\n",
    "    if \"ANSWER:\" in orig_ans:\n",
    "        # keep the colon, if you like, otherwise change to split(\"ANSWER:\", 1)[1]\n",
    "        after = orig_ans.split(\"ANSWER:\", 1)[1].strip()\n",
    "    else:\n",
    "        after = orig_ans.strip()\n",
    "    \n",
    "    return after\n",
    "     \"\"\"\n",
    "\n",
    "\n",
    "@retry(Exception, tries=3, delay=3)\n",
    "def get_llama_resp(lsd: dict, context: list, ground_truth: str, prompt_dict: dict, link: str, response=True,\n",
    "                   session=None, cbc=None, model=\"llama\", limited_context=None,\n",
    "                   method=[\"ans_contains_gt\", \"gt_contains_ans\", \"resample\"]):\n",
    "    #print(f\"in get llama resp, gt is {ground_truth}, context is {context}\")\n",
    "    isd4 = \"d4\" in lsd['name']\n",
    "    if isd4:\n",
    "        gtv = lsd['d4_map'][ground_truth]\n",
    "        if isinstance(gtv, str):\n",
    "            gtv = [gtv]\n",
    "        ground_truth = [s.lower() for s in gtv]\n",
    "    if \"hierarchical\" in method and not isd4:\n",
    "        dtype = get_base_dtype(limited_context)\n",
    "        fixed_labels = sotab_top_hier[dtype]\n",
    "    else:\n",
    "        fixed_labels = list(set([fix_labels(s, lsd) for s in lsd['label_set']]))\n",
    "    context_labels = \", \".join(fixed_labels)\n",
    "    fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n",
    "    if model in [\"llama-zs\", \"opt-iml-30b-zs\"]:\n",
    "        pipe, local_llm, llm_chain = set_pipeline(k=1)\n",
    "    prompt = prompt_context_insert(context_labels, context, MAX_LEN, model)\n",
    "    d_p = prompt_dict.get(prompt, -1)\n",
    "    #skip existing logic\n",
    "    if d_p != -1 and \"skip-existing\" in method:\n",
    "        # recompute_results(prompt_dict, prompt, \"llama\", cbc, lsd)\n",
    "        return prompt\n",
    "    elif d_p != -1:\n",
    "        while prompt_dict.get(prompt, -1) != -1:\n",
    "            prompt = prompt + \"*\"\n",
    "    #response logic\n",
    "    if not response:\n",
    "        orig_ans = ans_n = \"\"\n",
    "    else:\n",
    "        orig_ans = apply_basic_rules(limited_context, None)\n",
    "        if orig_ans is None:\n",
    "            orig_ans = query_correct_model(model, prompt, context_labels, context, session, link, lsd)\n",
    "            #hierarchical matching logic\n",
    "            if \"hierarchical\" in method and dtype == \"other\" and orig_ans not in ['email', 'URL', 'WebHTMLAction',\n",
    "                                                                                  'Photograph']:\n",
    "                next_label_set = sotab_other_hier.get(orig_ans, -1)\n",
    "                if next_label_set == -1:\n",
    "                    print(f\"Original answer {orig_ans} not found in hierarchy\")\n",
    "                    next_label_set = sotab_other_hier['text']\n",
    "                fixed_labels = list(set([fix_labels(s, lsd) for s in next_label_set]))\n",
    "                context_labels = \", \".join(fixed_labels)\n",
    "                fixed_labels = sorted(fixed_labels, key=len, reverse=True)\n",
    "                orig_ans = query_correct_model(model, prompt, context_labels, context, session, link, lsd)\n",
    "                #fuzzy matching logic\n",
    "            ans_n = fuzzy_label_match(orig_ans, fixed_labels, session, link, prompt, lsd, model, method=method).lower()\n",
    "        else:\n",
    "            ans_n = orig_ans.lower()\n",
    "    #print(f\"final label set was {fixed_labels}, prediction was {ans_n}, ground truth was {ground_truth} \\n\")\n",
    "    if isd4:\n",
    "        res = ans_n in ground_truth\n",
    "    else:\n",
    "        res = ans_n == ground_truth\n",
    "    ans_dict = {\"response\": ans_n, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res,\n",
    "                \"original_model_answer\": orig_ans}\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "    #recompute_results(prompt_dict, prompt, \"llama\", cbc, lsd)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "@retry(Exception, tries=5, delay=3)\n",
    "def get_bloomz_resp(lsd: dict, context: str, ground_truth: str, prompt_dict: dict, response=True, session=None):\n",
    "    prompt = prompt_context_insert(context_labels, context, 2000, \"bloomz\")\n",
    "    if prompt_dict.get(prompt, -1) != -1:\n",
    "        return prompt\n",
    "    if response:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda:0\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=5)\n",
    "    else:\n",
    "        response = \"\"\n",
    "    ans = tokenizer.decode(outputs[0]).split()[-1]\n",
    "    ans = ''.join(e for e in ans if e.isalnum()).lower()\n",
    "    res = ans == ground_truth\n",
    "    ans_dict = {\"response\": ans, \"context\": context, \"ground_truth\": ground_truth, \"correct\": res}\n",
    "    prompt_dict[prompt] = ans_dict\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85950645-b679-4139-baed-fcaa30e91aa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:59:02.128891Z",
     "start_time": "2025-07-23T08:59:02.088955Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:29.044356Z",
     "iopub.status.busy": "2025-08-08T13:51:29.044079Z",
     "iopub.status.idle": "2025-08-08T13:51:29.076419Z",
     "shell.execute_reply": "2025-08-08T13:51:29.075794Z"
    },
    "papermill": {
     "duration": 0.039098,
     "end_time": "2025-08-08T13:51:29.077350",
     "exception": false,
     "start_time": "2025-08-08T13:51:29.038252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_integer(val):\n",
    "    return pd.to_numeric(val, downcast='integer', errors='ignore')\n",
    "\n",
    "\n",
    "def derive_meta_features(col):\n",
    "    features = {}\n",
    "    if not col.astype(str).apply(str.isnumeric).all():\n",
    "        return {\"std\": round(col.astype(str).str.len().std(), 2), \"mean\": round(col.astype(str).str.len().mean(), 2),\n",
    "                \"mode\": col.astype(str).str.len().mode().iloc[0].item(), \"median\": col.astype(str).str.len().median(),\n",
    "                \"max\": col.astype(str).str.len().max(), \"min\": col.astype(str).str.len().min(),\n",
    "                \"rolling-mean-window-4\": [0.0]}\n",
    "    col = col.dropna().astype(float)\n",
    "    if col.apply(float.is_integer).all():\n",
    "        col = col.astype(int)\n",
    "    #print(f\"Collecting metafeatures for column {col} \\n\")\n",
    "    features['std'] = round(col.std(), 2)\n",
    "    features['mean'] = round(col.mean(), 2)\n",
    "    features['mode'] = col.mode().iloc[0].item()\n",
    "    features['median'] = col.median()\n",
    "    features['max'] = col.max()\n",
    "    features['min'] = col.min()\n",
    "    indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=4)\n",
    "    features['rolling-mean-window-4'] = list(col.rolling(window=indexer, min_periods=1).mean())\n",
    "    return features\n",
    "\n",
    "\n",
    "def fix_mode(d):\n",
    "    if isinstance(d['mode'], pd.Series):\n",
    "        d['mode'] = d['mode'].loc[0].item()\n",
    "    return d\n",
    "\n",
    "\n",
    "def split_meta_features(d):\n",
    "    return pd.Series(\n",
    "        [d.get('std', \"N/A\"), d.get('mean', \"N/A\"), d.get('median', \"N/A\"), d.get('mode', \"N/A\"), d.get('max', \"N/A\"),\n",
    "         d.get('min', \"N/A\")])\n",
    "\n",
    "\n",
    "def prompt_context_insert(context_labels: str, context: str, max_len: int = 2000, model: str = \"gpt-3.5\"):\n",
    "    if model == \"bloomz\":\n",
    "        s = f'SYSTEM: You are an AI research assistant. You use a tone that is technical and scientific. USER: Please select the field from {context_labels} which best describes the context below. Respond with the name of the field and nothing else. \\n CONTEXT: {context}'\n",
    "    elif model == \"gpt-3.5\":\n",
    "        s = f'SYSTEM: Please select the field from {context_labels} which best describes the context. Respond only with the name of the field. \\n CONTEXT: {context}'\n",
    "    elif model == \"ada-personal\":\n",
    "        s = f'{context}$'\n",
    "    elif model == \"llama-old\":\n",
    "        s = f'INSTRUCTION: Select the field from the category which matches the input. \\n CATEGORIES: {context_labels} \\n INPUT:{context} \\n OUTPUT: '\n",
    "    elif \"-zs\" in model:\n",
    "        ct = \"[\" + \", \".join(context).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")[\n",
    "                   :MAX_LEN - 100 - len(context_labels)] + \"]\"\n",
    "        lb = \"\\n\".join([\"- \" + c for c in context_labels.split(\", \")])\n",
    "        #s = f'How might one classify the following input? \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n ANSWER:'\n",
    "        if model == \"opt-iml-max-30b-zs\":\n",
    "            s = f'Select the option which best describes the input. \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n'\n",
    "        else:\n",
    "            # Original prompt\n",
    "            s = f'INSTRUCTION: Select the option which best describes the input. \\n INPUT: {ct} .\\n OPTIONS:\\n {lb} \\n ANSWER:'\n",
    "            \n",
    "    elif model == \"llama\":\n",
    "        ct = \"[\" + \", \".join(context).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")[\n",
    "                   :MAX_LEN - 100 - len(context_labels)] + \"]\"\n",
    "        lb = \"\\n\".join([\"- \" + c for c in context_labels.split(\", \")])\n",
    "        \n",
    "        s = f'INSTRUCTION: Select the category which best matches the input. \\n INPUT:{context} .\\n OPTIONS:\\n{lb} \\n CATEGORY: '\n",
    "\n",
    "    elif model == \"llama-retry\":\n",
    "        s = f'INSTRUCTION: Select the category which best matches the input. \\n INPUT:{context} \\n CATEGORY: '\n",
    "    elif model == \"alpaca-fine-tuned\":\n",
    "        ct = \"[\" + \", \".join(context).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")[\n",
    "                   :MAX_LEN - 100 - len(context_labels)] + \"]\"\n",
    "        lb = \"\\n\".join([\"- \" + c for c in context_labels.split(\", \")])\n",
    "\n",
    "        s = f'INSTRUCTION: Select the category which best matches the input. \\n INPUT:{context} .\\n OPTIONS:\\n{lb} \\n CATEGORY:'\n",
    "    #Truncate if prompt exceeds maximum length\n",
    "    if len(s) > max_len:\n",
    "        s = s[:max_len - 3]\n",
    "    return s\n",
    "\n",
    "\n",
    "def recompute_results(prompt_dict, prompt, model_str, cbc_pred, label_set):\n",
    "    dict_val = prompt_dict.get(prompt, -1)\n",
    "    dict_val['cbc_pred'] = cbc_pred\n",
    "    if model_str == \"llama\":\n",
    "        if cbc_pred and (cbc_pred in catboost_cats):\n",
    "            print(f\"using cbcpred label: {cbc_pred} \\n\")\n",
    "            dict_val['response'] = fix_labels(cbc_pred, label_set)\n",
    "        dict_val['correct'] = ((dict_val['ground_truth'] == dict_val['response']) or (\n",
    "                    dict_val['response'] and (dict_val['response']) in dict_val['ground_truth']))\n",
    "    prompt_dict[prompt] = dict_val\n",
    "\n",
    "\n",
    "def make_json(prompt, var_params):\n",
    "    p = deepcopy(params)\n",
    "    if var_params:\n",
    "        for k, v in var_params.items():\n",
    "            p[k] = v\n",
    "    return {\n",
    "        \"data\": [\n",
    "            prompt,\n",
    "            p['max_new_tokens'],\n",
    "            p['do_sample'],\n",
    "            p['temperature'],\n",
    "            p['top_p'],\n",
    "            p['typical_p'],\n",
    "            p['repetition_penalty'],\n",
    "            p['encoder_repetition_penalty'],\n",
    "            p['top_k'],\n",
    "            p['min_length'],\n",
    "            p['no_repeat_ngram_size'],\n",
    "            p['num_beams'],\n",
    "            p['penalty_alpha'],\n",
    "            p['length_penalty'],\n",
    "            p['early_stopping'],\n",
    "            p['seed'],\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def ans_contains_gt(ans_n, fixed_labels):\n",
    "    for fixed_label in fixed_labels:\n",
    "        if fixed_label in ans_n:\n",
    "            print(f\"Fuzzy label {ans_n} contains gt label {fixed_label}: MATCH \\n\")\n",
    "            ans_n = fixed_label\n",
    "            return ans_n\n",
    "    return None\n",
    "\n",
    "\n",
    "def gt_contains_ans(ans_n, fixed_labels):\n",
    "    if ans_n == \"\":\n",
    "        return None\n",
    "    for fixed_label in fixed_labels:\n",
    "        if ans_n in fixed_label:\n",
    "            print(f\"GT label {fixed_label} contains fuzzy label {ans_n}: MATCH \\n\")\n",
    "            ans_n = fixed_label\n",
    "            return ans_n\n",
    "    return None\n",
    "\n",
    "\n",
    "def basic_contains(ans_n, fixed_labels, method):\n",
    "    #TODO: not sure the order should be fixed like this, could be made flexible\n",
    "    if ans_n in fixed_labels:\n",
    "        return ans_n\n",
    "    if \"ans_contains_gt\" in method:\n",
    "        res = ans_contains_gt(ans_n, fixed_labels)\n",
    "        if res:\n",
    "            return res\n",
    "    if \"gt_contains_ans\" in method:\n",
    "        res = gt_contains_ans(ans_n, fixed_labels)\n",
    "        if res:\n",
    "            return res\n",
    "    return None\n",
    "\n",
    "\n",
    "def fuzzy_label_match(orig_ans, fixed_labels, session, link, prompt, lsd, model,\n",
    "                      method=[\"ans_contains_gt\", \"gt_contains_ans\", \"resample\"]):\n",
    "    #answer is already in label set, no fuzzy match needed\n",
    "    ans_n = fix_labels(orig_ans, lsd)\n",
    "    res = basic_contains(ans_n, fixed_labels, method)\n",
    "    if res:\n",
    "        return res\n",
    "    if \"similarity\" in method:\n",
    "        ans_embedding = sent_model.encode(ans_n)\n",
    "        lbl_embeddings = sent_model.encode(fixed_labels)\n",
    "        sims = {lbl: util.pytorch_cos_sim(ans_embedding, le) for lbl, le in zip(fixed_labels, lbl_embeddings)}\n",
    "        return max(sims, key=sims.get)\n",
    "    if \"resample\" in method:\n",
    "        #fuzzy label matching strategy\n",
    "        for k in range(2, 6):\n",
    "            if \"gpt\" in model:\n",
    "                ans_n = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    temperature=0 + k / 10,\n",
    "                ).choices[0]['message']['content'].lower()\n",
    "            elif model in [\"llama-zs\", \"opt-iml-30b-zs\"]:\n",
    "                pipe, local_llm, llm_chain = set_pipeline(k=k)\n",
    "                ans_n = llm_chain.run(prompt)\n",
    "            elif model in [\"topp-zs\", \"flan-ul2-zs\"]:\n",
    "                ans_n = get_topp_resp(prompt, k)\n",
    "            else:\n",
    "                rep_pen = params['repetition_penalty']\n",
    "                top_p = params['top_p']\n",
    "                temp = params['temperature']\n",
    "                ans_n = call_llama_model(session, link, prompt, lsd,\n",
    "                                         {'no_repeat_ngram_size': 1, 'top_p': top_p - (0.1 * k), 'temperature': 0.9})\n",
    "                params['top_p'] = top_p\n",
    "                params['temperature'] = temp\n",
    "            res = basic_contains(ans_n, fixed_labels, method)\n",
    "            if res:\n",
    "                return res\n",
    "    #print(\"Applying fallback label, 'text' \\n\")\n",
    "    return 'text'\n",
    "\n",
    "\n",
    "INTEGER_SET = set(r\"0123456789,/\\+-.^_()[] :\")\n",
    "\n",
    "\n",
    "def get_base_dtype(context):\n",
    "    dtype = \"integer\"\n",
    "    for item in context:\n",
    "        if not all(char in INTEGER_SET for char in item):\n",
    "            #print(f\"String is OTHER because: {[char for char in item if char not in INTEGER_SET]}\")\n",
    "            return \"other\"\n",
    "        try:\n",
    "            if item.endswith(\".0\") or item.endswith(\",0\"):\n",
    "                item = item[:-2]\n",
    "                item = str(int(item))\n",
    "            if item.endswith(\".00\") or item.endswith(\",00\"):\n",
    "                item = item[:-3]\n",
    "                item = str(int(item))\n",
    "        except:\n",
    "            return \"float\"\n",
    "        temp_item = re.sub(r\"[^a-zA-Z0-9.]\", \"\", item)\n",
    "        if not temp_item.isdigit():\n",
    "            #print(f\"string is FLOAT because {temp_item} is not an integer\")\n",
    "            dtype = \"float\"\n",
    "    return dtype\n",
    "\n",
    "\n",
    "def query_correct_model(model, prompt, context_labels, context, session, link, lsd):\n",
    "    if model in [\"llama-zs\", \"opt-iml-max-30b-zs\"]:\n",
    "        orig_ans = llm_chain.run(prompt)\n",
    "        if orig_ans is None:\n",
    "            prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"llama-retry\")\n",
    "            orig_ans = llm_chain.run(prompt)\n",
    "    elif model in [\"topp-zs\", \"flan-ul2-zs\"]:\n",
    "        orig_ans = get_topp_resp(prompt, 1)\n",
    "    else:\n",
    "        orig_ans = call_llama_model(session, link, prompt, lsd, None)\n",
    "        if orig_ans is None:\n",
    "            prompt = prompt_context_insert(context_labels, context, MAX_LEN, \"llama-retry\")\n",
    "            orig_ans = call_llama_model(session, link, prompt, lsd, None)\n",
    "    return orig_ans\n",
    "\n",
    "\n",
    "def get_df_sample_col(col, rand_seed, len_context, min_variance=2, replace=False):\n",
    "    df = pd.Series(col)\n",
    "    ignore_list = [\"None\", 'none', 'NaN', 'nan', 'N/A', 'na', '']\n",
    "    sample_list = list(set(p[:75] for p in pd.unique(df.astype(str)[col]) if p not in ignore_list))\n",
    "    if len(sample_list) < 1:\n",
    "        return [\"None\"] * len_context\n",
    "    if len(sample_list) < len_context:\n",
    "        sample_list = sample_list * len_context\n",
    "    if len(sample_list) > len_context:\n",
    "        sample_list = sample_list[:len_context]\n",
    "    assert len(sample_list) == len_context, f\"An index in val_indices is length {len(sample_list)}\"\n",
    "    return sample_list\n",
    "\n",
    "\n",
    "def check_substr_contains_only_set(str, acceptable_chars):\n",
    "    validation = set(str)\n",
    "    print(\"Checking if it contains only \", acceptable_chars)\n",
    "    if validation.issubset(acceptable_chars):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def insert_source(context, fname):\n",
    "    pattern = r\"_([^_]*)_\"  # Matches substrings that start and end with \"_\"\n",
    "    matcher = re.search(pattern, fname)\n",
    "    addstr = str(matcher.group()).replace(\"_\", \"\").split(\".\")[0]\n",
    "    #context.insert(0, \"SRC_FILE: \" + addstr + \"COL_VALS: \")\n",
    "    context.insert(0, \"SRC: \" + addstr)\n",
    "    return context\n",
    "\n",
    "\n",
    "def get_df_sample(df, rand_seed, val_indices, len_context, min_variance=1, replace=False, full=False, other_col=False,\n",
    "                  max_len=8000):\n",
    "    column_samples = {}\n",
    "    ignore_list = [\"None\", 'none', 'NaN', 'nan', 'N/A', 'na', '']\n",
    "    for col in df.columns:\n",
    "        sample_list = list(\n",
    "            set(p[:max_len // (len_context * 3)] for p in pd.unique(df.astype(str)[col]) if p not in ignore_list))\n",
    "        #reformat integer samples\n",
    "        sl_mod = []\n",
    "        # Meta-features\n",
    "        if full:\n",
    "            meta_features = derive_meta_features(df[col])\n",
    "            meta_features['rolling-mean-window-4'] = meta_features['rolling-mean-window-4'][:5]\n",
    "        # Sampling from other columns\n",
    "        if other_col:\n",
    "            sample_list_fill_size = len_context - len(sample_list)\n",
    "            nc = len(df.columns)\n",
    "            per_column_context = max(1, sample_list_fill_size // nc)\n",
    "            for idx, oc in enumerate(df.columns):\n",
    "                items = df[oc].astype(str).iloc[0:per_column_context].tolist()\n",
    "                sample_list = sample_list + [\"OC: \" + str(item) for item in items]\n",
    "        if not sample_list:\n",
    "            sample_list = [\"None\"]\n",
    "        if len(sample_list) < len_context:\n",
    "            sample_list = sample_list * len_context\n",
    "        if len(sample_list) > len_context:\n",
    "            sample_list = sample_list[:len_context]\n",
    "        assert len(sample_list) == len_context, \"An index in val_indices is length \" + str(len(sample_list))\n",
    "        if full:\n",
    "            if meta_features['std'] == \"N/A\":\n",
    "                sample_list = sample_list + [\"\" for k, v in meta_features.items()]\n",
    "            else:\n",
    "                sample_list = sample_list + [str(k) + \": \" + str(v) for k, v in meta_features.items()]\n",
    "        # print(\"sample list\")\n",
    "        # print(sample_list)\n",
    "        column_samples[col] = sample_list\n",
    "        # print(\"column samples\")\n",
    "        # print(column_samples)\n",
    "    return pd.DataFrame.from_dict(column_samples)\n",
    "\n",
    "\n",
    "NUMERIC_AND_COMMA = set('0123456789,')\n",
    "\n",
    "BOOLEAN_SET = [\"True\", \"true\", \"False\", \"false\", \"yes\", \"Yes\", \"No\", \"no\"]\n",
    "\n",
    "\n",
    "def apply_basic_rules(context, lbl):\n",
    "    if not context:\n",
    "        return lbl\n",
    "    if not isinstance(context, list):\n",
    "        return lbl\n",
    "    try:\n",
    "        if all(s.endswith(\" g\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" kg\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" lb\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" lbs\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" pounds\") for s in context):\n",
    "            lbl = \"weight\"\n",
    "        if all(s.endswith(\" cal\") for s in context):\n",
    "            lbl = \"calories\"\n",
    "        if all(s.endswith(\" kcal\") for s in context):\n",
    "            lbl = \"calories\"\n",
    "        if all(s.endswith(\" calories\") for s in context):\n",
    "            lbl = \"calories\"\n",
    "        if all(\"review\" in s.lower() for s in context):\n",
    "            lbl = \"review\"\n",
    "        if all(\"recipe\" in s.lower() for s in context):\n",
    "            lbl = \"recipe\"\n",
    "        if lbl and \"openopen\" in lbl:\n",
    "            lbl = \"openinghours\"\n",
    "        if all(s in BOOLEAN_SET for s in context):\n",
    "            lbl = \"medical_boolean\"\n",
    "        return lbl\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} in apply_basic_rules with context {context}\")\n",
    "        return lbl\n",
    "\n",
    "\n",
    "def get_cbc_pred(orig_label, numeric_labels):\n",
    "    try:\n",
    "        #FOR VALIDATION\n",
    "        #cbc_filematch = dfv[dfv['df_path'] == str(f)]\n",
    "        #FOR TEST SET\n",
    "        cbc_filematch = dft[dft['df_path'] == str(f)]\n",
    "        cbc_labelmatch = cbc_filematch[cbc_filematch['label'] == orig_label]\n",
    "        if len(cbc_labelmatch) == 1:\n",
    "            cbc_pred = numeric_labels[cbc_labelmatch['preds'].item()]\n",
    "        else:\n",
    "            cbc_pred = None\n",
    "    except Exception as e:\n",
    "        print(\"cbc excpetion: \")\n",
    "        print(e)\n",
    "        cbc_pred = None\n",
    "\n",
    "\n",
    "def run_val(model: str, save_path: str, inputs: list, label_set: list, input_df: pd.DataFrame, resume: bool = True,\n",
    "            results: bool = True, stop_early: int = -1, rand_seed: int = 13, sample_size: int = 5, link: str = None,\n",
    "            response: bool = True, summ_stats: bool = False, table_src: bool = False, other_col: bool = False,\n",
    "            skip_short: bool = False, min_var: int = 0, method: list = [\"similarity\"]):\n",
    "    inputs = [Path(f) for f in inputs]\n",
    "\n",
    "    infmods = \"sherlock\" in model or \"doduo\" in model\n",
    "    isd4 = \"d4\" in label_set['name']\n",
    "    if resume and os.path.isfile(save_path):\n",
    "        with open(save_path, 'r', encoding='utf-8') as f:\n",
    "            prompt_dict = json.load(f)\n",
    "    else:\n",
    "        prompt_dict = {}\n",
    "    s = requests.Session()\n",
    "    if \"-zs\" in model:\n",
    "        base_model.eval()\n",
    "    if isinstance(inputs, dict):\n",
    "        labels = [\"_\".join(k.split(\"_\")[:-1]) for k in inputs.keys()]\n",
    "        inputs = list(inputs.values())\n",
    "    for idx, f in tqdm(enumerate(inputs), total=len(inputs)):\n",
    "        if idx % 100 == 0:\n",
    "            with open(save_path, 'w', encoding='utf-8') as alt_f:\n",
    "                #print(\"pd\", prompt_dict, \"\\n\")\n",
    "                json.dump(prompt_dict, alt_f, ensure_ascii=False, indent=4)\n",
    "        if stop_early > -1 and idx == stop_early:\n",
    "            break\n",
    "        if isd4:\n",
    "            f_df = f\n",
    "            label_indices = [2]\n",
    "            gt_labels = labels[idx]\n",
    "        else:\n",
    "            gt_labels = input_df[input_df['table_name'] == f.name]\n",
    "            label_indices = gt_labels['column_index'].unique().tolist()\n",
    "\n",
    "            if f.suffix.lower() == '.csv':\n",
    "                f_df = pd.read_csv(f)\n",
    "            else:\n",
    "                f_df = pd.read_json(f, compression='infer', lines=True)\n",
    "\n",
    "        if infmods:\n",
    "            label_indices = [\"values\"]\n",
    "            key = get_sherlock_resp(f_df, gt_labels, prompt_dict, model, label_indices, str(f), label_set)\n",
    "            continue\n",
    "        sample_df = get_df_sample(f_df, rand_seed, label_indices, sample_size, full=summ_stats, other_col=other_col,\n",
    "                                  max_len=MAX_LEN)\n",
    "        #print(f\"in main loop, sample_df is {sample_df}\")\n",
    "        f_df_cols = f_df.columns\n",
    "        for idx, col in enumerate(f_df_cols):\n",
    "            if idx not in label_indices:\n",
    "                continue\n",
    "            #NOTE: skipping evaluation for columns with insufficient variance in the column\n",
    "            #       if len(pd.unique(sample_df.astype(str)[col])) < min_var:\n",
    "            #         continue\n",
    "            if isd4:\n",
    "                orig_label = gt_labels\n",
    "            else:\n",
    "                gt_row = gt_labels[gt_labels['column_index'] == idx]\n",
    "                orig_label = gt_row['label'].item()\n",
    "            label = fix_labels(orig_label, label_set)\n",
    "            limited_context = sample_df[col].tolist()[:sample_size]\n",
    "            #NOTE: could consider using min_var here\n",
    "            #if full and len(pd.unique(sample_df[col].tolist())) < 3:\n",
    "            if table_src:\n",
    "                context = insert_source(sample_df[col].tolist(), f.name)\n",
    "            else:\n",
    "                context = sample_df[col].tolist()\n",
    "            if \"gpt-3.5\" in model:\n",
    "                key = get_chatgpt_resp(label_set, context, label, prompt_dict, response=response, session=s,\n",
    "                                       method=method)\n",
    "            elif \"ada-personal\" in model:\n",
    "                key = get_ada_resp(label_set, context, label, prompt_dict, response=response, session=s)\n",
    "            elif \"bloomz\" in model:\n",
    "                key = get_bloomz_resp(label_set, context, label, prompt_dict, response=response, session=s)\n",
    "            elif \"llama\" in model or \"-zs\" in model:\n",
    "                #cbc_pred = get_cbc_pred(orig_label, numeric_labels)\n",
    "                cbc_pred = None\n",
    "                key = get_llama_resp(label_set, context, label, prompt_dict, link=link, response=response, session=s,\n",
    "                                     cbc=cbc_pred, model=model, limited_context=limited_context, method=method)\n",
    "                # print(\"Key: \", key, \"\\n\")\n",
    "                #print(\"pdk\", prompt_dict[key], \"\\n\")\n",
    "            prompt_dict[key]['original_label'] = orig_label\n",
    "            prompt_dict[key]['file+idx'] = str(f) + \"_\" + str(idx)\n",
    "    with open(save_path, 'w', encoding='utf-8') as my_f:\n",
    "        json.dump(prompt_dict, my_f, ensure_ascii=False, indent=4)\n",
    "    if results:\n",
    "        results_checker(save_path, skip_duplicates=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "392f663e-4f8f-4671-ab88-21575e2fffbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:59:02.170496Z",
     "start_time": "2025-07-23T08:59:02.168746Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:29.100915Z",
     "iopub.status.busy": "2025-08-08T13:51:29.100482Z",
     "iopub.status.idle": "2025-08-08T13:51:29.103689Z",
     "shell.execute_reply": "2025-08-08T13:51:29.103035Z"
    },
    "papermill": {
     "duration": 0.016212,
     "end_time": "2025-08-08T13:51:29.104798",
     "exception": false,
     "start_time": "2025-08-08T13:51:29.088586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02993b81-f6b9-4b7b-b14f-a0ba901faef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:59:02.264190Z",
     "start_time": "2025-07-23T08:59:02.245511Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:29.128725Z",
     "iopub.status.busy": "2025-08-08T13:51:29.128395Z",
     "iopub.status.idle": "2025-08-08T13:51:29.160598Z",
     "shell.execute_reply": "2025-08-08T13:51:29.159758Z"
    },
    "papermill": {
     "duration": 0.045246,
     "end_time": "2025-08-08T13:51:29.161614",
     "exception": false,
     "start_time": "2025-08-08T13:51:29.116368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from statistics import mean\n",
    "\n",
    "ENDINGS = [\"ANSWER:\", \"CATEGORY:\"]\n",
    "\n",
    "\n",
    "def results_checker_doduo(file_name, skip_duplicates=True):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "    correct = 0\n",
    "    n = len(d)\n",
    "    per_class_results = dict()\n",
    "    for k, v in d.items():\n",
    "        response_set = set(v[\"response\"])\n",
    "        for r in response_set:\n",
    "            per_class_results.setdefault(r, {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"Total\": 0})\n",
    "        per_class_results.setdefault(v[\"ground_truth\"], {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"Total\": 0})\n",
    "        if v['correct'] == True:\n",
    "            correct += 1\n",
    "            per_class_results[v[\"ground_truth\"]][\"TP\"] += 1\n",
    "        else:\n",
    "            per_class_results[v[\"ground_truth\"]][\"FN\"] += 1\n",
    "            for r in response_set:\n",
    "                per_class_results[r][\"FP\"] += 1\n",
    "        per_class_results[v[\"ground_truth\"]][\"Total\"] += 1\n",
    "\n",
    "    for k, v in per_class_results.items():\n",
    "        v['F1'] = (2 * v[\"TP\"]) / (2 * v[\"TP\"] + v[\"FP\"] + v[\"FN\"])\n",
    "\n",
    "    weighted_f1 = sum([v[\"F1\"] * v[\"Total\"] for k, v in per_class_results.items()]) / n\n",
    "    unweighted_f1 = mean([v[\"F1\"] for k, v in per_class_results.items()])\n",
    "\n",
    "    print(\n",
    "        f\"Total entries: {n} \\n Accuracy: {round(correct / n, 4)} \\n Weighted F1: {round(weighted_f1, 4)} \\n Unweighted F1: {round(unweighted_f1, 4)}\")\n",
    "\n",
    "\n",
    "def results_checker(file_name, inference_time, skip_duplicates=False):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    if skip_duplicates:\n",
    "        d = {k: v for k, v in d.items() if \"CATEGORY: *\" not in str(k)}\n",
    "\n",
    "    # build the lists\n",
    "    y_true = [v[\"ground_truth\"] for v in d.values()]\n",
    "    y_pred = [v[\"response\"] for v in d.values()]\n",
    "\n",
    "    # overall stats\n",
    "    correct = sum(1 for gt, pred in zip(y_true, y_pred) if gt == pred)\n",
    "    n = len(y_true)\n",
    "    print(f\"Total entries: {n}\")\n",
    "    print(f\"Accuracy:     {correct / n:.4f}\\n\")\n",
    "\n",
    "    # per-class report\n",
    "    print(classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        digits=4,  # 4 decimal places\n",
    "        zero_division=0  # to avoid warnings if a class is never predicted\n",
    "    ))\n",
    "\n",
    "    # --- new: build a flattened metrics dict ---\n",
    "    raw_report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    flat = {}\n",
    "    # raw_report has keys for each label, plus 'macro avg', 'weighted avg', and 'accuracy'\n",
    "    for label, m in raw_report.items():\n",
    "        if label == \"accuracy\":\n",
    "            flat[\"accuracy\"] = m\n",
    "        else:\n",
    "            for metric_name, val in m.items():\n",
    "                flat[f\"{label}_{metric_name}\"] = val\n",
    "\n",
    "    # add summary fields\n",
    "    flat[\"total_entries\"] = n\n",
    "    # filename identifier: take it from your JSON filename variable\n",
    "    flat[\"run_name\"]      = os.path.basename(file_name).replace(\".json\",\"\")\n",
    "    flat[\"inference_time\"] = inference_time\n",
    "\n",
    "    # convert to one-row DataFrame\n",
    "    df = pd.DataFrame([flat])\n",
    "\n",
    "    metrics_csv = \"./all_metrics.csv\"\n",
    "\n",
    "    # append (or create) the master CSV\n",
    "    if not os.path.isfile(metrics_csv):\n",
    "        df.to_csv(metrics_csv, index=False, float_format=\"%.4f\")\n",
    "    else:\n",
    "        df.to_csv(metrics_csv, mode=\"a\", header=False, index=False, float_format=\"%.4f\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8db2960f-ac1d-4199-9cbc-85b6d60f66e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:59:02.313578Z",
     "start_time": "2025-07-23T08:59:02.308669Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:29.185467Z",
     "iopub.status.busy": "2025-08-08T13:51:29.185040Z",
     "iopub.status.idle": "2025-08-08T13:51:29.189876Z",
     "shell.execute_reply": "2025-08-08T13:51:29.189103Z"
    },
    "papermill": {
     "duration": 0.017939,
     "end_time": "2025-08-08T13:51:29.191003",
     "exception": false,
     "start_time": "2025-08-08T13:51:29.173064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def missing_entries(f1, f2):\n",
    "    with open(f1, \"r\") as file1:\n",
    "        d1 = json.load(file1)\n",
    "    with open(f2, \"r\") as file2:\n",
    "        d2 = json.load(file2)\n",
    "    paths1 = set([v[\"file+idx\"] for _, v in d1.items()])\n",
    "    paths2 = set([v[\"file+idx\"] for _, v in d2.items()])\n",
    "    return paths1 - paths2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23213247-0e7a-4b24-9eb5-a623264183ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T09:15:08.408318Z",
     "start_time": "2025-07-23T09:15:08.398735Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:29.215311Z",
     "iopub.status.busy": "2025-08-08T13:51:29.215060Z",
     "iopub.status.idle": "2025-08-08T13:51:29.235506Z",
     "shell.execute_reply": "2025-08-08T13:51:29.234631Z"
    },
    "papermill": {
     "duration": 0.034217,
     "end_time": "2025-08-08T13:51:29.236854",
     "exception": false,
     "start_time": "2025-08-08T13:51:29.202637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_val_parquet(\n",
    "        model: str,\n",
    "        save_path: str,\n",
    "        labels_path: str,\n",
    "        data_path: str,\n",
    "        label_set: dict,\n",
    "        resume: bool = True,\n",
    "        results: bool = True,\n",
    "        stop_early: int = -1,\n",
    "        rand_seed: int = 13,\n",
    "        sample_size: int = 5,\n",
    "        link: str = None,\n",
    "        response: bool = True,\n",
    "        summ_stats: bool = False,\n",
    "        table_src: bool = False,\n",
    "        other_col: bool = False,\n",
    "        skip_short: bool = False,\n",
    "        min_var: int = 0,\n",
    "        method: list = [\"similarity\"],\n",
    "        results_checker=None,\n",
    "        MAX_LEN: int = 1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Validation loop adapted for parquet-based inputs:\n",
    "\n",
    "    - labels_path: path to a parquet file with columns ['__index_level_0__', 'type']\n",
    "    - data_path:   path to a parquet file with columns ['__index_level_0__', 'values']\n",
    "    - label_set:   dict containing 'name', 'label_set', 'dict_map', 'abbrev_map'\n",
    "\n",
    "    Each row in the merged DataFrame represents one column to predict:\n",
    "      - __index_level_0__ (column index)\n",
    "      - type (ground truth label)\n",
    "      - values (comma-separated or list of column values)\n",
    "    \"\"\"\n",
    "\n",
    "    total_inference_time = 0.0\n",
    "    inference_times = []\n",
    "    \n",
    "    # Load or initialize cache\n",
    "    if resume and os.path.isfile(save_path):\n",
    "        with open(save_path, 'r', encoding='utf-8') as f:\n",
    "            prompt_dict = json.load(f)\n",
    "    else:\n",
    "        prompt_dict = {}\n",
    "    \n",
    "\n",
    "    # Read parquet inputs and bring index into a column\n",
    "    labels_df = pd.read_parquet(labels_path).reset_index()\n",
    "    data_df = pd.read_parquet(data_path).reset_index()\n",
    "\n",
    "    # Identify the index column name (either __index_level_0__ or generic index)\n",
    "    labels_idx_col = '__index_level_0__' if '__index_level_0__' in labels_df.columns else 'index'\n",
    "    data_idx_col = '__index_level_0__' if '__index_level_0__' in data_df.columns else 'index'\n",
    "\n",
    "    # Rename for clarity: index → col_idx, type → label, values stays values\n",
    "    labels_df = labels_df.rename(columns={labels_idx_col: 'col_idx', 'type': 'label'})\n",
    "    data_df = data_df.rename(columns={data_idx_col: 'col_idx', 'values': 'values'})\n",
    "\n",
    "    # Remap labels using LABEL_MAP_LC\n",
    "    # assumes remap_labels(series, mapping) is defined and LABEL_MAP_LC is available\n",
    "\n",
    "    #labels_df['label'] = remap_labels(labels_df['label'], LABEL_MAP_LC)\n",
    "\n",
    "    # Filter out __none__ labels\n",
    "    labels_df = labels_df[labels_df['label'] != \"__none__\"]\n",
    "\n",
    "    # Merge on column index\n",
    "    merged = pd.merge(labels_df, data_df, on='col_idx', how='inner')\n",
    "\n",
    "    # Prepare session and model\n",
    "    session = requests.Session()\n",
    "    if \"-zs\" in model:\n",
    "        base_model.eval()\n",
    "\n",
    "\n",
    "    # Iterate over each column instance\n",
    "    for idx, row in tqdm(enumerate(merged.itertuples(index=False)), total=len(merged)):\n",
    "        \n",
    "        # Periodic cache save\n",
    "        if idx % 100 == 0:\n",
    "            with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(prompt_dict, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        if stop_early > -1 and idx == stop_early:\n",
    "            break\n",
    "\n",
    "        col_idx = row.col_idx\n",
    "        orig_label = row.label\n",
    "        raw_vals = row.values\n",
    "\n",
    "        # Parse raw values into a list\n",
    "        if isinstance(raw_vals, str):\n",
    "            vals = raw_vals.split(',')\n",
    "        else:\n",
    "            vals = list(raw_vals)\n",
    "\n",
    "        # Deduplicate and sample\n",
    "        vals = [str(x) for x in vals]\n",
    "        unique_vals = pd.unique(vals)\n",
    "        context_list = unique_vals.tolist()[:sample_size]\n",
    "\n",
    "        # Build context\n",
    "        if table_src:\n",
    "            context = insert_source(context_list, str(col_idx))\n",
    "        else:\n",
    "            context = context_list\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        # Model call\n",
    "        if \"gpt-3.5\" in model:\n",
    "            key = get_chatgpt_resp(label_set, context, orig_label,\n",
    "                                   prompt_dict, response=response,\n",
    "                                   session=session, method=method)\n",
    "        elif \"ada-personal\" in model:\n",
    "            key = get_ada_resp(label_set, context, orig_label,\n",
    "                               prompt_dict, response=response,\n",
    "                               session=session)\n",
    "        elif \"bloomz\" in model:\n",
    "            key = get_bloomz_resp(label_set, context, orig_label,\n",
    "                                  prompt_dict, response=response,\n",
    "                                  session=session)\n",
    "        else:\n",
    "            key = get_llama_resp(label_set, context, orig_label,\n",
    "                                 prompt_dict, link=link,\n",
    "                                 response=response,\n",
    "                                 session=session,\n",
    "                                 cbc=None,\n",
    "                                 model=model,\n",
    "                                 limited_context=context_list,\n",
    "                                 method=method)\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "        # Record metadata\n",
    "        prompt_dict[key]['original_label'] = orig_label\n",
    "        prompt_dict[key]['file+idx'] = str(col_idx)\n",
    "\n",
    "\n",
    "        inference_time = end_time - start_time\n",
    "        inference_times.append(inference_time)\n",
    "        total_inference_time += inference_time\n",
    "\n",
    "    n_calls = len(inference_times)\n",
    "    print(f\"Total inference time: {total_inference_time:.2f}s over {n_calls} calls\")\n",
    "    print(f\"  → average per call: {total_inference_time/n_calls:.3f}s\")\n",
    "    print(f\"  → min / max per call: {min(inference_times):.3f}s / {max(inference_times):.3f}s\")\n",
    "    # Final cache save\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(prompt_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Optional result summary\n",
    "    #if results and results_checker is not None:\n",
    "    #    results_checker(save_path, skip_duplicates=False)\n",
    "\n",
    "    return total_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbb07a01-53a9-4639-b197-fb24a0479456",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T09:15:17.282673Z",
     "start_time": "2025-07-23T09:15:17.278604Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:29.261045Z",
     "iopub.status.busy": "2025-08-08T13:51:29.260747Z",
     "iopub.status.idle": "2025-08-08T13:51:29.265475Z",
     "shell.execute_reply": "2025-08-08T13:51:29.264584Z"
    },
    "papermill": {
     "duration": 0.018397,
     "end_time": "2025-08-08T13:51:29.266866",
     "exception": false,
     "start_time": "2025-08-08T13:51:29.248469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_set = {\n",
    "  \"name\": \"custom_csv\",     # any string that does NOT contain \"d4\"\n",
    "  \"label_set\": LABELS,      # the list of your labels, used by similarity\n",
    "  \"dict_map\": { lab: lab for lab in LABELS },\n",
    "  \"abbrev_map\": {}          # or your real abbrev map\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cd83aaa-8825-403e-9f26-b62e8fd0765e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T09:17:52.502998Z",
     "start_time": "2025-07-23T09:15:18.885893Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:29.291009Z",
     "iopub.status.busy": "2025-08-08T13:51:29.290724Z",
     "iopub.status.idle": "2025-08-08T13:51:58.374929Z",
     "shell.execute_reply": "2025-08-08T13:51:58.374094Z"
    },
    "papermill": {
     "duration": 29.097963,
     "end_time": "2025-08-08T13:51:58.376347",
     "exception": false,
     "start_time": "2025-08-08T13:51:29.278384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omadbek/.conda/envs/archetype/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7812718ca9470f9f3ba2063c9845fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omadbek/.conda/envs/archetype/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cf9c69f38c4b99afba765bf49d3b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2267921/4185749951.py:99: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unique_vals = pd.unique(vals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inference time: 14.99s over 55 calls\n",
      "  → average per call: 0.273s\n",
      "  → min / max per call: 0.000s / 0.922s\n"
     ]
    }
   ],
   "source": [
    "filename = f\"custom-data-{model_name}-{tune}.json\"\n",
    "\n",
    "sp = f\"./custom_data_logs/{filename}\"\n",
    "\n",
    "dirpath = os.path.dirname(sp)\n",
    "os.makedirs(dirpath, exist_ok=True)\n",
    "\n",
    "# Test set\n",
    "labels_path = test_labels_path\n",
    "data_path = test_data_path\n",
    "\n",
    "\n",
    "if model_name == \"llama\":\n",
    "    # LLAMA\n",
    "    inference_time = run_val_parquet(\n",
    "        model=\"llama\",\n",
    "        save_path=sp,\n",
    "        labels_path=labels_path,\n",
    "        data_path=data_path,\n",
    "        label_set=label_set,\n",
    "        method=[\"similarity\"],\n",
    "        resume=False,\n",
    "        sample_size=5,\n",
    "        link = \"http://localhost:11434/api/generate\"\n",
    "    )\n",
    "elif model_name == \"flan-ul2\":\n",
    "    base_model, tokenizer, template, pt, MAX_LEN = init_model(model_name)\n",
    "    inference_time = run_val_parquet(\n",
    "        model=\"flan-ul2-zs\",\n",
    "        save_path=sp,\n",
    "        labels_path= labels_path,\n",
    "        data_path= data_path,\n",
    "        label_set=label_set,\n",
    "        method=[\"similarity\"],\n",
    "        resume=True,\n",
    "        sample_size=5\n",
    "    )\n",
    "elif model_name == \"flan-t5\":\n",
    "    base_model, tokenizer, template, pt, MAX_LEN = init_model(model_name)\n",
    "    inference_time = run_val_parquet(\n",
    "        model=\"topp-zs\",\n",
    "        save_path=sp,\n",
    "        labels_path= labels_path,\n",
    "        data_path= data_path,\n",
    "        label_set=label_set,\n",
    "        method=[\"similarity\"],\n",
    "        resume=True,\n",
    "        sample_size=5\n",
    "    )\n",
    "elif model_name == \"flan-t5-xxl\":\n",
    "    base_model, tokenizer, template, pt, MAX_LEN = init_model(model_name)\n",
    "    inference_time = run_val_parquet(\n",
    "        model=\"topp-zs\",\n",
    "        save_path=sp,\n",
    "        labels_path= labels_path,\n",
    "        data_path= data_path,\n",
    "        label_set=label_set,\n",
    "        method=[\"similarity\"],\n",
    "        resume=True,\n",
    "        sample_size=5\n",
    "    )\n",
    "elif model_name == \"alpaca-fine-tuned\":\n",
    "    base_model, tokenizer, template, pt, MAX_LEN = init_model(model_name)\n",
    "    inference_time = run_val_parquet(\n",
    "        model=\"topp-zs\",\n",
    "        save_path=sp,\n",
    "        labels_path= labels_path,\n",
    "        data_path= data_path,\n",
    "        label_set=label_set,\n",
    "        method=[\"similarity\"],\n",
    "        resume=True,\n",
    "        sample_size=5\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Invalid model name: {model_name}. Please choose from: llama, flan-ul2, flan-t5, flan-t5-xxl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe4311a83efa0299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T09:17:52.670456Z",
     "start_time": "2025-07-23T09:17:52.666956Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:58.403343Z",
     "iopub.status.busy": "2025-08-08T13:51:58.402551Z",
     "iopub.status.idle": "2025-08-08T13:51:58.407101Z",
     "shell.execute_reply": "2025-08-08T13:51:58.406551Z"
    },
    "papermill": {
     "duration": 0.018974,
     "end_time": "2025-08-08T13:51:58.408222",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.389248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File exists: ./custom_data_logs/custom-data-flan-t5-xxl-flan-t5-xxl_run8.json\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(sp):\n",
    "    print(f\"✅ File exists: {sp}\")\n",
    "else:\n",
    "    print(f\"❌ File not found: {sp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f9962a6115f5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T09:17:52.744876Z",
     "start_time": "2025-07-23T09:17:52.742697Z"
    },
    "papermill": {
     "duration": 0.012159,
     "end_time": "2025-08-08T13:51:58.432664",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.420505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8536b25-b8ce-48ce-a5e6-d33ad261362d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T09:17:52.878208Z",
     "start_time": "2025-07-23T09:17:52.820551Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:58.455674Z",
     "iopub.status.busy": "2025-08-08T13:51:58.455264Z",
     "iopub.status.idle": "2025-08-08T13:51:58.509082Z",
     "shell.execute_reply": "2025-08-08T13:51:58.508296Z"
    },
    "papermill": {
     "duration": 0.06541,
     "end_time": "2025-08-08T13:51:58.510402",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.444992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries: 55\n",
      "Accuracy:     0.8545\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "            age     1.0000    1.0000    1.0000         3\n",
      "    case_status     0.0000    0.0000    0.0000         2\n",
      "contact_setting     0.0000    0.0000    0.0000         1\n",
      "           date     1.0000    1.0000    1.0000        12\n",
      "         gender     0.6667    1.0000    0.8000         2\n",
      "             id     1.0000    1.0000    1.0000         5\n",
      "       location     1.0000    1.0000    1.0000         6\n",
      "medical_boolean     0.7826    0.9000    0.8372        20\n",
      "     occupation     1.0000    1.0000    1.0000         1\n",
      "        outcome     0.0000    0.0000    0.0000         2\n",
      "       symptoms     0.0000    0.0000    0.0000         1\n",
      "\n",
      "       accuracy                         0.8545        55\n",
      "      macro avg     0.5863    0.6273    0.6034        55\n",
      "   weighted avg     0.7997    0.8545    0.8244        55\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_precision</th>\n",
       "      <th>age_recall</th>\n",
       "      <th>age_f1-score</th>\n",
       "      <th>age_support</th>\n",
       "      <th>case_status_precision</th>\n",
       "      <th>case_status_recall</th>\n",
       "      <th>case_status_f1-score</th>\n",
       "      <th>case_status_support</th>\n",
       "      <th>contact_setting_precision</th>\n",
       "      <th>contact_setting_recall</th>\n",
       "      <th>...</th>\n",
       "      <th>macro avg_recall</th>\n",
       "      <th>macro avg_f1-score</th>\n",
       "      <th>macro avg_support</th>\n",
       "      <th>weighted avg_precision</th>\n",
       "      <th>weighted avg_recall</th>\n",
       "      <th>weighted avg_f1-score</th>\n",
       "      <th>weighted avg_support</th>\n",
       "      <th>total_entries</th>\n",
       "      <th>run_name</th>\n",
       "      <th>inference_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627273</td>\n",
       "      <td>0.603383</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.799736</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.82444</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55</td>\n",
       "      <td>custom-data-flan-t5-xxl-flan-t5-xxl_run8</td>\n",
       "      <td>14.988487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age_precision  age_recall  age_f1-score  age_support  \\\n",
       "0            1.0         1.0           1.0          3.0   \n",
       "\n",
       "   case_status_precision  case_status_recall  case_status_f1-score  \\\n",
       "0                    0.0                 0.0                   0.0   \n",
       "\n",
       "   case_status_support  contact_setting_precision  contact_setting_recall  \\\n",
       "0                  2.0                        0.0                     0.0   \n",
       "\n",
       "   ...  macro avg_recall  macro avg_f1-score  macro avg_support  \\\n",
       "0  ...          0.627273            0.603383               55.0   \n",
       "\n",
       "   weighted avg_precision  weighted avg_recall  weighted avg_f1-score  \\\n",
       "0                0.799736             0.854545                0.82444   \n",
       "\n",
       "   weighted avg_support  total_entries  \\\n",
       "0                  55.0             55   \n",
       "\n",
       "                                   run_name  inference_time  \n",
       "0  custom-data-flan-t5-xxl-flan-t5-xxl_run8       14.988487  \n",
       "\n",
       "[1 rows x 56 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_checker(sp, inference_time)\n",
    "\n",
    "#print(f\"Temperature: {temperature}\")\n",
    "#print(f\"Top-p: {top_p}\")\n",
    "## Load Alpaca weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8d392-38ea-45d5-8e85-281049b5bdca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:55:08.686780Z",
     "start_time": "2025-07-21T13:55:08.684395Z"
    },
    "papermill": {
     "duration": 0.012678,
     "end_time": "2025-08-08T13:51:58.536020",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.523342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad31181-a79e-4f90-a55b-7d104570a862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:55:08.787883Z",
     "start_time": "2025-07-21T13:55:08.785568Z"
    },
    "papermill": {
     "duration": 0.012231,
     "end_time": "2025-08-08T13:51:58.560959",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.548728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efc5dc90d553807d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:55:08.869541Z",
     "start_time": "2025-07-21T13:55:08.866366Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:58.580113Z",
     "iopub.status.busy": "2025-08-08T13:51:58.579824Z",
     "iopub.status.idle": "2025-08-08T13:51:58.583405Z",
     "shell.execute_reply": "2025-08-08T13:51:58.582646Z"
    },
    "papermill": {
     "duration": 0.014416,
     "end_time": "2025-08-08T13:51:58.584485",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.570069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(\"Temperature: {temperature}\")\n",
    "#print(\"Top-p: {top_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2bbabd084979a",
   "metadata": {
    "papermill": {
     "duration": 0.009088,
     "end_time": "2025-08-08T13:51:58.602809",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.593721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Check alpaca not needed for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "174c5f23-882e-45fd-9a03-d1deca2c363a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:55:09.070660Z",
     "start_time": "2025-07-21T13:55:09.068218Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:58.622171Z",
     "iopub.status.busy": "2025-08-08T13:51:58.621890Z",
     "iopub.status.idle": "2025-08-08T13:51:58.625843Z",
     "shell.execute_reply": "2025-08-08T13:51:58.624964Z"
    },
    "papermill": {
     "duration": 0.01506,
     "end_time": "2025-08-08T13:51:58.627074",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.612014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from huggingface_hub import snapshot_download\n",
    "\n",
    "# Replace the repo_id below with the exact model‐ID you used.\n",
    "# Often it’s something like \"TheBloke/alpaca-13b-……\"\n",
    "#repo_id = \"chavinlo/alpaca-native\"\n",
    "#local_path = snapshot_download(repo_id)\n",
    "\n",
    "#print(\"Alpaca-13b is here:\", local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0aca134-208c-4793-abd4-a1478fe79538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:55:09.129941Z",
     "start_time": "2025-07-21T13:55:09.127019Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:58.647041Z",
     "iopub.status.busy": "2025-08-08T13:51:58.646433Z",
     "iopub.status.idle": "2025-08-08T13:51:58.650404Z",
     "shell.execute_reply": "2025-08-08T13:51:58.649661Z"
    },
    "papermill": {
     "duration": 0.015092,
     "end_time": "2025-08-08T13:51:58.651605",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.636513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#    elif model == \"alpaca-13b\":\n",
    "#        MAX_LEN=2048\n",
    "#        tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "#        base_model = LlamaForCausalLM.from_pretrained(\n",
    "#            \"chavinlo/alpaca-native\",\n",
    "#            torch_dtype=torch.float16,\n",
    "#            load_in_8bit=True,\n",
    "#            device_map='auto',\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e1f6cd6-65d6-482f-877b-1d226ba21054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:55:09.216378Z",
     "start_time": "2025-07-21T13:55:09.213022Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:58.671269Z",
     "iopub.status.busy": "2025-08-08T13:51:58.670874Z",
     "iopub.status.idle": "2025-08-08T13:51:58.674568Z",
     "shell.execute_reply": "2025-08-08T13:51:58.673807Z"
    },
    "papermill": {
     "duration": 0.014804,
     "end_time": "2025-08-08T13:51:58.675774",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.660970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#local_path = \"/home/omadbek/.cache/huggingface/hub/models--chavinlo--alpaca-native/snapshots/3bf09cbff2fbd92d7d88a0f70ba24fca372befdf\"\n",
    "\n",
    "#tokenizer = LlamaTokenizer.from_pretrained(local_path)\n",
    "#model     = LlamaForCausalLM.from_pretrained(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96137456-a1b0-4145-bf75-e728d8f43683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:55:09.317450Z",
     "start_time": "2025-07-21T13:55:09.313854Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:58.695251Z",
     "iopub.status.busy": "2025-08-08T13:51:58.694860Z",
     "iopub.status.idle": "2025-08-08T13:51:58.698653Z",
     "shell.execute_reply": "2025-08-08T13:51:58.697922Z"
    },
    "papermill": {
     "duration": 0.014878,
     "end_time": "2025-08-08T13:51:58.699923",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.685045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python -m src.run --model_name=\"ArcheType-llama\" --model_path=\"/home/omadbek/.cache/huggingface/hub/models--chavinlo--alpaca-native/snapshots/3bf09cbff2fbd92d7d88a0f70ba24fca372befdf\" --save_path=\"./output\" --input_files=\"./WHO_data\" --input_labels=\"./custom_labels/cta_gt.csv\" --label_set=\"custom\" --method ans_contains_gt gt_contains_ans resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59f21daf-4aa6-4c3c-9765-829a27b14a40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:55:09.410306Z",
     "start_time": "2025-07-21T13:55:09.407159Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:58.719713Z",
     "iopub.status.busy": "2025-08-08T13:51:58.719146Z",
     "iopub.status.idle": "2025-08-08T13:51:58.722822Z",
     "shell.execute_reply": "2025-08-08T13:51:58.721918Z"
    },
    "papermill": {
     "duration": 0.014761,
     "end_time": "2025-08-08T13:51:58.724037",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.709276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96bb8e8-d647-4361-91f7-77e66fb6795e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:55:09.500306Z",
     "start_time": "2025-07-21T13:55:09.497828Z"
    },
    "papermill": {
     "duration": 0.009178,
     "end_time": "2025-08-08T13:51:58.742586",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.733408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d9b0404-dee8-4a46-b472-d65fc82e1dca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:55:09.592843Z",
     "start_time": "2025-07-21T13:55:09.589118Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:58.756789Z",
     "iopub.status.busy": "2025-08-08T13:51:58.756490Z",
     "iopub.status.idle": "2025-08-08T13:51:58.760769Z",
     "shell.execute_reply": "2025-08-08T13:51:58.760037Z"
    },
    "papermill": {
     "duration": 0.011156,
     "end_time": "2025-08-08T13:51:58.761653",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.750497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Test fine-tuned Alpaca\n",
    "#tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 2) Load the sharded safetensors into a LlamaForCausalLM:\n",
    "#model = LlamaForCausalLM.from_pretrained(\n",
    "#    model_path,\n",
    "#    torch_dtype=torch.float16,    # if you saved in fp16\n",
    "#    load_in_8bit=True,            # if you want to use 8-bit (requires bitsandbytes)\n",
    "#    device_map=\"auto\",            # or replace with e.g. {\"\": 0} for single-GPU\n",
    "#    low_cpu_mem_usage=True,       # helps when loading large models\n",
    "#)\n",
    "\n",
    "# 3) Switch to eval mode (optional, but common for inference):\n",
    "#model.eval()\n",
    "\n",
    "# 4) Now you can generate with your fine-tuned Alpaca:\n",
    "#prompt_text = \"INSTRUCTION: Select the option which best describes the input. INPUT: ['2024', '2019', '2018', '2018', '2020']. OPTIONS: - case_status - outcome - contact_setting - age - medical_boolean - symptoms - date - id - location - occupation - gender ANSWER:\"\n",
    "#input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "#outputs = model.generate(input_ids, max_length=400, do_sample=True, temperature=0.7)\n",
    "#print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5d7e658-0be2-469e-88cd-5155d17260b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:55:09.673043Z",
     "start_time": "2025-07-21T13:55:09.669593Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T13:51:58.773149Z",
     "iopub.status.busy": "2025-08-08T13:51:58.772873Z",
     "iopub.status.idle": "2025-08-08T13:51:58.776808Z",
     "shell.execute_reply": "2025-08-08T13:51:58.776059Z"
    },
    "papermill": {
     "duration": 0.011083,
     "end_time": "2025-08-08T13:51:58.777989",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.766906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import re\n",
    "\n",
    "#text = \"\"\"\n",
    "#\"original_model_answer\": \"based on the input, i would select:\\n\\n**case_status**\\n\\nthe categories in the input ('outpatient', 'inpatient', 'discharged', 'transferred') are all related to the status of a patient or individual, which is typically referred to as their \\\"case status\\\". this category matches the options provided.\",\n",
    "#\"\"\"\n",
    "\n",
    "#pattern = re.compile(r\"\\n\\n(?:\\*\\*([^*]+)\\*\\*|(\\w+))\\n\\n\")\n",
    "#for m in pattern.finditer(text):\n",
    "#    word = m.group(1) or m.group(2)\n",
    "#    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962c7508-837a-4fd7-9217-1933826c5f23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:55:09.806424Z",
     "start_time": "2025-07-21T13:55:09.803682Z"
    },
    "papermill": {
     "duration": 0.005237,
     "end_time": "2025-08-08T13:51:58.788867",
     "exception": false,
     "start_time": "2025-08-08T13:51:58.783630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38.049091,
   "end_time": "2025-08-08T13:52:00.616610",
   "environment_variables": {},
   "exception": null,
   "input_path": "custom_inference.ipynb",
   "output_path": "/home/omadbek/projects/ArcheType/papermill_notebooks/test_out_flan-t5-xxl_run8.ipynb",
   "parameters": {
    "model_name": "flan-t5-xxl",
    "sherlock_path": "/home/omadbek/projects/Sherlock",
    "tune": "flan-t5-xxl_run8"
   },
   "start_time": "2025-08-08T13:51:22.567519",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01a7f8f13f574e4caaf43039dd59b20c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2938832e93974c948b260e1573f91771",
       "max": 55.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ce94b150321549f7b6920511f0ee577e",
       "value": 55.0
      }
     },
     "02831ac0645649ff9733f2f41bc04e41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "16348870f85a45e3b1c33774482a9468": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6b7e7047add1458d9669974b57cc05fd",
       "placeholder": "​",
       "style": "IPY_MODEL_e3d06e73d120466fb2beca8a71a37371",
       "value": " 55/55 [00:15&lt;00:00,  3.75it/s]"
      }
     },
     "1f8229227ea2479dad96a80cd71ca57c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2938832e93974c948b260e1573f91771": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2da8f761032449258acbf94b33a294c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4d50e0a5d26c4b7caf0096e1e50cf20e",
       "max": 5.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_553c6ad50136438a9e584c9b5a7f7ef1",
       "value": 5.0
      }
     },
     "3f718258ab08434b9b9b21ea93b1dec0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4d50e0a5d26c4b7caf0096e1e50cf20e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "553c6ad50136438a9e584c9b5a7f7ef1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "55e96d7ef5d647c2bf1f97c3372d089e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_74fd7dc3f1074fa58f273211f2962bd4",
       "placeholder": "​",
       "style": "IPY_MODEL_1f8229227ea2479dad96a80cd71ca57c",
       "value": " 5/5 [00:11&lt;00:00,  2.25s/it]"
      }
     },
     "6b7e7047add1458d9669974b57cc05fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "74fd7dc3f1074fa58f273211f2962bd4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b7812718ca9470f9f3ba2063c9845fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cc7b8205eb634212af35e86fafe753cf",
        "IPY_MODEL_2da8f761032449258acbf94b33a294c1",
        "IPY_MODEL_55e96d7ef5d647c2bf1f97c3372d089e"
       ],
       "layout": "IPY_MODEL_d817facbc8894e39b47243fda42f802e"
      }
     },
     "88cf9c69f38c4b99afba765bf49d3b47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9f5d0a8b1bcc4c88ac980526129ecd97",
        "IPY_MODEL_01a7f8f13f574e4caaf43039dd59b20c",
        "IPY_MODEL_16348870f85a45e3b1c33774482a9468"
       ],
       "layout": "IPY_MODEL_3f718258ab08434b9b9b21ea93b1dec0"
      }
     },
     "8fb561130ee246f7b1d2254e49e39e5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9f5d0a8b1bcc4c88ac980526129ecd97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ef6a0b5a72d0452ab453fd28ee44a75a",
       "placeholder": "​",
       "style": "IPY_MODEL_8fb561130ee246f7b1d2254e49e39e5c",
       "value": "100%"
      }
     },
     "cc7b8205eb634212af35e86fafe753cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f9872567bdd34e5a9f6c6c6e7dca6495",
       "placeholder": "​",
       "style": "IPY_MODEL_02831ac0645649ff9733f2f41bc04e41",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "ce94b150321549f7b6920511f0ee577e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d817facbc8894e39b47243fda42f802e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e3d06e73d120466fb2beca8a71a37371": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ef6a0b5a72d0452ab453fd28ee44a75a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f9872567bdd34e5a9f6c6c6e7dca6495": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}